<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file source-language="en" target-language="ko" datatype="htmlbody" original="pytorch">
    <body>
      <group id="pytorch">
        <trans-unit id="6fd372cb1c5e72e14169cfb741de07d656f9f533" translate="yes" xml:space="preserve">
          <source>A sparse tensor is represented as a pair of dense tensors: a tensor of values and a 2D tensor of indices. A sparse tensor can be constructed by providing these two tensors, as well as the size of the sparse tensor (which cannot be inferred from these tensors!) Suppose we want to define a sparse tensor with the entry 3 at location (0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2). We would then write:</source>
          <target state="translated">A sparse tensor is represented as a pair of dense tensors: a tensor of values and a 2D tensor of indices. A sparse tensor can be constructed by providing these two tensors, as well as the size of the sparse tensor (which cannot be inferred from these tensors!) Suppose we want to define a sparse tensor with the entry 3 at location (0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2). We would then write:</target>
        </trans-unit>
        <trans-unit id="25e7a287394a0eef59197bb60e52ac1f6a84a5df" translate="yes" xml:space="preserve">
          <source>A store implementation that uses a file to store the underlying key-value pairs.</source>
          <target state="translated">A store implementation that uses a file to store the underlying key-value pairs.</target>
        </trans-unit>
        <trans-unit id="6eb9ae0897420968aee10a2b095c52bec596e05c" translate="yes" xml:space="preserve">
          <source>A string</source>
          <target state="translated">끈</target>
        </trans-unit>
        <trans-unit id="62ebd3a50dba8122d0758d5d23ce37a93f24ab7e" translate="yes" xml:space="preserve">
          <source>A structure that encapsulates information of a worker in the system. Contains the name and ID of the worker. This class is not meant to be constructed directly, rather, an instance can be retrieved through &lt;a href=&quot;#torch.distributed.rpc.get_worker_info&quot;&gt;&lt;code&gt;get_worker_info()&lt;/code&gt;&lt;/a&gt; and the result can be passed in to functions such as &lt;a href=&quot;#torch.distributed.rpc.rpc_sync&quot;&gt;&lt;code&gt;rpc_sync()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt;&lt;code&gt;rpc_async()&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.rpc.remote&quot;&gt;&lt;code&gt;remote()&lt;/code&gt;&lt;/a&gt; to avoid copying a string on every invocation.</source>
          <target state="translated">A structure that encapsulates information of a worker in the system. Contains the name and ID of the worker. This class is not meant to be constructed directly, rather, an instance can be retrieved through &lt;a href=&quot;#torch.distributed.rpc.get_worker_info&quot;&gt; &lt;code&gt;get_worker_info()&lt;/code&gt; &lt;/a&gt; and the result can be passed in to functions such as &lt;a href=&quot;#torch.distributed.rpc.rpc_sync&quot;&gt; &lt;code&gt;rpc_sync()&lt;/code&gt; &lt;/a&gt;, &lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt; &lt;code&gt;rpc_async()&lt;/code&gt; &lt;/a&gt;, &lt;a href=&quot;#torch.distributed.rpc.remote&quot;&gt; &lt;code&gt;remote()&lt;/code&gt; &lt;/a&gt; to avoid copying a string on every invocation.</target>
        </trans-unit>
        <trans-unit id="eb68c1bf6abfe77fe220468c23dfc4e2ccab3d1b" translate="yes" xml:space="preserve">
          <source>A tensor can be constructed from a Python &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt;&lt;code&gt;list&lt;/code&gt;&lt;/a&gt; or sequence using the &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;torch.tensor()&lt;/code&gt;&lt;/a&gt; constructor:</source>
          <target state="translated">A tensor can be constructed from a Python &lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html#list&quot;&gt; &lt;code&gt;list&lt;/code&gt; &lt;/a&gt; or sequence using the &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;torch.tensor()&lt;/code&gt; &lt;/a&gt; constructor:</target>
        </trans-unit>
        <trans-unit id="78690c5a267b02f430ea07580ff8566f6ba33315" translate="yes" xml:space="preserve">
          <source>A tensor can be created with &lt;code&gt;requires_grad=True&lt;/code&gt; so that &lt;a href=&quot;autograd#module-torch.autograd&quot;&gt;&lt;code&gt;torch.autograd&lt;/code&gt;&lt;/a&gt; records operations on them for automatic differentiation.</source>
          <target state="translated">A tensor can be created with &lt;code&gt;requires_grad=True&lt;/code&gt; so that &lt;a href=&quot;autograd#module-torch.autograd&quot;&gt; &lt;code&gt;torch.autograd&lt;/code&gt; &lt;/a&gt; records operations on them for automatic differentiation.</target>
        </trans-unit>
        <trans-unit id="28d24ca39991219e32d7e5d9431b5638f6178577" translate="yes" xml:space="preserve">
          <source>A tensor containing the STFT result with shape described above</source>
          <target state="translated">A tensor containing the STFT result with shape described above</target>
        </trans-unit>
        <trans-unit id="4fa2d7d56cae6fbb5c5952f1e0d2ce41103f7c68" translate="yes" xml:space="preserve">
          <source>A tensor containing the complex-to-complex Fourier transform result</source>
          <target state="translated">A tensor containing the complex-to-complex Fourier transform result</target>
        </trans-unit>
        <trans-unit id="f7a71af7c596a286496bba8f33f4ec3e6cafe018" translate="yes" xml:space="preserve">
          <source>A tensor containing the complex-to-complex inverse Fourier transform result</source>
          <target state="translated">A tensor containing the complex-to-complex inverse Fourier transform result</target>
        </trans-unit>
        <trans-unit id="247163b1a472b2aa12f5bee5985796f6dfb72690" translate="yes" xml:space="preserve">
          <source>A tensor containing the complex-to-real inverse Fourier transform result</source>
          <target state="translated">A tensor containing the complex-to-real inverse Fourier transform result</target>
        </trans-unit>
        <trans-unit id="795a9e2ee149db6f297c73b4efe85421d1b43891" translate="yes" xml:space="preserve">
          <source>A tensor containing the real-to-complex Fourier transform result</source>
          <target state="translated">A tensor containing the real-to-complex Fourier transform result</target>
        </trans-unit>
        <trans-unit id="808c26c158b1b41512b2f49967eb9955a23d3ece" translate="yes" xml:space="preserve">
          <source>A tensor equivalent to converting all the input tensors into lists,</source>
          <target state="translated">A tensor equivalent to converting all the input tensors into lists,</target>
        </trans-unit>
        <trans-unit id="9f00a5099d29a70d5489d75e795a620e76bd2892" translate="yes" xml:space="preserve">
          <source>A tensor equivalent to converting all the input tensors into lists, do &lt;code&gt;itertools.combinations&lt;/code&gt; or &lt;code&gt;itertools.combinations_with_replacement&lt;/code&gt; on these lists, and finally convert the resulting list into tensor.</source>
          <target state="translated">A tensor equivalent to converting all the input tensors into lists, do &lt;code&gt;itertools.combinations&lt;/code&gt; or &lt;code&gt;itertools.combinations_with_replacement&lt;/code&gt; on these lists, and finally convert the resulting list into tensor.</target>
        </trans-unit>
        <trans-unit id="ffa7cd4e81e99e52c54f7880515bfb697dd56d45" translate="yes" xml:space="preserve">
          <source>A tensor of shape equal to the broadcasted shape of &lt;code&gt;condition&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;</source>
          <target state="translated">A tensor of shape equal to the broadcasted shape of &lt;code&gt;condition&lt;/code&gt; , &lt;code&gt;x&lt;/code&gt; , &lt;code&gt;y&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="9f636fee4017e68defd8eca1418692d893e6ca2e" translate="yes" xml:space="preserve">
          <source>A tensor of specific data type can be constructed by passing a &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt; and/or a &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt; to a constructor or tensor creation op:</source>
          <target state="translated">A tensor of specific data type can be constructed by passing a &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; and/or a &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt; to a constructor or tensor creation op:</target>
        </trans-unit>
        <trans-unit id="56a6db01c3a1cc54c7399f9ada8907490b637b87" translate="yes" xml:space="preserve">
          <source>A tensor or a tuple of tensors containing</source>
          <target state="translated">A tensor or a tuple of tensors containing</target>
        </trans-unit>
        <trans-unit id="7015c602ad96676ce56c48693c2eb3931eef821e" translate="yes" xml:space="preserve">
          <source>A thread-safe store implementation based on an underlying hashmap. This store can be used within the same process (for example, by other threads), but cannot be used across processes.</source>
          <target state="translated">A thread-safe store implementation based on an underlying hashmap. This store can be used within the same process (for example, by other threads), but cannot be used across processes.</target>
        </trans-unit>
        <trans-unit id="fc0920364dd37ceba72cafbd19ccdfa555706749" translate="yes" xml:space="preserve">
          <source>A transformer model.</source>
          <target state="translated">A transformer model.</target>
        </trans-unit>
        <trans-unit id="6179bf6b8f65bc90ae5ee40cc1d9226f83588987" translate="yes" xml:space="preserve">
          <source>A transformer model. User is able to modify the attributes as needed. The architecture is based on the paper &amp;ldquo;Attention Is All You Need&amp;rdquo;. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users can build the BERT(&lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;https://arxiv.org/abs/1810.04805&lt;/a&gt;) model with corresponding parameters.</source>
          <target state="translated">A transformer model. User is able to modify the attributes as needed. The architecture is based on the paper &amp;ldquo;Attention Is All You Need&amp;rdquo;. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users can build the BERT(&lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;https://arxiv.org/abs/1810.04805&lt;/a&gt;) model with corresponding parameters.</target>
        </trans-unit>
        <trans-unit id="8d66b15a6d20ea906de4e959af867fbbfb815b34" translate="yes" xml:space="preserve">
          <source>A tuple containing subtypes &lt;code&gt;T0&lt;/code&gt;, &lt;code&gt;T1&lt;/code&gt;, etc. (e.g. &lt;code&gt;Tuple[Tensor, Tensor]&lt;/code&gt;)</source>
          <target state="translated">A tuple containing subtypes &lt;code&gt;T0&lt;/code&gt; , &lt;code&gt;T1&lt;/code&gt; , etc. (e.g. &lt;code&gt;Tuple[Tensor, Tensor]&lt;/code&gt; )</target>
        </trans-unit>
        <trans-unit id="d1879bf0c8405911e4b464be498b79cafb324a20" translate="yes" xml:space="preserve">
          <source>A tuple of tensors containing</source>
          <target state="translated">A tuple of tensors containing</target>
        </trans-unit>
        <trans-unit id="a2414a0a13983fc4726a965cde7c58a73e7199ce" translate="yes" xml:space="preserve">
          <source>A user &lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt;&lt;code&gt;RRef&lt;/code&gt;&lt;/a&gt; instance to the result value. Use the blocking API &lt;a href=&quot;#torch.distributed.rpc.RRef.to_here&quot;&gt;&lt;code&gt;torch.distributed.rpc.RRef.to_here()&lt;/code&gt;&lt;/a&gt; to retrieve the result value locally.</source>
          <target state="translated">A user &lt;a href=&quot;#torch.distributed.rpc.RRef&quot;&gt; &lt;code&gt;RRef&lt;/code&gt; &lt;/a&gt; instance to the result value. Use the blocking API &lt;a href=&quot;#torch.distributed.rpc.RRef.to_here&quot;&gt; &lt;code&gt;torch.distributed.rpc.RRef.to_here()&lt;/code&gt; &lt;/a&gt; to retrieve the result value locally.</target>
        </trans-unit>
        <trans-unit id="327c954237e47bd5486826604abda3addea9acfd" translate="yes" xml:space="preserve">
          <source>A value which is either None or type &lt;code&gt;T&lt;/code&gt;</source>
          <target state="translated">A value which is either None or type &lt;code&gt;T&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="50a643ab21488e75a8cc3c466f11fce2f4ad673a" translate="yes" xml:space="preserve">
          <source>A wrapper around Python&amp;rsquo;s assert which is symbolically traceable.</source>
          <target state="translated">A wrapper around Python&amp;rsquo;s assert which is symbolically traceable.</target>
        </trans-unit>
        <trans-unit id="6c4e7377e95c980888bb30d4e414aed707349a68" translate="yes" xml:space="preserve">
          <source>A wrapper around any of the 3 key-value stores (&lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt;&lt;code&gt;TCPStore&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.FileStore&quot;&gt;&lt;code&gt;FileStore&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;#torch.distributed.HashStore&quot;&gt;&lt;code&gt;HashStore&lt;/code&gt;&lt;/a&gt;) that adds a prefix to each key inserted to the store.</source>
          <target state="translated">A wrapper around any of the 3 key-value stores (&lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt; &lt;code&gt;TCPStore&lt;/code&gt; &lt;/a&gt;, &lt;a href=&quot;#torch.distributed.FileStore&quot;&gt; &lt;code&gt;FileStore&lt;/code&gt; &lt;/a&gt;, and &lt;a href=&quot;#torch.distributed.HashStore&quot;&gt; &lt;code&gt;HashStore&lt;/code&gt; &lt;/a&gt;) that adds a prefix to each key inserted to the store.</target>
        </trans-unit>
        <trans-unit id="893fa7187a5a433ff31c6a646ba041165efd5e8a" translate="yes" xml:space="preserve">
          <source>A. Graves et al.: Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks: &lt;a href=&quot;https://www.cs.toronto.edu/~graves/icml_2006.pdf&quot;&gt;https://www.cs.toronto.edu/~graves/icml_2006.pdf&lt;/a&gt;</source>
          <target state="translated">A. Graves et al.: Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks: &lt;a href=&quot;https://www.cs.toronto.edu/~graves/icml_2006.pdf&quot;&gt;https://www.cs.toronto.edu/~graves/icml_2006.pdf&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="1eb4168992b5101108db3eeb3c96de61bbb45012" translate="yes" xml:space="preserve">
          <source>APIs in the RPC package are stable. There are multiple ongoing work items to improve performance and error handling, which will ship in future releases.</source>
          <target state="translated">APIs in the RPC package are stable. There are multiple ongoing work items to improve performance and error handling, which will ship in future releases.</target>
        </trans-unit>
        <trans-unit id="bccb0000d0e87e05464132ae5d1a9fb1c8982b8a" translate="yes" xml:space="preserve">
          <source>ATen operators</source>
          <target state="translated">ATen operators</target>
        </trans-unit>
        <trans-unit id="4bfc2c1b851857261545e0a7eb83c1db10ae6260" translate="yes" xml:space="preserve">
          <source>AX = B</source>
          <target state="translated">AX = B</target>
        </trans-unit>
        <trans-unit id="afc108ce2cbe35fd87218f8b08145cd05b2342e4" translate="yes" xml:space="preserve">
          <source>AX = b</source>
          <target state="translated">AX = b</target>
        </trans-unit>
        <trans-unit id="0d45206d15c0adf19b111143610d728deb04b811" translate="yes" xml:space="preserve">
          <source>A^T A / (m - 1)</source>
          <target state="translated">A^T A / (m - 1)</target>
        </trans-unit>
        <trans-unit id="086c7a89401b2c5d32553704f5cbdf2377564e0a" translate="yes" xml:space="preserve">
          <source>Abstract base class for creation of new pruning techniques.</source>
          <target state="translated">Abstract base class for creation of new pruning techniques.</target>
        </trans-unit>
        <trans-unit id="ce6150c1e37157c2b0ede38ac1f069f652c4bd27" translate="yes" xml:space="preserve">
          <source>Accepts as argument an instance of a BasePruningMethod or an iterable of them.</source>
          <target state="translated">Accepts as argument an instance of a BasePruningMethod or an iterable of them.</target>
        </trans-unit>
        <trans-unit id="f884a5a94ee360d4b70504f91c5e58e4219b6c1e" translate="yes" xml:space="preserve">
          <source>Accessing Module Parameters</source>
          <target state="translated">Accessing Module Parameters</target>
        </trans-unit>
        <trans-unit id="f2929dbc213f512c19707fd1ca48c5d74f64d113" translate="yes" xml:space="preserve">
          <source>Accumulate the elements of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; into the &lt;code&gt;self&lt;/code&gt; tensor by adding to the indices in the order given in &lt;code&gt;index&lt;/code&gt;. For example, if &lt;code&gt;dim == 0&lt;/code&gt; and &lt;code&gt;index[i] == j&lt;/code&gt;, then the &lt;code&gt;i&lt;/code&gt;th row of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; is added to the &lt;code&gt;j&lt;/code&gt;th row of &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="translated">Accumulate the elements of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; into the &lt;code&gt;self&lt;/code&gt; tensor by adding to the indices in the order given in &lt;code&gt;index&lt;/code&gt; . For example, if &lt;code&gt;dim == 0&lt;/code&gt; and &lt;code&gt;index[i] == j&lt;/code&gt; , then the &lt;code&gt;i&lt;/code&gt; th row of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; is added to the &lt;code&gt;j&lt;/code&gt; th row of &lt;code&gt;self&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="af92bf9a55b4e8ebc9cf19933cf41beac97277f2" translate="yes" xml:space="preserve">
          <source>Adaptive softmax is an approximate strategy for training models with large output spaces. It is most effective when the label distribution is highly imbalanced, for example in natural language modelling, where the word frequency distribution approximately follows the &lt;a href=&quot;https://en.wikipedia.org/wiki/Zipf%27s_law&quot;&gt;Zipf&amp;rsquo;s law&lt;/a&gt;.</source>
          <target state="translated">Adaptive softmax is an approximate strategy for training models with large output spaces. It is most effective when the label distribution is highly imbalanced, for example in natural language modelling, where the word frequency distribution approximately follows the &lt;a href=&quot;https://en.wikipedia.org/wiki/Zipf%27s_law&quot;&gt;Zipf&amp;rsquo;s law&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="6753d63f427d91531462240c7159228dba9dbe41" translate="yes" xml:space="preserve">
          <source>Adaptive softmax partitions the labels into several clusters, according to their frequency. These clusters may contain different number of targets each. Additionally, clusters containing less frequent labels assign lower dimensional embeddings to those labels, which speeds up the computation. For each minibatch, only clusters for which at least one target is present are evaluated.</source>
          <target state="translated">Adaptive softmax partitions the labels into several clusters, according to their frequency. These clusters may contain different number of targets each. Additionally, clusters containing less frequent labels assign lower dimensional embeddings to those labels, which speeds up the computation. For each minibatch, only clusters for which at least one target is present are evaluated.</target>
        </trans-unit>
        <trans-unit id="b7d62c155e1ec348bbbea519cad55c85951b3b6e" translate="yes" xml:space="preserve">
          <source>AdaptiveAvgPool1d</source>
          <target state="translated">AdaptiveAvgPool1d</target>
        </trans-unit>
        <trans-unit id="0068238690eed5246c61ee781eb3611fe6751168" translate="yes" xml:space="preserve">
          <source>AdaptiveAvgPool2d</source>
          <target state="translated">AdaptiveAvgPool2d</target>
        </trans-unit>
        <trans-unit id="65992cfd09d95132c3908bd45a21c959a32be4e1" translate="yes" xml:space="preserve">
          <source>AdaptiveAvgPool3d</source>
          <target state="translated">AdaptiveAvgPool3d</target>
        </trans-unit>
        <trans-unit id="a60f272cfc14a259a9716f4818eb79c22d95c973" translate="yes" xml:space="preserve">
          <source>AdaptiveLogSoftmaxWithLoss</source>
          <target state="translated">AdaptiveLogSoftmaxWithLoss</target>
        </trans-unit>
        <trans-unit id="84f10363ff1424545148abb336751c9a84b6497d" translate="yes" xml:space="preserve">
          <source>AdaptiveMaxPool1d</source>
          <target state="translated">AdaptiveMaxPool1d</target>
        </trans-unit>
        <trans-unit id="b001386de556a7b95313a763136a21be6a3d2d16" translate="yes" xml:space="preserve">
          <source>AdaptiveMaxPool2d</source>
          <target state="translated">AdaptiveMaxPool2d</target>
        </trans-unit>
        <trans-unit id="9315f841b98a9cfd3c84766eadb0e1b9b7bcbe32" translate="yes" xml:space="preserve">
          <source>AdaptiveMaxPool3d</source>
          <target state="translated">AdaptiveMaxPool3d</target>
        </trans-unit>
        <trans-unit id="53b75d7b44ea18bab3550e79b5f19b4d01eb4d3d" translate="yes" xml:space="preserve">
          <source>Add a scalar or tensor to &lt;code&gt;self&lt;/code&gt; tensor. If both &lt;code&gt;alpha&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; are specified, each element of &lt;code&gt;other&lt;/code&gt; is scaled by &lt;code&gt;alpha&lt;/code&gt; before being used.</source>
          <target state="translated">Add a scalar or tensor to &lt;code&gt;self&lt;/code&gt; tensor. If both &lt;code&gt;alpha&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; are specified, each element of &lt;code&gt;other&lt;/code&gt; is scaled by &lt;code&gt;alpha&lt;/code&gt; before being used.</target>
        </trans-unit>
        <trans-unit id="363f75ca10e654de02076c9e812636b00b9240b1" translate="yes" xml:space="preserve">
          <source>Add a set of hyperparameters to be compared in TensorBoard.</source>
          <target state="translated">Add a set of hyperparameters to be compared in TensorBoard.</target>
        </trans-unit>
        <trans-unit id="2981d9b125bbce3a8357f497272caa27d116b65e" translate="yes" xml:space="preserve">
          <source>Add audio data to summary.</source>
          <target state="translated">Add audio data to summary.</target>
        </trans-unit>
        <trans-unit id="378dceab0437ac530e9f4b067ecb1268bcc2db27" translate="yes" xml:space="preserve">
          <source>Add batched image data to summary.</source>
          <target state="translated">Add batched image data to summary.</target>
        </trans-unit>
        <trans-unit id="b6c70cf24a9dc9ee0a583b95ed8a5b22e79a979d" translate="yes" xml:space="preserve">
          <source>Add embedding projector data to summary.</source>
          <target state="translated">Add embedding projector data to summary.</target>
        </trans-unit>
        <trans-unit id="745fa53d610381d82d054b114f7c40ae6aaa4f61" translate="yes" xml:space="preserve">
          <source>Add graph data to summary.</source>
          <target state="translated">Add graph data to summary.</target>
        </trans-unit>
        <trans-unit id="360796ce4b88f8a72813e93a376d34c0228713ee" translate="yes" xml:space="preserve">
          <source>Add histogram to summary.</source>
          <target state="translated">Add histogram to summary.</target>
        </trans-unit>
        <trans-unit id="1b6b7d6099858bcb255707e61a495522a0414994" translate="yes" xml:space="preserve">
          <source>Add image data to summary.</source>
          <target state="translated">Add image data to summary.</target>
        </trans-unit>
        <trans-unit id="a392c90962df4acfb05e6d0ec46fd5a47980ff9d" translate="yes" xml:space="preserve">
          <source>Add meshes or 3D point clouds to TensorBoard. The visualization is based on Three.js, so it allows users to interact with the rendered object. Besides the basic definitions such as vertices, faces, users can further provide camera parameter, lighting condition, etc. Please check &lt;a href=&quot;https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene&quot;&gt;https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene&lt;/a&gt; for advanced usage.</source>
          <target state="translated">Add meshes or 3D point clouds to TensorBoard. The visualization is based on Three.js, so it allows users to interact with the rendered object. Besides the basic definitions such as vertices, faces, users can further provide camera parameter, lighting condition, etc. Please check &lt;a href=&quot;https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene&quot;&gt;https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene&lt;/a&gt; for advanced usage.</target>
        </trans-unit>
        <trans-unit id="701a19440eea643652da7e69f517b8c3d995eacc" translate="yes" xml:space="preserve">
          <source>Add scalar data to summary.</source>
          <target state="translated">Add scalar data to summary.</target>
        </trans-unit>
        <trans-unit id="7a54b0b6b6b02f66bd6beaecffa1d99a5f7eb192" translate="yes" xml:space="preserve">
          <source>Add text data to summary.</source>
          <target state="translated">Add text data to summary.</target>
        </trans-unit>
        <trans-unit id="c77f8dadb528de130d984e34a450fb3582dda9aa" translate="yes" xml:space="preserve">
          <source>Add video data to summary.</source>
          <target state="translated">Add video data to summary.</target>
        </trans-unit>
        <trans-unit id="8afe6f3185ed76e023f80393638df4f157d48f52" translate="yes" xml:space="preserve">
          <source>Adding export support for operators is an &lt;em&gt;advance usage&lt;/em&gt;.</source>
          <target state="translated">Adding export support for operators is an &lt;em&gt;advance usage&lt;/em&gt;.</target>
        </trans-unit>
        <trans-unit id="4c9c6ec239352be639e5bda7f0d0b398caeb9237" translate="yes" xml:space="preserve">
          <source>Adding support for operators</source>
          <target state="translated">Adding support for operators</target>
        </trans-unit>
        <trans-unit id="4e032cc35826dbb2a4a560c47d8b0573e59463ab" translate="yes" xml:space="preserve">
          <source>Additional args:</source>
          <target state="translated">Additional args:</target>
        </trans-unit>
        <trans-unit id="b02695e3612016d107e21b0824bdec94963550bb" translate="yes" xml:space="preserve">
          <source>Additionally accepts an optional &lt;code&gt;reduce&lt;/code&gt; argument that allows specification of an optional reduction operation, which is applied to all values in the tensor &lt;code&gt;src&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; at the indicies specified in the &lt;code&gt;index&lt;/code&gt;. For each value in &lt;code&gt;src&lt;/code&gt;, the reduction operation is applied to an index in &lt;code&gt;self&lt;/code&gt; which is specified by its index in &lt;code&gt;src&lt;/code&gt; for &lt;code&gt;dimension != dim&lt;/code&gt; and by the corresponding value in &lt;code&gt;index&lt;/code&gt; for &lt;code&gt;dimension = dim&lt;/code&gt;.</source>
          <target state="translated">Additionally accepts an optional &lt;code&gt;reduce&lt;/code&gt; argument that allows specification of an optional reduction operation, which is applied to all values in the tensor &lt;code&gt;src&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; at the indicies specified in the &lt;code&gt;index&lt;/code&gt; . For each value in &lt;code&gt;src&lt;/code&gt; , the reduction operation is applied to an index in &lt;code&gt;self&lt;/code&gt; which is specified by its index in &lt;code&gt;src&lt;/code&gt; for &lt;code&gt;dimension != dim&lt;/code&gt; and by the corresponding value in &lt;code&gt;index&lt;/code&gt; for &lt;code&gt;dimension = dim&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="d9baa44eaf376e383c39686ddc11f7bd15a1ea88" translate="yes" xml:space="preserve">
          <source>Adds a buffer to the module.</source>
          <target state="translated">Adds a buffer to the module.</target>
        </trans-unit>
        <trans-unit id="677c1d2632a65843e9a01046b892b48b9f5176ac" translate="yes" xml:space="preserve">
          <source>Adds a child module to the current module.</source>
          <target state="translated">Adds a child module to the current module.</target>
        </trans-unit>
        <trans-unit id="96b77db46874289e2dd12b6bc9489ce00f46440a" translate="yes" xml:space="preserve">
          <source>Adds a child pruning &lt;code&gt;method&lt;/code&gt; to the container.</source>
          <target state="translated">Adds a child pruning &lt;code&gt;method&lt;/code&gt; to the container.</target>
        </trans-unit>
        <trans-unit id="f9bccaca3fe9322a4122d15d5e4fdb25202c0320" translate="yes" xml:space="preserve">
          <source>Adds a parameter to the module.</source>
          <target state="translated">Adds a parameter to the module.</target>
        </trans-unit>
        <trans-unit id="7fc7b19e96d1e48b76f041d88e6e8c8791a06fa6" translate="yes" xml:space="preserve">
          <source>Adds all values from the tensor &lt;code&gt;other&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; at the indices specified in the &lt;code&gt;index&lt;/code&gt; tensor in a similar fashion as &lt;a href=&quot;#torch.Tensor.scatter_&quot;&gt;&lt;code&gt;scatter_()&lt;/code&gt;&lt;/a&gt;. For each value in &lt;code&gt;src&lt;/code&gt;, it is added to an index in &lt;code&gt;self&lt;/code&gt; which is specified by its index in &lt;code&gt;src&lt;/code&gt; for &lt;code&gt;dimension != dim&lt;/code&gt; and by the corresponding value in &lt;code&gt;index&lt;/code&gt; for &lt;code&gt;dimension = dim&lt;/code&gt;.</source>
          <target state="translated">Adds all values from the tensor &lt;code&gt;other&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; at the indices specified in the &lt;code&gt;index&lt;/code&gt; tensor in a similar fashion as &lt;a href=&quot;#torch.Tensor.scatter_&quot;&gt; &lt;code&gt;scatter_()&lt;/code&gt; &lt;/a&gt;. For each value in &lt;code&gt;src&lt;/code&gt; , it is added to an index in &lt;code&gt;self&lt;/code&gt; which is specified by its index in &lt;code&gt;src&lt;/code&gt; for &lt;code&gt;dimension != dim&lt;/code&gt; and by the corresponding value in &lt;code&gt;index&lt;/code&gt; for &lt;code&gt;dimension = dim&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="8562c008b76c53309d3a0973a409f19d43babb11" translate="yes" xml:space="preserve">
          <source>Adds many scalar data to summary.</source>
          <target state="translated">Adds many scalar data to summary.</target>
        </trans-unit>
        <trans-unit id="8058a9f23e00e9bb14b6abf55b56e8a8543f9c29" translate="yes" xml:space="preserve">
          <source>Adds precision recall curve. Plotting a precision-recall curve lets you understand your model&amp;rsquo;s performance under different threshold settings. With this function, you provide the ground truth labeling (T/F) and prediction confidence (usually the output of your model) for each target. The TensorBoard UI will let you choose the threshold interactively.</source>
          <target state="translated">Adds precision recall curve. Plotting a precision-recall curve lets you understand your model&amp;rsquo;s performance under different threshold settings. With this function, you provide the ground truth labeling (T/F) and prediction confidence (usually the output of your model) for each target. The TensorBoard UI will let you choose the threshold interactively.</target>
        </trans-unit>
        <trans-unit id="fd54948ee53264250bb1b351262b4ccb8986d94e" translate="yes" xml:space="preserve">
          <source>Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.</source>
          <target state="translated">Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.</target>
        </trans-unit>
        <trans-unit id="ab685d43fc5e49c46e526f76f07f3d59e811972e" translate="yes" xml:space="preserve">
          <source>Adds the scalar &lt;code&gt;other&lt;/code&gt; to each element of the input &lt;code&gt;input&lt;/code&gt; and returns a new resulting tensor.</source>
          <target state="translated">Adds the scalar &lt;code&gt;other&lt;/code&gt; to each element of the input &lt;code&gt;input&lt;/code&gt; and returns a new resulting tensor.</target>
        </trans-unit>
        <trans-unit id="b2b204082818c243f404c5bfdc3dc16bbc32640c" translate="yes" xml:space="preserve">
          <source>After a class is defined, it can be used in both TorchScript and Python interchangeably like any other TorchScript type:</source>
          <target state="translated">After a class is defined, it can be used in both TorchScript and Python interchangeably like any other TorchScript type:</target>
        </trans-unit>
        <trans-unit id="cd74631d206c5c267345dcc56490ab6ea1c83f20" translate="yes" xml:space="preserve">
          <source>After an enum is defined, it can be used in both TorchScript and Python interchangeably like any other TorchScript type. The type of the values of an enum must be &lt;code&gt;int&lt;/code&gt;, &lt;code&gt;float&lt;/code&gt;, or &lt;code&gt;str&lt;/code&gt;. All values must be of the same type; heterogenous types for enum values are not supported.</source>
          <target state="translated">After an enum is defined, it can be used in both TorchScript and Python interchangeably like any other TorchScript type. The type of the values of an enum must be &lt;code&gt;int&lt;/code&gt; , &lt;code&gt;float&lt;/code&gt; , or &lt;code&gt;str&lt;/code&gt; . All values must be of the same type; heterogenous types for enum values are not supported.</target>
        </trans-unit>
        <trans-unit id="c52a739b9d98878e44d92e91256235a6d294431c" translate="yes" xml:space="preserve">
          <source>After the call &lt;code&gt;tensor&lt;/code&gt; is going to be bitwise identical in all processes.</source>
          <target state="translated">After the call &lt;code&gt;tensor&lt;/code&gt; is going to be bitwise identical in all processes.</target>
        </trans-unit>
        <trans-unit id="f3dd860a8580060d82d526ad04c5badbd57df94f" translate="yes" xml:space="preserve">
          <source>After the call, all 16 tensors on the two nodes will have the all-reduced value of 16</source>
          <target state="translated">After the call, all 16 tensors on the two nodes will have the all-reduced value of 16</target>
        </trans-unit>
        <trans-unit id="e03f01ec1cb89a34bde4816ebeb0fe756b9b7364" translate="yes" xml:space="preserve">
          <source>After the call, all &lt;code&gt;tensor&lt;/code&gt; in &lt;code&gt;tensor_list&lt;/code&gt; is going to be bitwise identical in all processes.</source>
          <target state="translated">After the call, all &lt;code&gt;tensor&lt;/code&gt; in &lt;code&gt;tensor_list&lt;/code&gt; is going to be bitwise identical in all processes.</target>
        </trans-unit>
        <trans-unit id="9ae05a9ce240652b1911853b65ddd62f9764383c" translate="yes" xml:space="preserve">
          <source>AlexNet</source>
          <target state="translated">AlexNet</target>
        </trans-unit>
        <trans-unit id="ca5a1956913984160d31d9bf92ec542323ba8065" translate="yes" xml:space="preserve">
          <source>AlexNet model architecture from the &lt;a href=&quot;https://arxiv.org/abs/1404.5997&quot;&gt;&amp;ldquo;One weird trick&amp;hellip;&amp;rdquo;&lt;/a&gt; paper.</source>
          <target state="translated">AlexNet model architecture from the &lt;a href=&quot;https://arxiv.org/abs/1404.5997&quot;&gt;&amp;ldquo;One weird trick&amp;hellip;&amp;rdquo;&lt;/a&gt; paper.</target>
        </trans-unit>
        <trans-unit id="52fc4196dd42d00268d82ad7f69953e231b88729" translate="yes" xml:space="preserve">
          <source>Alexnet</source>
          <target state="translated">Alexnet</target>
        </trans-unit>
        <trans-unit id="c4836f5ef10696c1523f7a542472a979fa86940f" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;#torch.Tensor.clamp&quot;&gt;&lt;code&gt;clamp()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;#torch.Tensor.clamp&quot;&gt; &lt;code&gt;clamp()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="251f8be41a3289fe0ff13ca86e83184aaa6d1685" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;#torch.Tensor.clamp_&quot;&gt;&lt;code&gt;clamp_()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;#torch.Tensor.clamp_&quot;&gt; &lt;code&gt;clamp_()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="8b1674d7e68b7ce19ae1eac32e5f2d61ba39a3fa" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;#torch.Tensor.dim&quot;&gt;&lt;code&gt;dim()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Alias for &lt;a href=&quot;#torch.Tensor.dim&quot;&gt; &lt;code&gt;dim()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="b806d3eb44bb5995d21b017162c333c30909bfd1" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;#torch.Tensor.numel&quot;&gt;&lt;code&gt;numel()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Alias for &lt;a href=&quot;#torch.Tensor.numel&quot;&gt; &lt;code&gt;numel()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="8003ce6f6a4824ac7e86312aee5a809819964394" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt;&lt;code&gt;abs()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt; &lt;code&gt;abs()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="ae0811ab2275dcf323f302291b8578862167855b" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt;&lt;code&gt;torch.abs()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt; &lt;code&gt;torch.abs()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="dff47f32b9bc388203bf7e97dfa096239e9aa4fa" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.acos#torch.acos&quot;&gt;&lt;code&gt;torch.acos()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.acos#torch.acos&quot;&gt; &lt;code&gt;torch.acos()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="27f71b5c7ea505c6db6e7c3f268907cac51983c8" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.acosh#torch.acosh&quot;&gt;&lt;code&gt;torch.acosh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.acosh#torch.acosh&quot;&gt; &lt;code&gt;torch.acosh()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="8915eae6d7b9ee1bf0b6f73f5bcf701208d83a87" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.asin#torch.asin&quot;&gt;&lt;code&gt;torch.asin()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.asin#torch.asin&quot;&gt; &lt;code&gt;torch.asin()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="12a1bcfe361d7554bc235392cdf072ae77235de7" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.asinh#torch.asinh&quot;&gt;&lt;code&gt;torch.asinh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.asinh#torch.asinh&quot;&gt; &lt;code&gt;torch.asinh()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="21de09801d295b9e55c5ca95a97e6b707bdd91c4" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.atan#torch.atan&quot;&gt;&lt;code&gt;torch.atan()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.atan#torch.atan&quot;&gt; &lt;code&gt;torch.atan()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="f42b48abe60c1061b77c3d37b15698be8fee9c90" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.atanh#torch.atanh&quot;&gt;&lt;code&gt;torch.atanh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.atanh#torch.atanh&quot;&gt; &lt;code&gt;torch.atanh()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="b913a66219f4d1a88ba91e403c72efb7f3ef3abb" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.clamp#torch.clamp&quot;&gt;&lt;code&gt;torch.clamp()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.clamp#torch.clamp&quot;&gt; &lt;code&gt;torch.clamp()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="4ba0bdfda926bfb5d618f4ee65334d6fb8eb1eee" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.div#torch.div&quot;&gt;&lt;code&gt;torch.div()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.div#torch.div&quot;&gt; &lt;code&gt;torch.div()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="346a30cebac1d8f7c67c1c3a7f4f3bc3613f3f76" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.ge#torch.ge&quot;&gt;&lt;code&gt;torch.ge()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.ge#torch.ge&quot;&gt; &lt;code&gt;torch.ge()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="b313b53bc520608d37513795d2fb79da1dd596f1" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.gt#torch.gt&quot;&gt;&lt;code&gt;torch.gt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.gt#torch.gt&quot;&gt; &lt;code&gt;torch.gt()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="fd6c12495884b27e735c199934fdc7b613fa577d" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.le#torch.le&quot;&gt;&lt;code&gt;torch.le()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.le#torch.le&quot;&gt; &lt;code&gt;torch.le()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="982d40969f8de3053b6a7ba2029aed4446d9284b" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.lt#torch.lt&quot;&gt;&lt;code&gt;torch.lt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.lt#torch.lt&quot;&gt; &lt;code&gt;torch.lt()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="2d9a228af380b06ff04ec79aacc440723157779f" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.mul#torch.mul&quot;&gt;&lt;code&gt;torch.mul()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.mul#torch.mul&quot;&gt; &lt;code&gt;torch.mul()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="93282bf10a2917c15fe551a51f5161e90b61877f" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.ne#torch.ne&quot;&gt;&lt;code&gt;torch.ne()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.ne#torch.ne&quot;&gt; &lt;code&gt;torch.ne()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="5c53bd5a9189ae9c36a9718c39849a71cf4795d9" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.neg#torch.neg&quot;&gt;&lt;code&gt;torch.neg()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">Alias for &lt;a href=&quot;generated/torch.neg#torch.neg&quot;&gt; &lt;code&gt;torch.neg()&lt;/code&gt; &lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="84a5bea4d6c5059fe64ceca353dc77a1a8d6f0c7" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.sub#torch.sub&quot;&gt;&lt;code&gt;torch.sub()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.sub#torch.sub&quot;&gt; &lt;code&gt;torch.sub()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="07f60a052509a04e4581fb6abfc90937c2c71d1e" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;generated/torch.trunc#torch.trunc&quot;&gt;&lt;code&gt;torch.trunc()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;generated/torch.trunc#torch.trunc&quot;&gt; &lt;code&gt;torch.trunc()&lt;/code&gt; &lt;/a&gt; 별칭</target>
        </trans-unit>
        <trans-unit id="d26757f4ad01130675ba8ea2672c12a2db5b8aab" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.abs#torch.abs&quot;&gt;&lt;code&gt;torch.abs()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;torch.abs#torch.abs&quot;&gt; &lt;code&gt;torch.abs()&lt;/code&gt; &lt;/a&gt; 별칭</target>
        </trans-unit>
        <trans-unit id="f1445190c40314f7662b67191fe5caef59adeec0" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.acos#torch.acos&quot;&gt;&lt;code&gt;torch.acos()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.acos#torch.acos&quot;&gt; &lt;code&gt;torch.acos()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="b7d58a750debce958ceee5c25ad01617693d3ea2" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.acosh#torch.acosh&quot;&gt;&lt;code&gt;torch.acosh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.acosh#torch.acosh&quot;&gt; &lt;code&gt;torch.acosh()&lt;/code&gt; &lt;/a&gt; 대한 별칭 .</target>
        </trans-unit>
        <trans-unit id="3aec840f94f18b783a10543e6249d4a95dd33cf1" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.asin#torch.asin&quot;&gt;&lt;code&gt;torch.asin()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.asin#torch.asin&quot;&gt; &lt;code&gt;torch.asin()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="606d0a5ac071a495928d8903629e0d3c2c53055d" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.asinh#torch.asinh&quot;&gt;&lt;code&gt;torch.asinh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.asinh#torch.asinh&quot;&gt; &lt;code&gt;torch.asinh()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="dee63cafa3f7f5b85ddbc8fc58438f4ee2396691" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.atan#torch.atan&quot;&gt;&lt;code&gt;torch.atan()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.atan#torch.atan&quot;&gt; &lt;code&gt;torch.atan()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="8c0014cc8ab298138389abb57c1b96765b473b42" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.atanh#torch.atanh&quot;&gt;&lt;code&gt;torch.atanh()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.atanh#torch.atanh&quot;&gt; &lt;code&gt;torch.atanh()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="23c4daa625a3c02d837a48a43fcee80a5cf020be" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.clamp#torch.clamp&quot;&gt;&lt;code&gt;torch.clamp()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.clamp#torch.clamp&quot;&gt; &lt;code&gt;torch.clamp()&lt;/code&gt; &lt;/a&gt; 대한 별칭 .</target>
        </trans-unit>
        <trans-unit id="2cc182b7bdf6fcacb622e0e829ca673368ffde7e" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.div#torch.div&quot;&gt;&lt;code&gt;torch.div()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.div#torch.div&quot;&gt; &lt;code&gt;torch.div()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="caa996e0ac1f8de7d67f7455447758713ba8feee" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.ge#torch.ge&quot;&gt;&lt;code&gt;torch.ge()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.ge#torch.ge&quot;&gt; &lt;code&gt;torch.ge()&lt;/code&gt; &lt;/a&gt; 대한 별칭 .</target>
        </trans-unit>
        <trans-unit id="392a801bbfecc8b02cebb4cb713410918e491a2f" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.gt#torch.gt&quot;&gt;&lt;code&gt;torch.gt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.gt#torch.gt&quot;&gt; &lt;code&gt;torch.gt()&lt;/code&gt; &lt;/a&gt; 대한 별칭 .</target>
        </trans-unit>
        <trans-unit id="9b13ac85fa2f8f4adaad80131c925608d0b684eb" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.le#torch.le&quot;&gt;&lt;code&gt;torch.le()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.le#torch.le&quot;&gt; &lt;code&gt;torch.le()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="6b6841b18d2b1421ff7068e9ab0cf0387e0beeda" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.lt#torch.lt&quot;&gt;&lt;code&gt;torch.lt()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.lt#torch.lt&quot;&gt; &lt;code&gt;torch.lt()&lt;/code&gt; &lt;/a&gt; 대한 별칭 .</target>
        </trans-unit>
        <trans-unit id="4bb2a8621505158fea7a3e7e83a0736217506d1f" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.mul#torch.mul&quot;&gt;&lt;code&gt;torch.mul()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.mul#torch.mul&quot;&gt; &lt;code&gt;torch.mul()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="3eaccfa1051feb57b0d0c06ee1f7f0ca18ed2f48" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.ne#torch.ne&quot;&gt;&lt;code&gt;torch.ne()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.ne#torch.ne&quot;&gt; &lt;code&gt;torch.ne()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="80bfac967c271e4287120446d7e735504a25deee" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.neg#torch.neg&quot;&gt;&lt;code&gt;torch.neg()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;torch.neg#torch.neg&quot;&gt; &lt;code&gt;torch.neg()&lt;/code&gt; &lt;/a&gt; 별칭</target>
        </trans-unit>
        <trans-unit id="63adb787982c4f010005f5042a7176ac57a5c3dd" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.sub#torch.sub&quot;&gt;&lt;code&gt;torch.sub()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.sub#torch.sub&quot;&gt; &lt;code&gt;torch.sub()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="004a1c4ca4c726826c71b8d3f3c668a30c19eca5" translate="yes" xml:space="preserve">
          <source>Alias for &lt;a href=&quot;torch.trunc#torch.trunc&quot;&gt;&lt;code&gt;torch.trunc()&lt;/code&gt;&lt;/a&gt;</source>
          <target state="translated">&lt;a href=&quot;torch.trunc#torch.trunc&quot;&gt; &lt;code&gt;torch.trunc()&lt;/code&gt; &lt;/a&gt; 별칭</target>
        </trans-unit>
        <trans-unit id="f7b4372e3de7ce07bac66d661704328db6f955a4" translate="yes" xml:space="preserve">
          <source>Alias for field number 0</source>
          <target state="translated">필드 번호 0의 ​​별명</target>
        </trans-unit>
        <trans-unit id="bbef6b362c3d3509157f18014e4e5a25eb4e07ea" translate="yes" xml:space="preserve">
          <source>Alias for field number 1</source>
          <target state="translated">필드 번호 1의 별명</target>
        </trans-unit>
        <trans-unit id="cb7d09e2006a3aec07c13d82496b6f2adb24a1f7" translate="yes" xml:space="preserve">
          <source>Alias for field number 2</source>
          <target state="translated">필드 번호 2의 별명</target>
        </trans-unit>
        <trans-unit id="2116d748feb69a3af8d3d3f32852bff649bb421e" translate="yes" xml:space="preserve">
          <source>Alias for field number 3</source>
          <target state="translated">필드 번호 3의 별명</target>
        </trans-unit>
        <trans-unit id="c8d021883b0a18d7d212863e386fe9e4590e884d" translate="yes" xml:space="preserve">
          <source>Alias of &lt;a href=&quot;generated/torch.det#torch.det&quot;&gt;&lt;code&gt;torch.det()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.det#torch.det&quot;&gt; &lt;code&gt;torch.det()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="50e05f8a73a607a4d8a0fae45b49062e73a2e308" translate="yes" xml:space="preserve">
          <source>Alias of &lt;a href=&quot;generated/torch.outer#torch.outer&quot;&gt;&lt;code&gt;torch.outer()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.outer#torch.outer&quot;&gt; &lt;code&gt;torch.outer()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="47e1617de19cb0d4a15c8e0fcabf777055dd3bb7" translate="yes" xml:space="preserve">
          <source>Alias of &lt;a href=&quot;torch.outer#torch.outer&quot;&gt;&lt;code&gt;torch.outer()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.outer#torch.outer&quot;&gt; &lt;code&gt;torch.outer()&lt;/code&gt; &lt;/a&gt; 별칭 .</target>
        </trans-unit>
        <trans-unit id="7d50263b9330ad372189eff67c5c8da79c613de2" translate="yes" xml:space="preserve">
          <source>All RNN modules accept packed sequences as inputs.</source>
          <target state="translated">모든 RNN 모듈은 패킹 된 시퀀스를 입력으로 받아들입니다.</target>
        </trans-unit>
        <trans-unit id="bcd80e614fd3639e154329146ec9d72471b9fa7b" translate="yes" xml:space="preserve">
          <source>All Tensors that have &lt;a href=&quot;autograd#torch.Tensor.requires_grad&quot;&gt;&lt;code&gt;requires_grad&lt;/code&gt;&lt;/a&gt; which is &lt;code&gt;False&lt;/code&gt; will be leaf Tensors by convention.</source>
          <target state="translated">이 모든 텐서 &lt;a href=&quot;autograd#torch.Tensor.requires_grad&quot;&gt; &lt;code&gt;requires_grad&lt;/code&gt; &lt;/a&gt; 입니다 &lt;code&gt;False&lt;/code&gt; 관례 잎 텐서됩니다.</target>
        </trans-unit>
        <trans-unit id="fae256c49cab7960ddc648f6bae5910cd2d3503e" translate="yes" xml:space="preserve">
          <source>All TorchVision models, except for quantized versions, are exportable to ONNX. More details can be found in &lt;a href=&quot;torchvision/models&quot;&gt;TorchVision&lt;/a&gt;.</source>
          <target state="translated">양자화 된 버전을 제외한 모든 TorchVision 모델은 ONNX로 내보낼 수 있습니다. 자세한 내용은 &lt;a href=&quot;torchvision/models&quot;&gt;TorchVision&lt;/a&gt; 에서 찾을 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="5dce89529d95dc69f4c1f1ca05bfc2111d81383b" translate="yes" xml:space="preserve">
          <source>All arguments are forwarded to the &lt;code&gt;setuptools.Extension&lt;/code&gt; constructor.</source>
          <target state="translated">모든 인수는 &lt;code&gt;setuptools.Extension&lt;/code&gt; 생성자 로 전달됩니다 .</target>
        </trans-unit>
        <trans-unit id="7c7fdd1a2c6981ce3b8e5aa9cb3e2ddd838235f0" translate="yes" xml:space="preserve">
          <source>All dimension names of &lt;code&gt;self&lt;/code&gt; must be present in &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt;. &lt;a href=&quot;#torch.Tensor.names&quot;&gt;&lt;code&gt;names&lt;/code&gt;&lt;/a&gt; may contain additional names that are not in &lt;code&gt;self.names&lt;/code&gt;; the output tensor has a size-one dimension for each of those new names.</source>
          <target state="translated">모든 차원 이름 &lt;code&gt;self&lt;/code&gt; 에 있어야합니다 &lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; &lt;/a&gt; . &lt;a href=&quot;#torch.Tensor.names&quot;&gt; &lt;code&gt;names&lt;/code&gt; &lt;/a&gt; 에는 &lt;code&gt;self.names&lt;/code&gt; 에 없는 추가 이름 이 포함될 수 있습니다 . 출력 텐서는 각각의 새 이름에 대해 크기가 1 차원입니다.</target>
        </trans-unit>
        <trans-unit id="1576ac96be05f9aa6b0251d2e542e4b0f488bd8b" translate="yes" xml:space="preserve">
          <source>All dimension names of &lt;code&gt;self&lt;/code&gt; must be present in &lt;code&gt;other.names&lt;/code&gt;. &lt;code&gt;other&lt;/code&gt; may contain named dimensions that are not in &lt;code&gt;self.names&lt;/code&gt;; the output tensor has a size-one dimension for each of those new names.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 의 모든 차원 이름은 &lt;code&gt;other.names&lt;/code&gt; 에 있어야합니다 . &lt;code&gt;other&lt;/code&gt; 는 &lt;code&gt;self.names&lt;/code&gt; 에 없는 명명 된 차원을 포함 할 수 있습니다 . 출력 텐서는 각각의 새 이름에 대해 크기가 1 차원입니다.</target>
        </trans-unit>
        <trans-unit id="91188b536d6c07689225e8b91d26aa659eef6d49" translate="yes" xml:space="preserve">
          <source>All elements must be greater than</source>
          <target state="translated">모든 요소는 다음보다 커야합니다.</target>
        </trans-unit>
        <trans-unit id="fa3a1109ed903de03a2630b0856e9124664ffc41" translate="yes" xml:space="preserve">
          <source>All functions must be valid TorchScript functions (including &lt;code&gt;__init__()&lt;/code&gt;).</source>
          <target state="translated">모든 함수는 유효한 TorchScript 함수 여야합니다 ( &lt;code&gt;__init__()&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="8102d7ae18041c98a7f876840a19500755dde277" translate="yes" xml:space="preserve">
          <source>All modules, no matter their device, are always loaded onto the CPU during loading. This is different from &lt;a href=&quot;torch.load#torch.load&quot;&gt;&lt;code&gt;torch.load()&lt;/code&gt;&lt;/a&gt;&amp;rsquo;s semantics and may change in the future.</source>
          <target state="translated">장치에 관계없이 모든 모듈은로드하는 동안 항상 CPU에로드됩니다. 이것은 &lt;a href=&quot;torch.load#torch.load&quot;&gt; &lt;code&gt;torch.load()&lt;/code&gt; &lt;/a&gt; 의 의미와 다르며 향후 변경 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="391810af4c5adb65a3bbe96665345a54a154a4ff" translate="yes" xml:space="preserve">
          <source>All of &lt;code&gt;dims&lt;/code&gt; must be consecutive in order in the &lt;code&gt;self&lt;/code&gt; tensor, but not necessary contiguous in memory.</source>
          <target state="translated">모든 &lt;code&gt;dims&lt;/code&gt; 은 &lt;code&gt;self&lt;/code&gt; 텐서 에서 순서대로 연속되어야 하지만 메모리에서 연속적 일 필요는 없습니다.</target>
        </trans-unit>
        <trans-unit id="6c24f14499bb0bd88c18f05a4fe7875666f28f6d" translate="yes" xml:space="preserve">
          <source>All of the dims of &lt;code&gt;self&lt;/code&gt; must be named in order to use this method. The resulting tensor is a view on the original tensor.</source>
          <target state="translated">이 방법을 사용하려면 &lt;code&gt;self&lt;/code&gt; 의 모든 희미한 이름을 지정해야합니다. 결과 텐서는 원래 텐서의 뷰입니다.</target>
        </trans-unit>
        <trans-unit id="8d9325f5f71bde68b50e39d98dd2b2fc4b9b3f89" translate="yes" xml:space="preserve">
          <source>All operations that support named tensors propagate names.</source>
          <target state="translated">명명 된 텐서를 지원하는 모든 작업은 이름을 전파합니다.</target>
        </trans-unit>
        <trans-unit id="c773fd48da7cc8c1f4621a11671c67f1f6303a0c" translate="yes" xml:space="preserve">
          <source>All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using &lt;code&gt;mean = [0.485, 0.456, 0.406]&lt;/code&gt; and &lt;code&gt;std = [0.229, 0.224, 0.225]&lt;/code&gt;. You can use the following transform to normalize:</source>
          <target state="translated">모든 사전 훈련 된 모델은 동일한 방식으로 정규화 된 입력 이미지를 기대합니다. [0, 1] 범위로로드 된 다음 &lt;code&gt;mean = [0.485, 0.456, 0.406]&lt;/code&gt; 및 &lt;code&gt;std = [0.229, 0.224, 0.225]&lt;/code&gt; 사용하여 정규화됩니다 . 다음 변환을 사용하여 정규화 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="ec2e3e9eabe115169e1deab63808ca48a84df052" translate="yes" xml:space="preserve">
          <source>All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB videos of shape (3 x T x H x W), where H and W are expected to be 112, and T is a number of video frames in a clip. The images have to be loaded in to a range of [0, 1] and then normalized using &lt;code&gt;mean = [0.43216, 0.394666, 0.37645]&lt;/code&gt; and &lt;code&gt;std = [0.22803, 0.22145, 0.216989]&lt;/code&gt;.</source>
          <target state="translated">사전 훈련 된 모든 모델은 동일한 방식으로 정규화 된 입력 이미지를 기대합니다. 즉, H와 W는 112가 될 것으로 예상되는 형태 (3 x T x H x W)의 3 채널 RGB 비디오 미니 배치, T는 클립의 비디오 프레임 수. 이미지는 [0, 1] 범위로로드 한 다음 &lt;code&gt;mean = [0.43216, 0.394666, 0.37645]&lt;/code&gt; 및 &lt;code&gt;std = [0.22803, 0.22145, 0.216989]&lt;/code&gt; 사용하여 정규화해야 합니다.</target>
        </trans-unit>
        <trans-unit id="7463404aa8619413edebf6ff4257cb99add42cba" translate="yes" xml:space="preserve">
          <source>All previously saved modules, no matter their device, are first loaded onto CPU, and then are moved to the devices they were saved from. If this fails (e.g. because the run time system doesn&amp;rsquo;t have certain devices), an exception is raised.</source>
          <target state="translated">이전에 저장 한 모든 모듈은 장치에 관계없이 먼저 CPU에로드 된 다음 저장된 장치로 이동됩니다. 이것이 실패하면 (예 : 런타임 시스템에 특정 장치가 없기 때문에) 예외가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="f4fb70a59932061a3d80fc551911560fa69384f6" translate="yes" xml:space="preserve">
          <source>All summed &lt;code&gt;dim&lt;/code&gt; are squeezed (see &lt;a href=&quot;generated/torch.squeeze#torch.squeeze&quot;&gt;&lt;code&gt;torch.squeeze()&lt;/code&gt;&lt;/a&gt;), resulting an output tensor having &lt;code&gt;dim&lt;/code&gt; fewer dimensions than &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">모든 합산 된 &lt;code&gt;dim&lt;/code&gt; 이 압축되어 ( &lt;a href=&quot;generated/torch.squeeze#torch.squeeze&quot;&gt; &lt;code&gt;torch.squeeze()&lt;/code&gt; &lt;/a&gt; 참조 ) 출력 텐서 가 &lt;code&gt;input&lt;/code&gt; 보다 &lt;code&gt;dim&lt;/code&gt; 더 적은 차원을 갖게 됩니다.</target>
        </trans-unit>
        <trans-unit id="6bd30243e5faf7879695a6c25ee7d928afafccc6" translate="yes" xml:space="preserve">
          <source>All tensors need to be of the same size.</source>
          <target state="translated">모든 텐서는 크기가 같아야합니다.</target>
        </trans-unit>
        <trans-unit id="647c455337c3b30ca33d7810c4e9c4f5a9641226" translate="yes" xml:space="preserve">
          <source>All the weights and biases are initialized from</source>
          <target state="translated">모든 가중치와 편향은 다음에서 초기화됩니다.</target>
        </trans-unit>
        <trans-unit id="970c0ae90b9eb34e0cb6a356cf47964705868146" translate="yes" xml:space="preserve">
          <source>Allows the model to jointly attend to information from different representation subspaces.</source>
          <target state="translated">모델이 다른 표현 부분 공간의 정보에 공동으로 참여할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="5702cc92d2bcf7e780dcdf99f53eee4b5e88684e" translate="yes" xml:space="preserve">
          <source>Allows the model to jointly attend to information from different representation subspaces. See reference: Attention Is All You Need</source>
          <target state="translated">모델이 다른 표현 부분 공간의 정보에 공동으로 참여할 수 있습니다. 참조 참조 :주의가 필요한 모든 것</target>
        </trans-unit>
        <trans-unit id="bcb917dff27ba02ff781ce514dcf9c4e2df73ba5" translate="yes" xml:space="preserve">
          <source>Alpha Dropout is a type of Dropout that maintains the self-normalizing property. For an input with zero mean and unit standard deviation, the output of Alpha Dropout maintains the original mean and standard deviation of the input. Alpha Dropout goes hand-in-hand with SELU activation function, which ensures that the outputs have zero mean and unit standard deviation.</source>
          <target state="translated">Alpha Dropout은 자체 정규화 속성을 유지하는 Dropout 유형입니다. 평균이 0이고 단위 표준 편차가있는 입력의 경우 알파 드롭 아웃의 출력은 입력의 원래 평균과 표준 편차를 유지합니다. Alpha Dropout은 SELU 활성화 기능과 함께 사용되어 출력의 평균 및 단위 표준 편차가 0이되도록합니다.</target>
        </trans-unit>
        <trans-unit id="468dc142cbb278c344ed57a0c2341d7dabf6044d" translate="yes" xml:space="preserve">
          <source>AlphaDropout</source>
          <target state="translated">AlphaDropout</target>
        </trans-unit>
        <trans-unit id="697ae46b13c17cf9f965bf8eeb7388f2f2599380" translate="yes" xml:space="preserve">
          <source>Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default &lt;code&gt;momentum&lt;/code&gt; of 0.1.</source>
          <target state="translated">또한 기본적으로 훈련 중에이 계층은 계산 된 평균 및 분산의 추정치를 계속 실행하며, 평가 중에 정규화에 사용됩니다. 실행 추정치는 기본 &lt;code&gt;momentum&lt;/code&gt; 0.1 로 유지됩니다 .</target>
        </trans-unit>
        <trans-unit id="8bf20b1915a4ba9029b2263ed8aac08cdcf55753" translate="yes" xml:space="preserve">
          <source>Also functions as a decorator. (Make sure to instantiate with parenthesis.)</source>
          <target state="translated">장식 자 역할도합니다. (괄호로 인스턴스화해야합니다.)</target>
        </trans-unit>
        <trans-unit id="51924beeca0c98eba05c19c63804e09424b9e355" translate="yes" xml:space="preserve">
          <source>Also known as Glorot initialization.</source>
          <target state="translated">Glorot 초기화라고도합니다.</target>
        </trans-unit>
        <trans-unit id="99d08174dc1487c12f409bf77bbf4a6a16dc84ad" translate="yes" xml:space="preserve">
          <source>Also known as He initialization.</source>
          <target state="translated">He 초기화라고도합니다.</target>
        </trans-unit>
        <trans-unit id="9319f57915da9aa68da62d35edfd64135450d0d5" translate="yes" xml:space="preserve">
          <source>Also note that &lt;code&gt;len(input_tensor_lists)&lt;/code&gt;, and the size of each element in &lt;code&gt;input_tensor_lists&lt;/code&gt; (each element is a list, therefore &lt;code&gt;len(input_tensor_lists[i])&lt;/code&gt;) need to be the same for all the distributed processes calling this function.</source>
          <target state="translated">또한 유의 &lt;code&gt;len(input_tensor_lists)&lt;/code&gt; 와 각 요소의 크기 &lt;code&gt;input_tensor_lists&lt;/code&gt; (각 요소 목록이며, 따라서 &lt;code&gt;len(input_tensor_lists[i])&lt;/code&gt; )이 함수를 호출 모든 분산 프로세스에 대해 동일해야한다.</target>
        </trans-unit>
        <trans-unit id="3a0c3f7b9176b92a14d4b6b5cc0f2c401a31947c" translate="yes" xml:space="preserve">
          <source>Also note that &lt;code&gt;len(output_tensor_lists)&lt;/code&gt;, and the size of each element in &lt;code&gt;output_tensor_lists&lt;/code&gt; (each element is a list, therefore &lt;code&gt;len(output_tensor_lists[i])&lt;/code&gt;) need to be the same for all the distributed processes calling this function.</source>
          <target state="translated">또한 유의 &lt;code&gt;len(output_tensor_lists)&lt;/code&gt; 와 각 요소의 크기 &lt;code&gt;output_tensor_lists&lt;/code&gt; (각 요소 목록이며, 따라서 &lt;code&gt;len(output_tensor_lists[i])&lt;/code&gt; )이 함수를 호출 모든 분산 프로세스에 대해 동일해야한다.</target>
        </trans-unit>
        <trans-unit id="043751522414e7c9aaee8aec5948d07968c78350" translate="yes" xml:space="preserve">
          <source>Although the recipe for forward pass needs to be defined within this function, one should call the &lt;a href=&quot;#torch.nn.Module&quot;&gt;&lt;code&gt;Module&lt;/code&gt;&lt;/a&gt; instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.</source>
          <target state="translated">포워드 패스를위한 레시피는이 함수 내에서 정의되어야 하지만, 전자는 등록 된 후크를 실행하고 후자는 자동으로 무시하므로 나중에 &lt;a href=&quot;#torch.nn.Module&quot;&gt; &lt;code&gt;Module&lt;/code&gt; &lt;/a&gt; 인스턴스를 호출해야 합니다.</target>
        </trans-unit>
        <trans-unit id="a16ed36f1048b1b7e767b2e1655943a9aa7dfbb6" translate="yes" xml:space="preserve">
          <source>An &lt;code&gt;RRef&lt;/code&gt; (Remote REFerence) is a reference to a value of some type &lt;code&gt;T&lt;/code&gt; (e.g. &lt;code&gt;Tensor&lt;/code&gt;) on a remote worker. This handle keeps the referenced remote value alive on the owner, but there is no implication that the value will be transferred to the local worker in the future. RRefs can be used in multi-machine training by holding references to &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.Module&quot;&gt;nn.Modules&lt;/a&gt; that exist on other workers, and calling the appropriate functions to retrieve or modify their parameters during training. See &lt;a href=&quot;rpc/rref#remote-reference-protocol&quot;&gt;Remote Reference Protocol&lt;/a&gt; for more details.</source>
          <target state="translated">&lt;code&gt;RRef&lt;/code&gt; (원격 참조)는 어떤 유형의 값에 대한 참조 인 &lt;code&gt;T&lt;/code&gt; (예 &lt;code&gt;Tensor&lt;/code&gt; 원격 작업자). 이 핸들은 참조 된 원격 값을 소유자에게 유지하지만 나중에 값이 로컬 작업자에게 전송된다는 의미는 없습니다. RRef는 다른 워커에 존재 하는 &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.Module&quot;&gt;nn.Modules에&lt;/a&gt; 대한 참조를 보유 하고 적절한 함수를 호출하여 훈련 중에 매개 변수를 검색하거나 수정 함으로써 다중 머신 훈련에 사용할 수 있습니다 . 자세한 내용은 &lt;a href=&quot;rpc/rref#remote-reference-protocol&quot;&gt;원격 참조 프로토콜&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="366e57ddf274ce33b825366c8f324bc7720a8851" translate="yes" xml:space="preserve">
          <source>An Elman RNN cell with tanh or ReLU non-linearity.</source>
          <target state="translated">tanh 또는 ReLU 비선형 성이있는 Elman RNN 셀.</target>
        </trans-unit>
        <trans-unit id="cda80fd86e845c48c7faf0e81a98aca3de57ab49" translate="yes" xml:space="preserve">
          <source>An abstract structure encapsulating the options passed into the RPC backend. An instance of this class can be passed in to &lt;a href=&quot;#torch.distributed.rpc.init_rpc&quot;&gt;&lt;code&gt;init_rpc()&lt;/code&gt;&lt;/a&gt; in order to initialize RPC with specific configurations, such as the RPC timeout and &lt;code&gt;init_method&lt;/code&gt; to be used.</source>
          <target state="translated">RPC 백엔드로 전달 된 옵션을 캡슐화하는 추상 구조입니다. 이 클래스의 인스턴스는 사용할 RPC 시간 제한 및 &lt;code&gt;init_method&lt;/code&gt; 와 같은 특정 구성으로 RPC를 초기화하기 위해 &lt;a href=&quot;#torch.distributed.rpc.init_rpc&quot;&gt; &lt;code&gt;init_rpc()&lt;/code&gt; &lt;/a&gt; 에 전달할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="34f08a969a81be9fad81ba44cfa1144a898067e4" translate="yes" xml:space="preserve">
          <source>An additional dimension of size &lt;a href=&quot;#torch.Tensor.size&quot;&gt;&lt;code&gt;size&lt;/code&gt;&lt;/a&gt; is appended in the returned tensor.</source>
          <target state="translated">크기 &lt;a href=&quot;#torch.Tensor.size&quot;&gt; &lt;code&gt;size&lt;/code&gt; &lt;/a&gt; 의 추가 차원이 반환 된 텐서에 추가됩니다.</target>
        </trans-unit>
        <trans-unit id="619ad847534358910127729afc029b7953abcecf" translate="yes" xml:space="preserve">
          <source>An empty dict is assumed have type &lt;code&gt;Dict[str, Tensor]&lt;/code&gt;. The types of other dict literals are derived from the type of the members. See &lt;a href=&quot;#default-types&quot;&gt;Default Types&lt;/a&gt; for more details.</source>
          <target state="translated">빈 dict는 &lt;code&gt;Dict[str, Tensor]&lt;/code&gt; 유형을 갖는다 고 가정 합니다. 다른 dict 리터럴의 유형은 멤버 유형에서 파생됩니다. 자세한 내용은 &lt;a href=&quot;#default-types&quot;&gt;기본 유형&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="c1a0eecf8a5a6cdc72f4e0da994b510481da5a4d" translate="yes" xml:space="preserve">
          <source>An empty list is assumed have type &lt;code&gt;List[Tensor]&lt;/code&gt;. The types of other list literals are derived from the type of the members. See &lt;a href=&quot;#default-types&quot;&gt;Default Types&lt;/a&gt; for more details.</source>
          <target state="translated">빈 목록은 &lt;code&gt;List[Tensor]&lt;/code&gt; 유형이 있다고 가정 합니다. 다른 목록 리터럴의 유형은 멤버 유형에서 파생됩니다. 자세한 내용은 &lt;a href=&quot;#default-types&quot;&gt;기본 유형&lt;/a&gt; 을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="d84c564cad7cf9fb4e3caa453c3a49e8beeae54d" translate="yes" xml:space="preserve">
          <source>An empty list is assumed to be &lt;code&gt;List[Tensor]&lt;/code&gt; and empty dicts &lt;code&gt;Dict[str, Tensor]&lt;/code&gt;. To instantiate an empty list or dict of other types, use &lt;code&gt;Python 3 type hints&lt;/code&gt;.</source>
          <target state="translated">빈 목록은 &lt;code&gt;List[Tensor]&lt;/code&gt; 이고 빈 dicts &lt;code&gt;Dict[str, Tensor]&lt;/code&gt; 라고 가정합니다 . 빈 목록 또는 다른 유형의 사전을 인스턴스화하려면 &lt;code&gt;Python 3 type hints&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="fc24c0a7e765c257ec2c1deb32cf9b9b1bc99fd5" translate="yes" xml:space="preserve">
          <source>An empty sparse tensor can be constructed by specifying its size:</source>
          <target state="translated">빈 희소 텐서는 크기를 지정하여 생성 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="17f91bb7684b57d410e71c667849b986b1e265c6" translate="yes" xml:space="preserve">
          <source>An enum class of available backends.</source>
          <target state="translated">사용 가능한 백엔드의 열거 형 클래스입니다.</target>
        </trans-unit>
        <trans-unit id="a60c11218c3c0643831cc9fba424f6f68723b504" translate="yes" xml:space="preserve">
          <source>An enum-like class for available reduction operations: &lt;code&gt;SUM&lt;/code&gt;, &lt;code&gt;PRODUCT&lt;/code&gt;, &lt;code&gt;MIN&lt;/code&gt;, &lt;code&gt;MAX&lt;/code&gt;, &lt;code&gt;BAND&lt;/code&gt;, &lt;code&gt;BOR&lt;/code&gt;, and &lt;code&gt;BXOR&lt;/code&gt;.</source>
          <target state="translated">사용 가능한 축소 작업에 대한 열거 형 클래스 : &lt;code&gt;SUM&lt;/code&gt; , &lt;code&gt;PRODUCT&lt;/code&gt; , &lt;code&gt;MIN&lt;/code&gt; , &lt;code&gt;MAX&lt;/code&gt; , &lt;code&gt;BAND&lt;/code&gt; , &lt;code&gt;BOR&lt;/code&gt; 및 &lt;code&gt;BXOR&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c5d6691fbcb10d6ab33410657c5bcc031006a7f4" translate="yes" xml:space="preserve">
          <source>An enum-like class of available backends: GLOO, NCCL, MPI, and other registered backends.</source>
          <target state="translated">사용 가능한 백엔드의 열거 형 클래스 : GLOO, NCCL, MPI 및 기타 등록 된 백엔드.</target>
        </trans-unit>
        <trans-unit id="519ac03d4f41ca6837697ea6c98bf8c921545893" translate="yes" xml:space="preserve">
          <source>An example of such normalization can be found in the imagenet example &lt;a href=&quot;https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L101&quot;&gt;here&lt;/a&gt;</source>
          <target state="translated">이러한 정규화의 예는 &lt;a href=&quot;https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L101&quot;&gt;여기&lt;/a&gt; 이미지 넷 예 에서 찾을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="44802d0d5256af16bcf81baadaa0893f0644e7bc" translate="yes" xml:space="preserve">
          <source>An integral output tensor cannot accept a floating point tensor.</source>
          <target state="translated">적분 출력 텐서는 부동 소수점 텐서를 받아 들일 수 없습니다.</target>
        </trans-unit>
        <trans-unit id="ada1c75faa68087ab2e8ec52c71f9a5f98fa8946" translate="yes" xml:space="preserve">
          <source>An torch.Generator object.</source>
          <target state="translated">torch.Generator 개체입니다.</target>
        </trans-unit>
        <trans-unit id="ef94106a43404d867533b831248eaa8963699ea0" translate="yes" xml:space="preserve">
          <source>Another initialization method makes use of a file system that is shared and visible from all machines in a group, along with a desired &lt;code&gt;world_size&lt;/code&gt;. The URL should start with &lt;code&gt;file://&lt;/code&gt; and contain a path to a non-existent file (in an existing directory) on a shared file system. File-system initialization will automatically create that file if it doesn&amp;rsquo;t exist, but will not delete the file. Therefore, it is your responsibility to make sure that the file is cleaned up before the next &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; call on the same file path/name.</source>
          <target state="translated">또 다른 초기화 방법은 원하는 &lt;code&gt;world_size&lt;/code&gt; 와 함께 그룹의 모든 시스템에서 공유되고 볼 수있는 파일 시스템을 사용합니다 . URL은 &lt;code&gt;file://&lt;/code&gt; 로 시작해야 하며 공유 파일 시스템에 존재하지 않는 파일 (기존 디렉토리에 있음)에 대한 경로를 포함 해야 합니다. 파일 시스템 초기화는 파일이 존재하지 않는 경우 자동으로 생성되지만 파일을 삭제하지는 않습니다. 따라서 동일한 파일 경로 / 이름에 대한 다음 &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;init_process_group()&lt;/code&gt; &lt;/a&gt; 호출 전에 파일이 정리되었는지 확인하는 것은 사용자의 책임 입니다.</target>
        </trans-unit>
        <trans-unit id="1eec009a2337c1a488dd1b74c1aff7cc1d139bf1" translate="yes" xml:space="preserve">
          <source>Any other functionality from the &lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt;&lt;code&gt;typing&lt;/code&gt;&lt;/a&gt; module not explitily listed in this documentation is unsupported.</source>
          <target state="translated">이 문서에 명시 적으로 나열되지 않은 &lt;a href=&quot;https://docs.python.org/3/library/typing.html#module-typing&quot;&gt; &lt;code&gt;typing&lt;/code&gt; &lt;/a&gt; 모듈의 다른 기능 은 지원되지 않습니다.</target>
        </trans-unit>
        <trans-unit id="9267fbcc5ac03c112f491ee9bc7bbb21bb5ae1c5" translate="yes" xml:space="preserve">
          <source>Append the given callback function to this &lt;code&gt;Future&lt;/code&gt;, which will be run when the &lt;code&gt;Future&lt;/code&gt; is completed. Multiple callbacks can be added to the same &lt;code&gt;Future&lt;/code&gt;, and will be invoked in the same order as they were added. The callback must take one argument, which is the reference to this &lt;code&gt;Future&lt;/code&gt;. The callback function can use the &lt;code&gt;Future.wait()&lt;/code&gt; API to get the value.</source>
          <target state="translated">&lt;code&gt;Future&lt;/code&gt; 가 완료되면 실행될 이 &lt;code&gt;Future&lt;/code&gt; 에 주어진 콜백 함수를 추가합니다 . 동일한 &lt;code&gt;Future&lt;/code&gt; 에 여러 콜백을 추가 할 수 있으며 추가 된 것과 동일한 순서로 호출됩니다. 콜백은이 &lt;code&gt;Future&lt;/code&gt; 에 대한 참조 인 하나의 인수를 가져야합니다 . 콜백 함수는 &lt;code&gt;Future.wait()&lt;/code&gt; API를 사용하여 값을 가져올 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="f3f60044b06335eb6c4230e3c39f5e0218445878" translate="yes" xml:space="preserve">
          <source>Appendix</source>
          <target state="translated">Appendix</target>
        </trans-unit>
        <trans-unit id="c87f37c9f3693b628e726dc6f101341b952d29db" translate="yes" xml:space="preserve">
          <source>Appends a given module to the end of the list.</source>
          <target state="translated">목록 끝에 주어진 모듈을 추가합니다.</target>
        </trans-unit>
        <trans-unit id="379011982516fbdbffaea6138d56d674a94f5e18" translate="yes" xml:space="preserve">
          <source>Appends a given parameter at the end of the list.</source>
          <target state="translated">목록 끝에 주어진 매개 변수를 추가합니다.</target>
        </trans-unit>
        <trans-unit id="9b249ec6f74bf6c17017438ecc23770483a688cc" translate="yes" xml:space="preserve">
          <source>Appends modules from a Python iterable to the end of the list.</source>
          <target state="translated">Python iterable의 모듈을 목록 끝에 추가합니다.</target>
        </trans-unit>
        <trans-unit id="0e68b1f2c86ef40318501ff9b078b446cee7c813" translate="yes" xml:space="preserve">
          <source>Appends parameters from a Python iterable to the end of the list.</source>
          <target state="translated">Python iterable의 매개 변수를 목록 끝에 추가합니다.</target>
        </trans-unit>
        <trans-unit id="378aa8a7eda72c52086938707b31878d8a635ee4" translate="yes" xml:space="preserve">
          <source>Applied element-wise, as:</source>
          <target state="translated">다음과 같이 요소별로 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="47ea67db7d24ea56bb6efc03577734231d9fa58e" translate="yes" xml:space="preserve">
          <source>Applies 2D average-pooling operation in</source>
          <target state="translated">2D 평균 풀링 작업을</target>
        </trans-unit>
        <trans-unit id="26a3eb8c50568931de48e1e7fa354baa87e37c44" translate="yes" xml:space="preserve">
          <source>Applies 3D average-pooling operation in</source>
          <target state="translated">3D 평균 풀링 작업을</target>
        </trans-unit>
        <trans-unit id="897491bacd3642b7742fd85de111604a2b1e2139" translate="yes" xml:space="preserve">
          <source>Applies &lt;code&gt;callable&lt;/code&gt; for each element in &lt;code&gt;self&lt;/code&gt; tensor and the given &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; and stores the results in &lt;code&gt;self&lt;/code&gt; tensor. &lt;code&gt;self&lt;/code&gt; tensor and the given &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 텐서 및 주어진 &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; 각 요소에 대해 &lt;code&gt;callable&lt;/code&gt; 을 적용 하고 결과를 &lt;code&gt;self&lt;/code&gt; 텐서 에 저장합니다 . &lt;code&gt;self&lt;/code&gt; 텐서 및 주어진 &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; 는 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;브로드 캐스팅 가능&lt;/a&gt; 해야합니다 .</target>
        </trans-unit>
        <trans-unit id="854151a01bba98f938fa936327548dc77fb47b3f" translate="yes" xml:space="preserve">
          <source>Applies &lt;code&gt;fn&lt;/code&gt; recursively to every submodule (as returned by &lt;code&gt;.children()&lt;/code&gt;) as well as self. Typical use includes initializing the parameters of a model (see also &lt;a href=&quot;../nn.init#nn-init-doc&quot;&gt;torch.nn.init&lt;/a&gt;).</source>
          <target state="translated">self뿐만 아니라 모든 하위 모듈 ( &lt;code&gt;.children()&lt;/code&gt; 의해 반환 됨 )에 &lt;code&gt;fn&lt;/code&gt; 을 재귀 적으로 적용 합니다. 일반적인 사용에는 모델의 매개 변수 초기화가 포함됩니다 ( &lt;a href=&quot;../nn.init#nn-init-doc&quot;&gt;torch.nn.init&lt;/a&gt; 참조 ).</target>
        </trans-unit>
        <trans-unit id="f3ce1d7a64849414112d779b273ff91d45596516" translate="yes" xml:space="preserve">
          <source>Applies Alpha Dropout over the input.</source>
          <target state="translated">입력에 알파 드롭 아웃을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="13e7aada5fd7c3d78fcf6b9c0d54d77028725d8c" translate="yes" xml:space="preserve">
          <source>Applies Batch Normalization for each channel across a batch of data.</source>
          <target state="translated">데이터 배치에서 각 채널에 대해 배치 정규화를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="29f9114f73a8742a5df31f7a4732b3ef2d2fa5ed" translate="yes" xml:space="preserve">
          <source>Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; .</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; 논문에 설명 된대로 2D 또는 3D 입력 (선택 사항 인 추가 채널 차원이있는 1D 입력의 미니 배치)에 배치 정규화를 적용합니다 .</target>
        </trans-unit>
        <trans-unit id="77df134b3c1dec023ae7933dc984fdf0b9a37d0b" translate="yes" xml:space="preserve">
          <source>Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; .</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; 논문에 설명 된대로 4D 입력 (추가 채널 차원이있는 2D 입력의 미니 배치)에 배치 정규화를 적용합니다 .</target>
        </trans-unit>
        <trans-unit id="d5008d35280faa701906a5c7bb40531dff590ddb" translate="yes" xml:space="preserve">
          <source>Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; .</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; 논문에 설명 된대로 5D 입력 (추가 채널 차원이있는 3D 입력의 미니 배치)에 배치 정규화를 적용합니다 .</target>
        </trans-unit>
        <trans-unit id="ebba8e6ee8c679b795f34839517e1330fe0f6cd0" translate="yes" xml:space="preserve">
          <source>Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; .</source>
          <target state="translated">문서 Batch Normalization &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt; 문서에 설명 된대로 N 차원 입력 (추가 채널 차원이있는 [N-2] D 입력의 미니 배치)에 배치 정규화를 적용합니다 .</target>
        </trans-unit>
        <trans-unit id="4c0b063a3d01c57a39497f55e2c1c93c68f9edde" translate="yes" xml:space="preserve">
          <source>Applies Group Normalization over a mini-batch of inputs as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1803.08494&quot;&gt;Group Normalization&lt;/a&gt;</source>
          <target state="translated">논문 &lt;a href=&quot;https://arxiv.org/abs/1803.08494&quot;&gt;그룹 정규화에&lt;/a&gt; 설명 된대로 입력의 미니 배치에 대해 그룹 정규화를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="6ddcab60a314dad6ac4e8205f6d0bbe42d633f04" translate="yes" xml:space="preserve">
          <source>Applies Instance Normalization for each channel in each data sample in a batch.</source>
          <target state="translated">일괄 적으로 각 데이터 샘플의 각 채널에 대해 인스턴스 정규화를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="8eba97c1edce80ebe5a8f2d8c4ea438567d2707c" translate="yes" xml:space="preserve">
          <source>Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;Instance Normalization: The Missing Ingredient for Fast Stylization&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;Instance Normalization : The Missing Ingredient for Fast Stylization&lt;/a&gt; 문서에 설명 된대로 3D 입력 (선택 사항 인 추가 채널 차원이있는 1D 입력의 미니 배치)에 인스턴스 정규화를 적용합니다 .</target>
        </trans-unit>
        <trans-unit id="1c1378db79ff8ef5bf285d891bdde624c5ed8210" translate="yes" xml:space="preserve">
          <source>Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;Instance Normalization: The Missing Ingredient for Fast Stylization&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;Instance Normalization : The Missing Ingredient for Fast Stylization&lt;/a&gt; 문서에 설명 된대로 4D 입력 (추가 채널 차원이있는 2D 입력의 미니 배치)에 인스턴스 정규화를 적용합니다 .</target>
        </trans-unit>
        <trans-unit id="331da363e42c71d65e839afe96ad5dce770a5331" translate="yes" xml:space="preserve">
          <source>Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;Instance Normalization: The Missing Ingredient for Fast Stylization&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1607.08022&quot;&gt;Instance Normalization : The Missing Ingredient for Fast Stylization&lt;/a&gt; 문서에 설명 된대로 5D 입력 (추가 채널 차원이있는 3D 입력의 미니 배치)에 인스턴스 정규화를 적용합니다 .</target>
        </trans-unit>
        <trans-unit id="b97189b49bbea1acf1c1f0194b9ab27dea9973b9" translate="yes" xml:space="preserve">
          <source>Applies Layer Normalization for last certain number of dimensions.</source>
          <target state="translated">마지막 특정 수의 차원에 대해 레이어 정규화를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="835e00e544806b9f13082cf66ead874e6019f00b" translate="yes" xml:space="preserve">
          <source>Applies Layer Normalization over a mini-batch of inputs as described in the paper &lt;a href=&quot;https://arxiv.org/abs/1607.06450&quot;&gt;Layer Normalization&lt;/a&gt;</source>
          <target state="translated">문서 레이어 정규화에 설명 된대로 입력의 미니 배치에 대해 &lt;a href=&quot;https://arxiv.org/abs/1607.06450&quot;&gt;레이어 정규화를 적용합니다.&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="aeb29e205e308a59093be0d4988fa5cd74bac086" translate="yes" xml:space="preserve">
          <source>Applies SoftMax over features to each spatial location.</source>
          <target state="translated">기능에 SoftMax를 각 공간 위치에 적용합니다.</target>
        </trans-unit>
        <trans-unit id="68727ddc338094e31b998534ceb0a7cd1e1f8321" translate="yes" xml:space="preserve">
          <source>Applies a 1D adaptive average pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 대해 1D 적응 형 평균 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="0d6edcf464a5fa2406d025e84b2ed153de9c75ee" translate="yes" xml:space="preserve">
          <source>Applies a 1D adaptive max pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 대해 1D 적응 형 최대 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="b8f3289d5daff81c1311cb6f2ca1629e42cafa4f" translate="yes" xml:space="preserve">
          <source>Applies a 1D average pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 1D 평균 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="3c27388a69cba70f821eaba1e90c8524f7a152d3" translate="yes" xml:space="preserve">
          <source>Applies a 1D convolution over a quantized 1D input composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 양자화 된 1D 입력에 1D 컨볼 루션을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="322ebd47033ac7c9c030a4056a89e60da48e7a7e" translate="yes" xml:space="preserve">
          <source>Applies a 1D convolution over a quantized input signal composed of several quantized input planes.</source>
          <target state="translated">여러 양자화 된 입력 평면으로 구성된 양자화 된 입력 신호에 1D 컨볼 루션을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="dcfb7d54383b0ff98c06cfb1f40c5d85baf1b8d7" translate="yes" xml:space="preserve">
          <source>Applies a 1D convolution over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 1D 회선을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="ba598dc972597d5f42702090e4175518b59bd582" translate="yes" xml:space="preserve">
          <source>Applies a 1D max pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 대해 1D 최대 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="6e7aac7c6517399757e4cbb358ae6fceeed56898" translate="yes" xml:space="preserve">
          <source>Applies a 1D power-average pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 1D 전력 평균 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="8bfc842196843fc0db2f2d55709b870df2c8219f" translate="yes" xml:space="preserve">
          <source>Applies a 1D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of &lt;code&gt;p&lt;/code&gt; is zero, the gradient is set to zero as well.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 1D 전력 평균 풀링을 적용합니다. &lt;code&gt;p&lt;/code&gt; 의 거듭 제곱에 대한 모든 입력의 합 이 0이면 기울기도 0으로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="96ca07ea2e411b1ba9e96092e82956074c9632be" translate="yes" xml:space="preserve">
          <source>Applies a 1D transposed convolution operator over an input image composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 이미지에 1D 전치 컨볼 루션 연산자를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="4b460f2239dd693e787bc347edd22db345631a88" translate="yes" xml:space="preserve">
          <source>Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called &amp;ldquo;deconvolution&amp;rdquo;.</source>
          <target state="translated">&quot;디컨 볼 루션&quot;이라고도하는 여러 입력 평면으로 구성된 입력 신호에 대해 1D 전치 컨볼 루션 연산자를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="aa89da022829347e9eb20aeec1fec7371b510238" translate="yes" xml:space="preserve">
          <source>Applies a 2D adaptive average pooling over a quantized input signal composed of several quantized input planes.</source>
          <target state="translated">여러 양자화 된 입력 평면으로 구성된 양자화 된 입력 신호에 2D 적응 형 평균 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="2990176eebf3f395560769ef6ba618123ba3cbdc" translate="yes" xml:space="preserve">
          <source>Applies a 2D adaptive average pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 2D 적응 형 평균 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="8f9f8906e50a1a40da69b6542725773d5f97b476" translate="yes" xml:space="preserve">
          <source>Applies a 2D adaptive max pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 2D 적응 형 최대 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="d576b7f4a24ec8fd4a5244faa85b001a714e4b2b" translate="yes" xml:space="preserve">
          <source>Applies a 2D average pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 2D 평균 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="4eef15eebd1e3c58ec128ae4ac04428c46e00ff5" translate="yes" xml:space="preserve">
          <source>Applies a 2D bilinear upsampling to an input signal composed of several input channels.</source>
          <target state="translated">여러 입력 채널로 구성된 입력 신호에 2D 이중 선형 업 샘플링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="47468ad50007cdb83f448361561cc024fef32584" translate="yes" xml:space="preserve">
          <source>Applies a 2D convolution over a quantized 2D input composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 양자화 된 2D 입력에 2D 컨볼 루션을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="3f8605d5715138e74b1fe2f63f6110cb337b8e9e" translate="yes" xml:space="preserve">
          <source>Applies a 2D convolution over a quantized input signal composed of several quantized input planes.</source>
          <target state="translated">여러 양자화 된 입력 평면으로 구성된 양자화 된 입력 신호에 2D 컨볼 루션을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="01f53bdcdfe76a89f9da101eca78388d0e3db80b" translate="yes" xml:space="preserve">
          <source>Applies a 2D convolution over an input image composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 이미지에 2D 컨볼 루션을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="4d7022a0fdb1e618b639247856e6903aab3855e6" translate="yes" xml:space="preserve">
          <source>Applies a 2D convolution over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 2D 컨볼 루션을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="d5648bdd82e29b0ec766dab1d7ea5bd15fb57316" translate="yes" xml:space="preserve">
          <source>Applies a 2D fractional max pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 대해 2D 부분 최대 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="4dcf07079bf5dd6c28cbfd49dde4f39b4bf6b35b" translate="yes" xml:space="preserve">
          <source>Applies a 2D max pooling over a quantized input signal composed of several quantized input planes.</source>
          <target state="translated">여러 양자화 된 입력 평면으로 구성된 양자화 된 입력 신호에 대해 2D 최대 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="a476032b7da8421b8000aa56e627562cba0b8e78" translate="yes" xml:space="preserve">
          <source>Applies a 2D max pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 2D 최대 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="16be7a5fb7dca84029d9fea2edb8b4f2f0ee57f8" translate="yes" xml:space="preserve">
          <source>Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels.</source>
          <target state="translated">여러 입력 채널로 구성된 입력 신호에 2D 최근 접 이웃 업 샘플링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="41e06a057945b6ec3fbb4344ac26327e886f585c" translate="yes" xml:space="preserve">
          <source>Applies a 2D power-average pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 2D 전력 평균 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="d69ec22c35e226c6aa2b1af68d48e3fa60401801" translate="yes" xml:space="preserve">
          <source>Applies a 2D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of &lt;code&gt;p&lt;/code&gt; is zero, the gradient is set to zero as well.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 2D 전력 평균 풀링을 적용합니다. &lt;code&gt;p&lt;/code&gt; 의 거듭 제곱에 대한 모든 입력의 합 이 0이면 기울기도 0으로 설정됩니다.</target>
        </trans-unit>
        <trans-unit id="aa4b7ef410c54190539113762f6eeb2a66bf418c" translate="yes" xml:space="preserve">
          <source>Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called &amp;ldquo;deconvolution&amp;rdquo;.</source>
          <target state="translated">때때로 &quot;디컨 볼 루션&quot;이라고도하는 여러 입력 평면으로 구성된 입력 이미지에 2D 전치 컨볼 루션 연산자를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="b43e206b10ee89d84a5a812651aa838b543004e7" translate="yes" xml:space="preserve">
          <source>Applies a 2D transposed convolution operator over an input image composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 이미지에 2D 전치 컨볼 루션 연산자를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="e5d643fce9e39e07cff6793421bc79eff19c89aa" translate="yes" xml:space="preserve">
          <source>Applies a 3D adaptive average pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 3D 적응 형 평균 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="ae56276da0a49f342cfd4b640808d55b08117aa7" translate="yes" xml:space="preserve">
          <source>Applies a 3D adaptive max pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 3D 적응 형 최대 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="37570017fc80764b6133f223b871fd527ef1696c" translate="yes" xml:space="preserve">
          <source>Applies a 3D average pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 3D 평균 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="af8bbac2d382374f13d6575cf2d20a9ea3e1524d" translate="yes" xml:space="preserve">
          <source>Applies a 3D convolution over a quantized 3D input composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 양자화 된 3D 입력에 3D 컨볼 루션을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="c2ec5baf08b4d035b1c172c872a14f1141d860e8" translate="yes" xml:space="preserve">
          <source>Applies a 3D convolution over a quantized input signal composed of several quantized input planes.</source>
          <target state="translated">여러 양자화 된 입력 평면으로 구성된 양자화 된 입력 신호에 3D 컨볼 루션을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="f6f649a7867c44dcfc76acf042b1d45c5f149502" translate="yes" xml:space="preserve">
          <source>Applies a 3D convolution over an input image composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 이미지에 3D 회선을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="51acb800dbe635481a7193d943f2c4fdf6f9b633" translate="yes" xml:space="preserve">
          <source>Applies a 3D convolution over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 3D 컨볼 루션을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="a5c65698b496aec798c667f51e238522d8ca0853" translate="yes" xml:space="preserve">
          <source>Applies a 3D max pooling over an input signal composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 신호에 3D 최대 풀링을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="330c7c56dc877c7532afe521797eaa31d71f0cd0" translate="yes" xml:space="preserve">
          <source>Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called &amp;ldquo;deconvolution&amp;rdquo;</source>
          <target state="translated">&quot;디컨 볼 루션&quot;이라고도하는 여러 입력 평면으로 구성된 입력 이미지에 3D 전치 컨볼 루션 연산자를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="8ceaf63b96a331eabb115beb8c3e00540f6c5ffc" translate="yes" xml:space="preserve">
          <source>Applies a 3D transposed convolution operator over an input image composed of several input planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 이미지에 3D 전치 컨볼 루션 연산자를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="65ce4bde72c1f78502a6398b7865adc72e50ac02" translate="yes" xml:space="preserve">
          <source>Applies a 3D transposed convolution operator over an input image composed of several input planes. The transposed convolution operator multiplies each input value element-wise by a learnable kernel, and sums over the outputs from all input feature planes.</source>
          <target state="translated">여러 입력 평면으로 구성된 입력 이미지에 3D 전치 컨볼 루션 연산자를 적용합니다. 전치 컨볼 루션 연산자는 각 입력 값에 학습 가능한 커널을 요소별로 곱하고 모든 입력 특성 평면의 출력을 합산합니다.</target>
        </trans-unit>
        <trans-unit id="66c150b86ae084c85a0e9289376927637ec64e05" translate="yes" xml:space="preserve">
          <source>Applies a bilinear transformation to the incoming data:</source>
          <target state="translated">수신 데이터에 쌍 선형 변환을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="6ff756a7c7cfe8cec72e517aee9af4136e1371be" translate="yes" xml:space="preserve">
          <source>Applies a linear transformation to the incoming data:</source>
          <target state="translated">수신 데이터에 선형 변환을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="8ff337dc0db982c6291a92a0d7e5b13fd2dbe747" translate="yes" xml:space="preserve">
          <source>Applies a linear transformation to the incoming quantized data:</source>
          <target state="translated">들어오는 양자화 된 데이터에 선형 변환을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="a2436507aea812ee14f1556396ee7b6b4cbdbe1d" translate="yes" xml:space="preserve">
          <source>Applies a multi-layer Elman RNN with</source>
          <target state="translated">다층 Elman RNN을 다음과 같이 적용합니다.</target>
        </trans-unit>
        <trans-unit id="bc4223d58e32410e3d29e09ef9a1db009d298cd1" translate="yes" xml:space="preserve">
          <source>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</source>
          <target state="translated">입력 시퀀스에 다 계층 게이트 반복 장치 (GRU) RNN을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="02da21c27e34ce09b75309eefe898e142b0a5cb5" translate="yes" xml:space="preserve">
          <source>Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.</source>
          <target state="translated">입력 시퀀스에 다층 장단기 기억 (LSTM) RNN을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="7660fbeb32f57e21a2dce8c33a454664d3adf995" translate="yes" xml:space="preserve">
          <source>Applies a softmax followed by a logarithm.</source>
          <target state="translated">소프트 맥스 뒤에 로그를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="16d6a0fbab19784fd6044224005fc862384c1a6c" translate="yes" xml:space="preserve">
          <source>Applies a softmax function.</source>
          <target state="translated">소프트 맥스 함수를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="3511c68e1b0c335caf44167160bac43a1d7fe51a" translate="yes" xml:space="preserve">
          <source>Applies a softmin function.</source>
          <target state="translated">softmin 기능을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="cfc3d07eb38a8451b3b1a0481d05e3ce6d35b93e" translate="yes" xml:space="preserve">
          <source>Applies alpha dropout to the input.</source>
          <target state="translated">입력에 알파 드롭 아웃을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="d7a232c41e3f6477e97dc6f5c3a6a2571c32c1b2" translate="yes" xml:space="preserve">
          <source>Applies element-wise</source>
          <target state="translated">요소별로 적용</target>
        </trans-unit>
        <trans-unit id="113342e492fe7e12d769f5fd51f90c782c13aba3" translate="yes" xml:space="preserve">
          <source>Applies element-wise the function</source>
          <target state="translated">요소 별 함수 적용</target>
        </trans-unit>
        <trans-unit id="f558b6f91b1db6ba6cd7e44d550b3b8856e30ab7" translate="yes" xml:space="preserve">
          <source>Applies element-wise,</source>
          <target state="translated">요소별로 적용됩니다.</target>
        </trans-unit>
        <trans-unit id="bdcea2507c8f3d67607511bd4053b83de34dceec" translate="yes" xml:space="preserve">
          <source>Applies element-wise, the function</source>
          <target state="translated">요소 별, 함수 적용</target>
        </trans-unit>
        <trans-unit id="e6f0771cea3151fdc337ea7709543dfa37058eea" translate="yes" xml:space="preserve">
          <source>Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.</source>
          <target state="translated">채널이 두 번째 차원을 차지하는 여러 입력 평면으로 구성된 입력 신호에 로컬 응답 정규화를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="5dfd4a4a091440af428452b6ccfc1c2f2eb1887c" translate="yes" xml:space="preserve">
          <source>Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels.</source>
          <target state="translated">채널이 두 번째 차원을 차지하는 여러 입력 평면으로 구성된 입력 신호에 로컬 응답 정규화를 적용합니다. 채널 전체에 정규화를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="4ed90484b4c8624d14fcf12a0e8edf257e35f7b9" translate="yes" xml:space="preserve">
          <source>Applies pruning reparametrization to the tensor corresponding to the parameter called &lt;code&gt;name&lt;/code&gt; in &lt;code&gt;module&lt;/code&gt; without actually pruning any units.</source>
          <target state="translated">매개 변수 호출에 해당하는 텐서에 가지 치기 reparametrization을 적용 &lt;code&gt;name&lt;/code&gt; 의 &lt;code&gt;module&lt;/code&gt; 실제로 단위를 치기없이.</target>
        </trans-unit>
        <trans-unit id="459126de8057e720ba86989e3d80f97bc3223f0e" translate="yes" xml:space="preserve">
          <source>Applies quantized rectified linear unit function element-wise:</source>
          <target state="translated">양자화 된 정류 선형 단위 함수를 요소별로 적용합니다.</target>
        </trans-unit>
        <trans-unit id="fdc304cf2ec7d610cd24075ed2e43e1a79d41759" translate="yes" xml:space="preserve">
          <source>Applies spectral normalization to a parameter in the given module.</source>
          <target state="translated">주어진 모듈의 매개 변수에 스펙트럼 정규화를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="2134b29fbca7ca9ac873ada1558b714c901876b0" translate="yes" xml:space="preserve">
          <source>Applies the</source>
          <target state="translated">적용</target>
        </trans-unit>
        <trans-unit id="a78621f5f34b400e9542d53a0ce88e233a0027bb" translate="yes" xml:space="preserve">
          <source>Applies the Gaussian Error Linear Units function:</source>
          <target state="translated">Gaussian Error Linear Units 함수를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="26cda85cebacaa2f55fa0de1b622a487073b568a" translate="yes" xml:space="preserve">
          <source>Applies the HardTanh function element-wise</source>
          <target state="translated">HardTanh 함수를 요소별로 적용합니다.</target>
        </trans-unit>
        <trans-unit id="12f634982a2632889413c5e4a92650f28c9bdade" translate="yes" xml:space="preserve">
          <source>Applies the HardTanh function element-wise. See &lt;a href=&quot;generated/torch.nn.hardtanh#torch.nn.Hardtanh&quot;&gt;&lt;code&gt;Hardtanh&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">HardTanh 함수를 요소별로 적용합니다. 자세한 내용은 &lt;a href=&quot;generated/torch.nn.hardtanh#torch.nn.Hardtanh&quot;&gt; &lt;code&gt;Hardtanh&lt;/code&gt; &lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="95b3d284066a600a911ef55c57442d7f4b86507f" translate="yes" xml:space="preserve">
          <source>Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1.</source>
          <target state="translated">Softmax 함수를 n 차원 입력 Tensor에 적용하여 n 차원 출력 Tensor의 요소가 [0,1] 범위에 있고 합계가 1이되도록 조정합니다.</target>
        </trans-unit>
        <trans-unit id="01834f69d8e2aaf053d85470251564527cc332f1" translate="yes" xml:space="preserve">
          <source>Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range &lt;code&gt;[0, 1]&lt;/code&gt; and sum to 1.</source>
          <target state="translated">Softmin 함수를 n 차원 입력 Tensor에 적용하여 n 차원 출력 Tensor의 요소가 &lt;code&gt;[0, 1]&lt;/code&gt; 범위에 있고 합계가 1이되도록 조정합니다.</target>
        </trans-unit>
        <trans-unit id="e581807715bdab40e3c6fa2d722c3f49a2fef542" translate="yes" xml:space="preserve">
          <source>Applies the element-wise function</source>
          <target state="translated">요소 별 함수를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="c91f96c06647ec0aa46ecdb47707667cb0cb30c3" translate="yes" xml:space="preserve">
          <source>Applies the element-wise function:</source>
          <target state="translated">요소 별 함수를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="20e17ca8da87d2e89b6ddbe8ec999295cc6de103" translate="yes" xml:space="preserve">
          <source>Applies the function &lt;code&gt;callable&lt;/code&gt; to each element in the tensor, replacing each element with the value returned by &lt;code&gt;callable&lt;/code&gt;.</source>
          <target state="translated">텐서의 각 요소에 &lt;code&gt;callable&lt;/code&gt; 함수를 적용 하여 각 요소를 &lt;code&gt;callable&lt;/code&gt; 이 반환 한 값으로 바꿉니다 .</target>
        </trans-unit>
        <trans-unit id="4f92865eb288d8379f17f76a886fec1e420e13a3" translate="yes" xml:space="preserve">
          <source>Applies the hard shrinkage function element-wise</source>
          <target state="translated">하드 수축 기능을 요소별로 적용합니다.</target>
        </trans-unit>
        <trans-unit id="56fce9ee97b2fb62286ad99c1ca046ac1d4d2b2c" translate="yes" xml:space="preserve">
          <source>Applies the hard shrinkage function element-wise:</source>
          <target state="translated">하드 수축 기능을 요소별로 적용합니다.</target>
        </trans-unit>
        <trans-unit id="ec160fe4b0b150b142f63859285de9f24ee55b53" translate="yes" xml:space="preserve">
          <source>Applies the hardswish function, element-wise, as described in the paper:</source>
          <target state="translated">문서에 설명 된대로 요소별로 hardswish 함수를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="936f4f75c2b9f6e3826efbb88a4e51ae2ab84f70" translate="yes" xml:space="preserve">
          <source>Applies the latest &lt;code&gt;method&lt;/code&gt; by computing the new partial masks and returning its combination with the &lt;code&gt;default_mask&lt;/code&gt;. The new partial mask should be computed on the entries or channels that were not zeroed out by the &lt;code&gt;default_mask&lt;/code&gt;. Which portions of the tensor &lt;code&gt;t&lt;/code&gt; the new mask will be calculated from depends on the &lt;code&gt;PRUNING_TYPE&lt;/code&gt; (handled by the type handler):</source>
          <target state="translated">새로운 부분 마스크를 계산하고 &lt;code&gt;default_mask&lt;/code&gt; 와의 조합을 반환 하여 최신 &lt;code&gt;method&lt;/code&gt; 를 적용합니다 . 새 부분 마스크는 &lt;code&gt;default_mask&lt;/code&gt; 에 의해 제로화되지 않은 항목 또는 채널에서 계산되어야합니다 . 텐서의 어느 부분 &lt;code&gt;t&lt;/code&gt; 온 의존에서 새로운 마스크가 계산 될 &lt;code&gt;PRUNING_TYPE&lt;/code&gt; (타입 핸들러에 의해 처리)</target>
        </trans-unit>
        <trans-unit id="7d91ea3d6a32f01c8496b5becb21a5b21016d757" translate="yes" xml:space="preserve">
          <source>Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper:</source>
          <target state="translated">논문에 설명 된대로, 요소별로 무작위 누출 정류 라이너 유닛 함수를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="82d1c6b2a1ee5d5cde8c89bdcc2746329c59171c" translate="yes" xml:space="preserve">
          <source>Applies the rectified linear unit function element-wise. See &lt;a href=&quot;#torch.nn.quantized.ReLU&quot;&gt;&lt;code&gt;ReLU&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">정류 된 선형 단위 함수를 요소별로 적용합니다. 자세한 내용은 &lt;a href=&quot;#torch.nn.quantized.ReLU&quot;&gt; &lt;code&gt;ReLU&lt;/code&gt; &lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="bbd19df946d0f4bb7fc4682300e73774ba71feab" translate="yes" xml:space="preserve">
          <source>Applies the rectified linear unit function element-wise. See &lt;a href=&quot;generated/torch.nn.relu#torch.nn.ReLU&quot;&gt;&lt;code&gt;ReLU&lt;/code&gt;&lt;/a&gt; for more details.</source>
          <target state="translated">정류 된 선형 단위 함수를 요소별로 적용합니다. 자세한 내용은 &lt;a href=&quot;generated/torch.nn.relu#torch.nn.ReLU&quot;&gt; &lt;code&gt;ReLU&lt;/code&gt; &lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="78f0af302507bc331ec744dca018a734a0627b7b" translate="yes" xml:space="preserve">
          <source>Applies the rectified linear unit function element-wise:</source>
          <target state="translated">정류 된 선형 단위 함수를 요소별로 적용합니다.</target>
        </trans-unit>
        <trans-unit id="43edbfe54e1d38b1a754c8ddb026e3655a93da7b" translate="yes" xml:space="preserve">
          <source>Applies the silu function, element-wise.</source>
          <target state="translated">silu 함수를 요소별로 적용합니다.</target>
        </trans-unit>
        <trans-unit id="f0e7dde2fc1c92ff1943988e21d9ce05bb8c8f04" translate="yes" xml:space="preserve">
          <source>Applies the soft shrinkage function elementwise</source>
          <target state="translated">소프트 수축 기능을 요소별로 적용합니다.</target>
        </trans-unit>
        <trans-unit id="7e5fad00eee592f1c4a5342fdc927299a5993172" translate="yes" xml:space="preserve">
          <source>Applies the soft shrinkage function elementwise:</source>
          <target state="translated">소프트 수축 기능을 요소별로 적용합니다.</target>
        </trans-unit>
        <trans-unit id="4f3fa5f7feb89ef4634370ab877a48de7ffe1ed7" translate="yes" xml:space="preserve">
          <source>Applies weight normalization to a parameter in the given module.</source>
          <target state="translated">주어진 모듈의 매개 변수에 가중치 정규화를 적용합니다.</target>
        </trans-unit>
        <trans-unit id="adb3943666d494e8bd63152770abcc75d5223bd5" translate="yes" xml:space="preserve">
          <source>Applying &lt;a href=&quot;torch.diag_embed#torch.diag_embed&quot;&gt;&lt;code&gt;torch.diag_embed()&lt;/code&gt;&lt;/a&gt; to the output of this function with the same arguments yields a diagonal matrix with the diagonal entries of the input. However, &lt;a href=&quot;torch.diag_embed#torch.diag_embed&quot;&gt;&lt;code&gt;torch.diag_embed()&lt;/code&gt;&lt;/a&gt; has different default dimensions, so those need to be explicitly specified.</source>
          <target state="translated">동일한 인수를 사용하여이 함수의 출력에 &lt;a href=&quot;torch.diag_embed#torch.diag_embed&quot;&gt; &lt;code&gt;torch.diag_embed()&lt;/code&gt; &lt;/a&gt; 를 적용 하면 입력의 대각선 항목이있는 대각 행렬이 생성됩니다. 그러나 &lt;a href=&quot;torch.diag_embed#torch.diag_embed&quot;&gt; &lt;code&gt;torch.diag_embed()&lt;/code&gt; &lt;/a&gt; 는 기본 크기가 다르므로 명시 적으로 지정해야합니다.</target>
        </trans-unit>
        <trans-unit id="09ebc50e43a0da169dba1646717349a905de80e9" translate="yes" xml:space="preserve">
          <source>Applying &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;torch.diagonal()&lt;/code&gt;&lt;/a&gt; to the output of this function with the same arguments yields a matrix identical to input. However, &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt;&lt;code&gt;torch.diagonal()&lt;/code&gt;&lt;/a&gt; has different default dimensions, so those need to be explicitly specified.</source>
          <target state="translated">동일한 인수를 사용하여이 함수의 출력에 &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;torch.diagonal()&lt;/code&gt; &lt;/a&gt; 을 적용하면 입력과 동일한 행렬이 생성됩니다. 그러나 &lt;a href=&quot;torch.diagonal#torch.diagonal&quot;&gt; &lt;code&gt;torch.diagonal()&lt;/code&gt; &lt;/a&gt; 은 기본 크기가 다르므로 명시 적으로 지정해야합니다.</target>
        </trans-unit>
        <trans-unit id="d22103e4c9027e5c172cbf43f3dd7371a41d32a7" translate="yes" xml:space="preserve">
          <source>Arbitrary positional and keyword inputs are allowed to be passed into DataParallel but some types are specially handled. tensors will be &lt;strong&gt;scattered&lt;/strong&gt; on dim specified (default 0). tuple, list and dict types will be shallow copied. The other types will be shared among different threads and can be corrupted if written to in the model&amp;rsquo;s forward pass.</source>
          <target state="translated">임의의 위치 및 키워드 입력이 DataParallel로 전달 될 수 있지만 일부 유형은 특별히 처리됩니다. 텐서는 지정된 희미한 &lt;strong&gt;곳에 흩어져&lt;/strong&gt; 있습니다 (기본값 0). 튜플, 목록 및 dict 유형은 얕은 복사됩니다. 다른 유형은 다른 스레드간에 공유되며 모델의 순방향 패스에 기록되면 손상 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b6075aed00d09ffa3057c99d13847e6be9606a66" translate="yes" xml:space="preserve">
          <source>Args:</source>
          <target state="translated">Args:</target>
        </trans-unit>
        <trans-unit id="511f2c74f69da56453fcefe6e26731eae720fb15" translate="yes" xml:space="preserve">
          <source>Args: &lt;code&gt;mod&lt;/code&gt; a float module, either produced by torch.quantization utilities or directly from user</source>
          <target state="translated">Args : torch.quantization 유틸리티에서 생성하거나 사용자로부터 직접 생성 한 float 모듈을 &lt;code&gt;mod&lt;/code&gt; 합니다 .</target>
        </trans-unit>
        <trans-unit id="a0478ca5f4c068ca3ac12f9474f6a15865ce9053" translate="yes" xml:space="preserve">
          <source>Arguments:</source>
          <target state="translated">Arguments:</target>
        </trans-unit>
        <trans-unit id="3839352701cd3d321f8924d7921c49d951abf8f7" translate="yes" xml:space="preserve">
          <source>Arguments::</source>
          <target state="translated">Arguments::</target>
        </trans-unit>
        <trans-unit id="6104f39ed22a2cd32e98536a3447a01c4b9f4781" translate="yes" xml:space="preserve">
          <source>Arithmetic Operators</source>
          <target state="translated">산술 연산자</target>
        </trans-unit>
        <trans-unit id="c0e74bbb76aa26e443aa08680a9aad05da89267b" translate="yes" xml:space="preserve">
          <source>Art B. Owen. Scrambling Sobol and Niederreiter-Xing points. Journal of Complexity, 14(4):466-489, December 1998.</source>
          <target state="translated">Art B. Owen. Sobol 및 Niederreiter-Xing 포인트를 스크램블합니다. Journal of Complexity, 14 (4) : 466-489, 1998 년 12 월.</target>
        </trans-unit>
        <trans-unit id="44e38ee54f654b08e43c3403586961f50c567585" translate="yes" xml:space="preserve">
          <source>As a result of these changes, the following items are considered deprecated and should not appear in new code:</source>
          <target state="translated">이러한 변경으로 인해 다음 항목은 더 이상 사용되지 않는 것으로 간주되며 새 코드에 나타나지 않아야합니다.</target>
        </trans-unit>
        <trans-unit id="9f1d874b6f85d0af8d36d263ad9bfb71343abc57" translate="yes" xml:space="preserve">
          <source>As a special case, when &lt;code&gt;input&lt;/code&gt; has zero dimensions and a nonzero scalar value, it is treated as a one-dimensional tensor with one element.</source>
          <target state="translated">특별한 경우로, &lt;code&gt;input&lt;/code&gt; 에 차원이 0이고 스칼라 값이 0이 아닌 경우 요소가 1 개인 1 차원 텐서로 처리됩니다.</target>
        </trans-unit>
        <trans-unit id="50ba4ee86165b9263342121969588460e8e3d7d0" translate="yes" xml:space="preserve">
          <source>As a subset of Python, any valid TorchScript function is also a valid Python function. This makes it possible to &lt;code&gt;disable TorchScript&lt;/code&gt; and debug the function using standard Python tools like &lt;code&gt;pdb&lt;/code&gt;. The reverse is not true: there are many valid Python programs that are not valid TorchScript programs. Instead, TorchScript focuses specifically on the features of Python that are needed to represent neural network models in PyTorch.</source>
          <target state="translated">Python의 하위 집합 인 유효한 TorchScript 함수도 유효한 Python 함수입니다. 이를 통해 &lt;code&gt;disable TorchScript&lt;/code&gt; 를 비활성화 하고 &lt;code&gt;pdb&lt;/code&gt; 와 같은 표준 Python 도구를 사용하여 함수를 디버그 할 수 있습니다 . 그 반대는 사실이 아닙니다. 유효한 TorchScript 프로그램이 아닌 유효한 Python 프로그램이 많이 있습니다. 대신 TorchScript는 PyTorch에서 신경망 모델을 나타내는 데 필요한 Python의 기능에 특히 중점을 둡니다.</target>
        </trans-unit>
        <trans-unit id="b6cd63f503882dea2536baff15dc121bd94a50a6" translate="yes" xml:space="preserve">
          <source>As above, but the sample points are spaced uniformly at a distance of &lt;code&gt;dx&lt;/code&gt;.</source>
          <target state="translated">위와 같지만 샘플 포인트는 &lt;code&gt;dx&lt;/code&gt; 거리에서 균일 한 간격으로 배치 됩니다.</target>
        </trans-unit>
        <trans-unit id="4514250ef4bd2637a52c6977efbe9cd977631d0f" translate="yes" xml:space="preserve">
          <source>As described in the paper &lt;a href=&quot;https://arxiv.org/abs/1411.4280&quot;&gt;Efficient Object Localization Using Convolutional Networks&lt;/a&gt; , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease.</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1411.4280&quot;&gt;Convolutional Networks를 사용한 Efficient Object Localization&lt;/a&gt; 논문에 설명 된대로 피처 맵 내의 인접 픽셀이 강한 상관 관계가있는 경우 (일반적으로 초기 컨볼 루션 레이어의 경우) iid 드롭 아웃이 활성화를 정규화하지 않고 그렇지 않으면 효과적인 학습률을 초래합니다. 감소.</target>
        </trans-unit>
        <trans-unit id="99099b94ea4fc98fc5c41ec54e22d892cee1ab67" translate="yes" xml:space="preserve">
          <source>As of 0.4, this function does not support an &lt;code&gt;out&lt;/code&gt; keyword. As an alternative, the old &lt;code&gt;torch.ones_like(input, out=output)&lt;/code&gt; is equivalent to &lt;code&gt;torch.ones(input.size(), out=output)&lt;/code&gt;.</source>
          <target state="translated">0.4부터이 함수는 &lt;code&gt;out&lt;/code&gt; 키워드를 지원하지 않습니다 . 대안으로, 이전 &lt;code&gt;torch.ones_like(input, out=output)&lt;/code&gt; 은 &lt;code&gt;torch.ones(input.size(), out=output)&lt;/code&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="3fb950c139664617530977c05541f892b77d98f6" translate="yes" xml:space="preserve">
          <source>As of 0.4, this function does not support an &lt;code&gt;out&lt;/code&gt; keyword. As an alternative, the old &lt;code&gt;torch.zeros_like(input, out=output)&lt;/code&gt; is equivalent to &lt;code&gt;torch.zeros(input.size(), out=output)&lt;/code&gt;.</source>
          <target state="translated">0.4부터이 함수는 &lt;code&gt;out&lt;/code&gt; 키워드를 지원하지 않습니다 . 대안으로, 이전 &lt;code&gt;torch.zeros_like(input, out=output)&lt;/code&gt; 은 &lt;code&gt;torch.zeros(input.size(), out=output)&lt;/code&gt; 합니다.</target>
        </trans-unit>
        <trans-unit id="5d301eba7fb8fca354fa028e6aad9db923bee3a2" translate="yes" xml:space="preserve">
          <source>As of PyTorch v1.7, Windows support for the distributed package only covers collective communications with Gloo backend, &lt;code&gt;FileStore&lt;/code&gt;, and &lt;code&gt;DistributedDataParallel&lt;/code&gt;. Therefore, the &lt;code&gt;init_method&lt;/code&gt; argument in &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;init_process_group()&lt;/code&gt;&lt;/a&gt; must point to a file. This works for both local and shared file systems:</source>
          <target state="translated">PyTorch v1.7부터 분산 패키지에 대한 Windows 지원은 Gloo 백엔드, &lt;code&gt;FileStore&lt;/code&gt; 및 &lt;code&gt;DistributedDataParallel&lt;/code&gt; 과의 집합 적 통신 만 포함 합니다. 따라서 &lt;a href=&quot;#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;init_process_group()&lt;/code&gt; &lt;/a&gt; 의 &lt;code&gt;init_method&lt;/code&gt; 인수 는 파일을 가리켜 야합니다. 이것은 로컬 및 공유 파일 시스템 모두에서 작동합니다.</target>
        </trans-unit>
        <trans-unit id="80184e7d134febd3ee09d8016449ea572c67e780" translate="yes" xml:space="preserve">
          <source>As with &lt;a href=&quot;torch.nn.nllloss#torch.nn.NLLLoss&quot;&gt;&lt;code&gt;NLLLoss&lt;/code&gt;&lt;/a&gt;, the &lt;code&gt;input&lt;/code&gt; given is expected to contain &lt;em&gt;log-probabilities&lt;/em&gt; and is not restricted to a 2D Tensor. The targets are interpreted as &lt;em&gt;probabilities&lt;/em&gt; by default, but could be considered as &lt;em&gt;log-probabilities&lt;/em&gt; with &lt;code&gt;log_target&lt;/code&gt; set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">와 마찬가지로 &lt;a href=&quot;torch.nn.nllloss#torch.nn.NLLLoss&quot;&gt; &lt;code&gt;NLLLoss&lt;/code&gt; &lt;/a&gt; 의 &lt;code&gt;input&lt;/code&gt; 주어진을 포함 할 것으로 예상된다 &lt;em&gt;로그 확률&lt;/em&gt; 및 2 차원 텐서에 제한되지 않는다. 대상은로 해석됩니다 &lt;em&gt;확률&lt;/em&gt; 기본적으로, 그러나로 간주 될 수 &lt;em&gt;로그 확률&lt;/em&gt; 로 &lt;code&gt;log_target&lt;/code&gt; 의 로 설정 &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="99266bbca47ea9ff5e29020debec6a9680d5bf7a" translate="yes" xml:space="preserve">
          <source>As with image classification models, all pre-trained models expect input images normalized in the same way. The images have to be loaded in to a range of &lt;code&gt;[0, 1]&lt;/code&gt; and then normalized using &lt;code&gt;mean = [0.485, 0.456, 0.406]&lt;/code&gt; and &lt;code&gt;std = [0.229, 0.224, 0.225]&lt;/code&gt;. They have been trained on images resized such that their minimum size is 520.</source>
          <target state="translated">이미지 분류 모델과 마찬가지로 모든 사전 학습 된 모델은 동일한 방식으로 정규화 된 입력 이미지를 기대합니다. 이미지는 &lt;code&gt;[0, 1]&lt;/code&gt; 범위로로드 한 다음 &lt;code&gt;mean = [0.485, 0.456, 0.406]&lt;/code&gt; 및 &lt;code&gt;std = [0.229, 0.224, 0.225]&lt;/code&gt; 사용하여 정규화해야 합니다. 최소 크기가 520이되도록 크기가 조정 된 이미지에 대해 교육을 받았습니다.</target>
        </trans-unit>
        <trans-unit id="8b593995c88f61a609044a01e37e8e4ccc22e065" translate="yes" xml:space="preserve">
          <source>Assumptions</source>
          <target state="translated">Assumptions</target>
        </trans-unit>
        <trans-unit id="855f8e40c74ae8a51a99a06c8a2a032f398b5ed4" translate="yes" xml:space="preserve">
          <source>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group</source>
          <target state="translated">async_op이 True로 설정된 경우 비동기 작업 핸들입니다. 없음 (async_op이 아니거나 그룹의 일부가 아닌 경우)</target>
        </trans-unit>
        <trans-unit id="a78338cab2cb5c76c6ab7df23e81a35449f6df4f" translate="yes" xml:space="preserve">
          <source>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.</source>
          <target state="translated">async_op이 True로 설정된 경우 비동기 작업 핸들입니다. async_op이 아니거나 그룹의 일부가 아닌 경우 없음.</target>
        </trans-unit>
        <trans-unit id="4042af2d84df423621c1b7387da3334c7f85d192" translate="yes" xml:space="preserve">
          <source>Async work handle, if async_op is set to True. None, otherwise</source>
          <target state="translated">async_op이 True로 설정된 경우 비동기 작업 핸들입니다. 없음, 그렇지 않으면</target>
        </trans-unit>
        <trans-unit id="1889aec46759ffe70c5c9ddb3fee3b5ee5713f7b" translate="yes" xml:space="preserve">
          <source>At groups= &lt;code&gt;in_channels&lt;/code&gt;, each input channel is convolved with its own set of filters (of size</source>
          <target state="translated">groups = &lt;code&gt;in_channels&lt;/code&gt; 에서 각 입력 채널은 자체 필터 세트 (크기</target>
        </trans-unit>
        <trans-unit id="6510deaa2781bdfd17b677974666275b9af08554" translate="yes" xml:space="preserve">
          <source>At groups= &lt;code&gt;in_channels&lt;/code&gt;, each input channel is convolved with its own set of filters, of size</source>
          <target state="translated">groups = &lt;code&gt;in_channels&lt;/code&gt; 에서 각 입력 채널은 크기의 자체 필터 세트와 컨볼 루션됩니다.</target>
        </trans-unit>
        <trans-unit id="9d3f19004934696d7b2d6e8d06f0e3a963b23772" translate="yes" xml:space="preserve">
          <source>At groups= &lt;code&gt;in_channels&lt;/code&gt;, each input channel is convolved with its own set of filters, of size:</source>
          <target state="translated">groups = &lt;code&gt;in_channels&lt;/code&gt; 에서 각 입력 채널은 다음과 같은 크기의 자체 필터 세트와 연결됩니다.</target>
        </trans-unit>
        <trans-unit id="bcb1bde7e11ed9170bfe340f3dbc5867346695da" translate="yes" xml:space="preserve">
          <source>At groups=1, all inputs are convolved to all outputs.</source>
          <target state="translated">그룹 = 1에서 모든 입력은 모든 출력으로 컨볼 루션됩니다.</target>
        </trans-unit>
        <trans-unit id="f9d6240328fee5db3b7a1921b6420e8a8acf15ff" translate="yes" xml:space="preserve">
          <source>At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated.</source>
          <target state="translated">groups = 2에서 작업은 두 개의 conv 레이어를 나란히두고 각각 입력 채널의 절반을보고 출력 채널의 절반을 생성 한 다음 둘 다 연결하는 것과 같습니다.</target>
        </trans-unit>
        <trans-unit id="da760faa855b591ad5bebacae023465206f25e52" translate="yes" xml:space="preserve">
          <source>At p =</source>
          <target state="translated">p =에서</target>
        </trans-unit>
        <trans-unit id="ac85559223d37e290d1c1660fd99f2a4cf5904d9" translate="yes" xml:space="preserve">
          <source>At p = 1, one gets Sum Pooling (which is proportional to Average Pooling)</source>
          <target state="translated">p = 1에서 합계 풀링 (평균 풀링에 비례)을 얻습니다.</target>
        </trans-unit>
        <trans-unit id="9dbb94515aba090ad21fcfa70bd96411862efa24" translate="yes" xml:space="preserve">
          <source>At p = 1, one gets Sum Pooling (which is proportional to average pooling)</source>
          <target state="translated">p = 1에서 Sum Pooling (평균 풀링에 비례)을 얻습니다.</target>
        </trans-unit>
        <trans-unit id="74e0b9c80dca267a78a89ad68a2f6c73241b5973" translate="yes" xml:space="preserve">
          <source>Attention</source>
          <target state="translated">Attention</target>
        </trans-unit>
        <trans-unit id="9b0aef7ef75761256026f8850aa6308137966a97" translate="yes" xml:space="preserve">
          <source>Attribute Lookup On Python Modules</source>
          <target state="translated">Python 모듈의 속성 조회</target>
        </trans-unit>
        <trans-unit id="a6652617f2c799eb11ee727b16c5646c48af6905" translate="yes" xml:space="preserve">
          <source>Attributes</source>
          <target state="translated">Attributes</target>
        </trans-unit>
        <trans-unit id="cde0319b4a4a6d08df1fd51fb49e58f04a817a7e" translate="yes" xml:space="preserve">
          <source>Attributes of a ScriptModule can be marked constant by annotating them with &lt;code&gt;Final[T]&lt;/code&gt;</source>
          <target state="translated">ScriptModule의 속성은 &lt;code&gt;Final[T]&lt;/code&gt; 로 주석을 달아 상수로 표시 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="effc3797a92fafe90a0f719052ebdd67ab00baf2" translate="yes" xml:space="preserve">
          <source>Attributes: Same as torch.nn.quantized.Conv3d</source>
          <target state="translated">속성 : torch.nn.quantized.Conv3d와 동일</target>
        </trans-unit>
        <trans-unit id="78965d92bd603a457514f5c8bd35fc06957dafd3" translate="yes" xml:space="preserve">
          <source>Autograd currently supports named tensors in a limited manner: autograd ignores names on all tensors. Gradient computation is still correct but we lose the safety that names give us.</source>
          <target state="translated">Autograd는 현재 제한된 방식으로 명명 된 텐서를 지원합니다. autograd는 모든 텐서의 이름을 무시합니다. 그래디언트 계산은 여전히 ​​옳지 만 이름이주는 안전성을 잃습니다.</target>
        </trans-unit>
        <trans-unit id="1cead4310cfb62879040be96cb0226da15307414" translate="yes" xml:space="preserve">
          <source>Autograd is supported, see &lt;a href=&quot;#named-tensors-autograd-doc&quot;&gt;Autograd support&lt;/a&gt;. Because gradients are currently unnamed, optimizers may work but are untested.</source>
          <target state="translated">Autograd가 지원됩니다 . &lt;a href=&quot;#named-tensors-autograd-doc&quot;&gt;Autograd 지원을&lt;/a&gt; 참조 하세요 . 그래디언트는 현재 이름이 지정되지 않았기 때문에 최적화 프로그램은 작동 할 수 있지만 테스트되지 않았습니다.</target>
        </trans-unit>
        <trans-unit id="1fc85b24b64820e5c1a91ec86a7fdeb96b65e6ed" translate="yes" xml:space="preserve">
          <source>Autograd mechanics</source>
          <target state="translated">Autograd 역학</target>
        </trans-unit>
        <trans-unit id="a6a465a4bfe05284d5084187d5372a6d8da9b829" translate="yes" xml:space="preserve">
          <source>Autograd recording during the forward pass</source>
          <target state="translated">정방향 패스 중 Autograd 기록</target>
        </trans-unit>
        <trans-unit id="7807d41b89e768682ae049c6166592980a550e48" translate="yes" xml:space="preserve">
          <source>Autograd support</source>
          <target state="translated">Autograd 지원</target>
        </trans-unit>
        <trans-unit id="9b8005aa270f8147e0440468e241bd00f96800bc" translate="yes" xml:space="preserve">
          <source>Automatic Mixed Precision examples</source>
          <target state="translated">자동 혼합 정밀도 예</target>
        </trans-unit>
        <trans-unit id="0f3dc77bfd956e56fcd5b8e898b3fabd32b08034" translate="yes" xml:space="preserve">
          <source>Automatic Trace Checking</source>
          <target state="translated">자동 추적 검사</target>
        </trans-unit>
        <trans-unit id="1def506ac3e846cb4c938843152cd0b9bba71135" translate="yes" xml:space="preserve">
          <source>AvgPool1d</source>
          <target state="translated">AvgPool1d</target>
        </trans-unit>
        <trans-unit id="c673cdd08db86374f9ff97d13da9945ad47b1180" translate="yes" xml:space="preserve">
          <source>AvgPool2d</source>
          <target state="translated">AvgPool2d</target>
        </trans-unit>
        <trans-unit id="fc7fc296c7e1f04112232fe8a9e0e82d5c7f190f" translate="yes" xml:space="preserve">
          <source>AvgPool3d</source>
          <target state="translated">AvgPool3d</target>
        </trans-unit>
        <trans-unit id="6da0341102c44a220c312de3110fd1209e71eaf5" translate="yes" xml:space="preserve">
          <source>Ax = b</source>
          <target state="translated">도끼 = b</target>
        </trans-unit>
        <trans-unit id="ae4f281df5a5d0ff3cad6371f76d5c29b6d953ec" translate="yes" xml:space="preserve">
          <source>B</source>
          <target state="translated">B</target>
        </trans-unit>
        <trans-unit id="a9d087c32e7ca877fdba43990aa26038f88f00de" translate="yes" xml:space="preserve">
          <source>B \times P \times M</source>
          <target state="translated">B \ x P \ x M</target>
        </trans-unit>
        <trans-unit id="db6d439058d0e22e68b29c790c70d29e6828be28" translate="yes" xml:space="preserve">
          <source>B \times P \times R</source>
          <target state="translated">B \ x P \ x R</target>
        </trans-unit>
        <trans-unit id="2dfbff36e0eaece5eff122157b4277846a87f5f4" translate="yes" xml:space="preserve">
          <source>B \times R \times M</source>
          <target state="translated">B \ times R \ times M</target>
        </trans-unit>
        <trans-unit id="45bdfdeb5092f65ada4a9a1c95c08cee2ffa142d" translate="yes" xml:space="preserve">
          <source>BAND</source>
          <target state="translated">BAND</target>
        </trans-unit>
        <trans-unit id="0ef786803f54f9a9e02024a654989e60c7f87b66" translate="yes" xml:space="preserve">
          <source>BCELoss</source>
          <target state="translated">BCELoss</target>
        </trans-unit>
        <trans-unit id="c174768efb833ed7a3edebffb9950a4f61c4c940" translate="yes" xml:space="preserve">
          <source>BCEWithLogitsLoss</source>
          <target state="translated">BCEWithLogitsLoss</target>
        </trans-unit>
        <trans-unit id="2c89bbb2577ddfe977123efaba3737f659629fff" translate="yes" xml:space="preserve">
          <source>BLAS and LAPACK Operations</source>
          <target state="translated">BLAS 및 LAPACK 작업</target>
        </trans-unit>
        <trans-unit id="4aefcf5c188e5bba658f180d7b47a0379fa5e70c" translate="yes" xml:space="preserve">
          <source>BOR</source>
          <target state="translated">BOR</target>
        </trans-unit>
        <trans-unit id="bd606f52a5ad5bc6c6cc894b3accba4125210f66" translate="yes" xml:space="preserve">
          <source>BXOR</source>
          <target state="translated">BXOR</target>
        </trans-unit>
        <trans-unit id="e758ca64563fdd62965a2b97b76d555b1a70d938" translate="yes" xml:space="preserve">
          <source>Backend</source>
          <target state="translated">Backend</target>
        </trans-unit>
        <trans-unit id="b3776d63ad7b7a84bfe20d9c6d4a53a2b25d0e43" translate="yes" xml:space="preserve">
          <source>Backends</source>
          <target state="translated">Backends</target>
        </trans-unit>
        <trans-unit id="82921ea0c75d2b9e5a6aaafaf19c2a51a8ee3c26" translate="yes" xml:space="preserve">
          <source>Backends that come with PyTorch</source>
          <target state="translated">PyTorch와 함께 제공되는 백엔드</target>
        </trans-unit>
        <trans-unit id="64dd60fe1a049fe6db3eb1369dec2e42bf428e21" translate="yes" xml:space="preserve">
          <source>Background</source>
          <target state="translated">Background</target>
        </trans-unit>
        <trans-unit id="4c7660dd341e52619271ac35d19b4aa963dcdc89" translate="yes" xml:space="preserve">
          <source>Backward through &lt;a href=&quot;#torch.det&quot;&gt;&lt;code&gt;det()&lt;/code&gt;&lt;/a&gt; internally uses SVD results when &lt;code&gt;input&lt;/code&gt; is not invertible. In this case, double backward through &lt;a href=&quot;#torch.det&quot;&gt;&lt;code&gt;det()&lt;/code&gt;&lt;/a&gt; will be unstable in when &lt;code&gt;input&lt;/code&gt; doesn&amp;rsquo;t have distinct singular values. See &lt;a href=&quot;torch.svd#torch.svd&quot;&gt;&lt;code&gt;svd()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">&lt;a href=&quot;#torch.det&quot;&gt; &lt;code&gt;det()&lt;/code&gt; &lt;/a&gt; 를 통한 뒤로는 &lt;code&gt;input&lt;/code&gt; 이 반전되지 않을 때 내부적으로 SVD 결과를 사용합니다 . 이 경우 &lt;a href=&quot;#torch.det&quot;&gt; &lt;code&gt;det()&lt;/code&gt; &lt;/a&gt; 를 통한 이중 역방향 은 &lt;code&gt;input&lt;/code&gt; 고유 한 특이 값이 없을 때 불안정 합니다. 자세한 내용은 &lt;a href=&quot;torch.svd#torch.svd&quot;&gt; &lt;code&gt;svd()&lt;/code&gt; &lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="c28a70ba3b665ff380843011cb9bf4004fe6ce27" translate="yes" xml:space="preserve">
          <source>Backward through &lt;a href=&quot;#torch.logdet&quot;&gt;&lt;code&gt;logdet()&lt;/code&gt;&lt;/a&gt; internally uses SVD results when &lt;code&gt;input&lt;/code&gt; is not invertible. In this case, double backward through &lt;a href=&quot;#torch.logdet&quot;&gt;&lt;code&gt;logdet()&lt;/code&gt;&lt;/a&gt; will be unstable in when &lt;code&gt;input&lt;/code&gt; doesn&amp;rsquo;t have distinct singular values. See &lt;a href=&quot;torch.svd#torch.svd&quot;&gt;&lt;code&gt;svd()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">&lt;a href=&quot;#torch.logdet&quot;&gt; &lt;code&gt;logdet()&lt;/code&gt; &lt;/a&gt; 를 통한 역방향 은 &lt;code&gt;input&lt;/code&gt; 이 반전되지 않을 때 내부적으로 SVD 결과를 사용합니다 . 이 경우, &lt;a href=&quot;#torch.logdet&quot;&gt; &lt;code&gt;logdet()&lt;/code&gt; &lt;/a&gt; 를 통한 double back 은 &lt;code&gt;input&lt;/code&gt; 고유 한 특이 값이 없을 때 불안정 합니다. 자세한 내용은 &lt;a href=&quot;torch.svd#torch.svd&quot;&gt; &lt;code&gt;svd()&lt;/code&gt; &lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="28943ec2b2ba443d9dbd186c1f96eafcfe632579" translate="yes" xml:space="preserve">
          <source>Backward through &lt;a href=&quot;#torch.slogdet&quot;&gt;&lt;code&gt;slogdet()&lt;/code&gt;&lt;/a&gt; internally uses SVD results when &lt;code&gt;input&lt;/code&gt; is not invertible. In this case, double backward through &lt;a href=&quot;#torch.slogdet&quot;&gt;&lt;code&gt;slogdet()&lt;/code&gt;&lt;/a&gt; will be unstable in when &lt;code&gt;input&lt;/code&gt; doesn&amp;rsquo;t have distinct singular values. See &lt;a href=&quot;torch.svd#torch.svd&quot;&gt;&lt;code&gt;svd()&lt;/code&gt;&lt;/a&gt; for details.</source>
          <target state="translated">&lt;a href=&quot;#torch.slogdet&quot;&gt; &lt;code&gt;slogdet()&lt;/code&gt; &lt;/a&gt; 를 통한 역방향 은 &lt;code&gt;input&lt;/code&gt; 반전 할 수 없는 경우 내부적으로 SVD 결과를 사용합니다 . 이 경우 &lt;a href=&quot;#torch.slogdet&quot;&gt; &lt;code&gt;slogdet()&lt;/code&gt; &lt;/a&gt; 를 통한 이중 역방향 은 &lt;code&gt;input&lt;/code&gt; 고유 한 특이 값이 없을 때 불안정 합니다. 자세한 내용은 &lt;a href=&quot;torch.svd#torch.svd&quot;&gt; &lt;code&gt;svd()&lt;/code&gt; &lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="bb44593f48eb0328822a985f5aa285528658fb23" translate="yes" xml:space="preserve">
          <source>Bartlett window function.</source>
          <target state="translated">Bartlett 창 기능.</target>
        </trans-unit>
        <trans-unit id="8f6ce4f7d8508e7be7e4cda0e650d850c9905e8a" translate="yes" xml:space="preserve">
          <source>Base class for all neural network modules.</source>
          <target state="translated">모든 신경망 모듈의 기본 클래스입니다.</target>
        </trans-unit>
        <trans-unit id="3756aaad75fa5f15b1e3570c38539a1b4fa27093" translate="yes" xml:space="preserve">
          <source>Base class for all store implementations, such as the 3 provided by PyTorch distributed: (&lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt;&lt;code&gt;TCPStore&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.distributed.FileStore&quot;&gt;&lt;code&gt;FileStore&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;#torch.distributed.HashStore&quot;&gt;&lt;code&gt;HashStore&lt;/code&gt;&lt;/a&gt;).</source>
          <target state="translated">배포 된 PyTorch에서 제공하는 3 개 ( &lt;a href=&quot;#torch.distributed.TCPStore&quot;&gt; &lt;code&gt;TCPStore&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;#torch.distributed.FileStore&quot;&gt; &lt;code&gt;FileStore&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;#torch.distributed.HashStore&quot;&gt; &lt;code&gt;HashStore&lt;/code&gt; &lt;/a&gt; ) 와 같은 모든 저장소 구현의 기본 클래스입니다 .</target>
        </trans-unit>
        <trans-unit id="fb7f5bac791ae3c9dcca099683932046b2428b2e" translate="yes" xml:space="preserve">
          <source>BasePruningMethod</source>
          <target state="translated">BasePruningMethod</target>
        </trans-unit>
        <trans-unit id="7070665795a3b04470b95d5347795e00aee639f7" translate="yes" xml:space="preserve">
          <source>Basic name inference rules</source>
          <target state="translated">기본 이름 추론 규칙</target>
        </trans-unit>
        <trans-unit id="5fcebeefad3cdbbf8733aa928160dec7dc90c1a1" translate="yes" xml:space="preserve">
          <source>Basics</source>
          <target state="translated">Basics</target>
        </trans-unit>
        <trans-unit id="16f46715e1716fa628885ccd95ddf80183d01012" translate="yes" xml:space="preserve">
          <source>Batch sizes represent the number elements at each sequence step in the batch, not the varying sequence lengths passed to &lt;a href=&quot;torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence&quot;&gt;&lt;code&gt;pack_padded_sequence()&lt;/code&gt;&lt;/a&gt;. For instance, given data &lt;code&gt;abc&lt;/code&gt; and &lt;code&gt;x&lt;/code&gt; the &lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence&quot;&gt;&lt;code&gt;PackedSequence&lt;/code&gt;&lt;/a&gt; would contain data &lt;code&gt;axbc&lt;/code&gt; with &lt;code&gt;batch_sizes=[2,1,1]&lt;/code&gt;.</source>
          <target state="translated">배치 크기는 &lt;a href=&quot;torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence&quot;&gt; &lt;code&gt;pack_padded_sequence()&lt;/code&gt; &lt;/a&gt; 전달 된 다양한 시퀀스 길이가 아니라 배치의 각 시퀀스 단계에서 요소 수를 나타냅니다 . 예를 들어, 주어진 데이터 &lt;code&gt;abc&lt;/code&gt; 및 &lt;code&gt;x&lt;/code&gt; &lt;a href=&quot;#torch.nn.utils.rnn.PackedSequence&quot;&gt; &lt;code&gt;PackedSequence&lt;/code&gt; 는&lt;/a&gt; 데이터 포함될 것이다 &lt;code&gt;axbc&lt;/code&gt; 와 &lt;code&gt;batch_sizes=[2,1,1]&lt;/code&gt; 을 .</target>
        </trans-unit>
        <trans-unit id="640ed9b96cb507644eb24c180e159849e7025eaa" translate="yes" xml:space="preserve">
          <source>BatchNorm</source>
          <target state="translated">BatchNorm</target>
        </trans-unit>
        <trans-unit id="c7c537e5e7d31a4a94ee4f8e5ebc01c365e0771a" translate="yes" xml:space="preserve">
          <source>BatchNorm1d</source>
          <target state="translated">BatchNorm1d</target>
        </trans-unit>
        <trans-unit id="539a37ce419a6050de06321e52ac8941c6a520dd" translate="yes" xml:space="preserve">
          <source>BatchNorm2d</source>
          <target state="translated">BatchNorm2d</target>
        </trans-unit>
        <trans-unit id="4036bb17e086d06a41c9d03825932c8325ec759f" translate="yes" xml:space="preserve">
          <source>BatchNorm3d</source>
          <target state="translated">BatchNorm3d</target>
        </trans-unit>
        <trans-unit id="58a700d3fc7db1c024d5e6e31878faaaa556887d" translate="yes" xml:space="preserve">
          <source>Because named tensors can coexist with unnamed tensors, refining names gives a nice way to write named-tensor-aware code that works with both named and unnamed tensors.</source>
          <target state="translated">명명 된 텐서는 명명되지 않은 텐서와 공존 할 수 있기 때문에 이름을 정제하면 명명 된 텐서와 명명되지 않은 텐서 모두에서 작동하는 명명 된 텐서 인식 코드를 작성할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="85ad6aa0a04066bce002e177e5b44f0e8ada2c9c" translate="yes" xml:space="preserve">
          <source>Because the Batch Normalization is done for each channel in the &lt;code&gt;C&lt;/code&gt; dimension, computing statistics on &lt;code&gt;(N, +)&lt;/code&gt; slices, it&amp;rsquo;s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization.</source>
          <target state="translated">배치 정규화는 &lt;code&gt;C&lt;/code&gt; 차원의 각 채널에 대해 수행 되어 &lt;code&gt;(N, +)&lt;/code&gt; 슬라이스 에 대한 통계를 계산 하므로이 볼륨 배치 정규화 또는 시공간 배치 정규화라고 부르는 것이 일반적인 용어입니다.</target>
        </trans-unit>
        <trans-unit id="fd602c285d639d22638df01e41486fc6be835af0" translate="yes" xml:space="preserve">
          <source>Because the Batch Normalization is done over the &lt;code&gt;C&lt;/code&gt; dimension, computing statistics on &lt;code&gt;(N, D, H, W)&lt;/code&gt; slices, it&amp;rsquo;s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization.</source>
          <target state="translated">배치 정규화는 &lt;code&gt;(N, D, H, W)&lt;/code&gt; 슬라이스 에 대한 통계를 계산 하는 &lt;code&gt;C&lt;/code&gt; 차원에서 수행되기 때문에이 볼륨 배치 정규화 또는 시공간 배치 정규화라고 부르는 것이 일반적인 용어입니다.</target>
        </trans-unit>
        <trans-unit id="e3a6337703c27cf2e6ddd618d4baf0da4204696b" translate="yes" xml:space="preserve">
          <source>Because the Batch Normalization is done over the &lt;code&gt;C&lt;/code&gt; dimension, computing statistics on &lt;code&gt;(N, H, W)&lt;/code&gt; slices, it&amp;rsquo;s common terminology to call this Spatial Batch Normalization.</source>
          <target state="translated">배치 정규화는 &lt;code&gt;(N, H, W)&lt;/code&gt; 슬라이스 에 대한 통계를 계산 하는 &lt;code&gt;C&lt;/code&gt; 차원에서 수행되기 때문에이 공간 배치 정규화라고 부르는 것이 일반적인 용어입니다.</target>
        </trans-unit>
        <trans-unit id="288d84de2a904a705b5e7ec68955d71169bacf13" translate="yes" xml:space="preserve">
          <source>Because the Batch Normalization is done over the &lt;code&gt;C&lt;/code&gt; dimension, computing statistics on &lt;code&gt;(N, L)&lt;/code&gt; slices, it&amp;rsquo;s common terminology to call this Temporal Batch Normalization.</source>
          <target state="translated">배치 정규화는 &lt;code&gt;(N, L)&lt;/code&gt; 슬라이스 에 대한 통계를 계산 하는 &lt;code&gt;C&lt;/code&gt; 차원에서 수행되기 때문에이 임시 배치 정규화를 부르는 것이 일반적인 용어입니다.</target>
        </trans-unit>
        <trans-unit id="0da3b2b883ef3b23c37366196cd69afd8c187cdd" translate="yes" xml:space="preserve">
          <source>Because the signal is Hermitian in the time-domain, the result will be real in the frequency domain. Note that some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in &lt;code&gt;input[0]&lt;/code&gt; would result in one or more complex frequency terms which cannot be represented in a real output and so will always be ignored.</source>
          <target state="translated">신호가 시간 영역에서 Hermitian이기 때문에 결과는 주파수 영역에서 실제입니다. 일부 입력 주파수는 Hermitian 속성을 충족하기 위해 실수 값이어야합니다. 이러한 경우 가상 구성 요소는 무시됩니다. 예를 들어, &lt;code&gt;input[0]&lt;/code&gt; 허수 성분은 실제 출력으로 표현할 수없는 하나 이상의 복잡한 주파수 항을 생성하므로 항상 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="2e29f922f6839735f3a19bbe923a21ab693afacc" translate="yes" xml:space="preserve">
          <source>Because your script will be profiled, please ensure that it exits in a finite amount of time.</source>
          <target state="translated">스크립트가 프로파일 링되므로 제한된 시간 내에 종료되는지 확인하십시오.</target>
        </trans-unit>
        <trans-unit id="956a144b3d9996462b7b18cbedf9997fc2011650" translate="yes" xml:space="preserve">
          <source>Before going further, more details on TensorBoard can be found at &lt;a href=&quot;https://www.tensorflow.org/tensorboard/&quot;&gt;https://www.tensorflow.org/tensorboard/&lt;/a&gt;</source>
          <target state="translated">계속 진행하기 전에 TensorBoard에 대한 자세한 내용은 &lt;a href=&quot;https://www.tensorflow.org/tensorboard/&quot;&gt;https://www.tensorflow.org/tensorboard/&lt;/a&gt; 에서 확인할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="8b23ce245f346d21fc21404045956dd5e7ec60de" translate="yes" xml:space="preserve">
          <source>Before using RPC and distributed autograd primitives, initialization must take place. To initialize the RPC framework we need to use &lt;a href=&quot;#torch.distributed.rpc.init_rpc&quot;&gt;&lt;code&gt;init_rpc()&lt;/code&gt;&lt;/a&gt; which would initialize the RPC framework, RRef framework and distributed autograd.</source>
          <target state="translated">RPC 및 분산 autograd 프리미티브를 사용하기 전에 초기화가 수행되어야합니다. RPC 프레임 워크를 초기화하려면 RPC 프레임 워크, RRef 프레임 워크 및 분산 autograd를 초기화하는 &lt;a href=&quot;#torch.distributed.rpc.init_rpc&quot;&gt; &lt;code&gt;init_rpc()&lt;/code&gt; &lt;/a&gt; 를 사용해야합니다.</target>
        </trans-unit>
        <trans-unit id="3e921d24df994f442d57e133126d10fb2ac6e163" translate="yes" xml:space="preserve">
          <source>Below is an example of running a TorchScript function using RPC.</source>
          <target state="translated">다음은 RPC를 사용하여 TorchScript 함수를 실행하는 예입니다.</target>
        </trans-unit>
        <trans-unit id="dc8bffef30a767db701efa64dcad8d3b22a3cf6c" translate="yes" xml:space="preserve">
          <source>Bernoulli</source>
          <target state="translated">Bernoulli</target>
        </trans-unit>
        <trans-unit id="01f8c251cc6790b547148a802d152b484b1bf377" translate="yes" xml:space="preserve">
          <source>Besides the GLOO/MPI/NCCL backends, PyTorch distributed supports third-party backends through a run-time register mechanism. For references on how to develop a third-party backend through C++ Extension, please refer to &lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_extension.html&quot;&gt;Tutorials - Custom C++ and CUDA Extensions&lt;/a&gt; and &lt;code&gt;test/cpp_extensions/cpp_c10d_extension.cpp&lt;/code&gt;. The capability of third-party backends are decided by their own implementations.</source>
          <target state="translated">GLOO / MPI / NCCL 백엔드 외에도 PyTorch 분산은 런타임 레지스터 메커니즘을 통해 타사 백엔드를 지원합니다. C ++ Extension을 통해 타사 백엔드를 개발하는 방법에 대한 참조는 &lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_extension.html&quot;&gt;Tutorials-Custom C ++ 및 CUDA Extensions&lt;/a&gt; 및 &lt;code&gt;test/cpp_extensions/cpp_c10d_extension.cpp&lt;/code&gt; 를 참조하십시오 . 타사 백엔드의 기능은 자체 구현에 의해 결정됩니다.</target>
        </trans-unit>
        <trans-unit id="e26ae344044922af518669ed7912f3779c4b00f9" translate="yes" xml:space="preserve">
          <source>Bias:</source>
          <target state="translated">Bias:</target>
        </trans-unit>
        <trans-unit id="a8b51aa01c82ba019a69245f557ba6ce284edd3e" translate="yes" xml:space="preserve">
          <source>Bilinear</source>
          <target state="translated">Bilinear</target>
        </trans-unit>
        <trans-unit id="054debc367aa35026cc5f23e63b04983a105cd4a" translate="yes" xml:space="preserve">
          <source>Binary arithmetic ops: &lt;a href=&quot;name_inference#unifies-names-from-inputs-doc&quot;&gt;Unifies names from inputs&lt;/a&gt;</source>
          <target state="translated">이진 산술 연산 : &lt;a href=&quot;name_inference#unifies-names-from-inputs-doc&quot;&gt;입력의 이름 통합&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="5200f1acde5b24e6b432770b7da7700cd60a0c9b" translate="yes" xml:space="preserve">
          <source>Blackman window function.</source>
          <target state="translated">블랙맨 창 기능.</target>
        </trans-unit>
        <trans-unit id="0f52a562e394a39a60e84e9b3ac335fc3bd698d8" translate="yes" xml:space="preserve">
          <source>Block until the value of this &lt;code&gt;Future&lt;/code&gt; is ready.</source>
          <target state="translated">이 &lt;code&gt;Future&lt;/code&gt; 의 가치 가 준비 될 때까지 차단하십시오 .</target>
        </trans-unit>
        <trans-unit id="c2959d258f8603010c5e64b30d6b389ff4f7e543" translate="yes" xml:space="preserve">
          <source>Blocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value.</source>
          <target state="translated">소유자에서 로컬 노드로 RRef의 값을 복사하여 반환하는 차단 호출. 현재 노드가 소유자 인 경우 로컬 값에 대한 참조를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="b76ff4906f33c2dd97ddd929b9662ba8cac6174c" translate="yes" xml:space="preserve">
          <source>Boolean</source>
          <target state="translated">Boolean</target>
        </trans-unit>
        <trans-unit id="8bdc4a42a8aebafd81e1c9ebefc552e2af4c86e0" translate="yes" xml:space="preserve">
          <source>Both &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; must have integer types.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;other&lt;/code&gt; 모두 정수 유형이 있어야합니다.</target>
        </trans-unit>
        <trans-unit id="1f1c331482420b1dcc09832ffc768fcc272a654b" translate="yes" xml:space="preserve">
          <source>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.</source>
          <target state="translated">매개 변수와 영구 버퍼 (예 : 평균 실행)가 모두 포함됩니다. 키는 해당 매개 변수 및 버퍼 이름입니다.</target>
        </trans-unit>
        <trans-unit id="fbb16819919dac69a8f12c76f83bc60666b08a4a" translate="yes" xml:space="preserve">
          <source>Break and Continue</source>
          <target state="translated">중단하고 계속</target>
        </trans-unit>
        <trans-unit id="7263f9de457f4107fd587921961d9e2a1123f9c7" translate="yes" xml:space="preserve">
          <source>Broadcasting semantics</source>
          <target state="translated">방송 의미론</target>
        </trans-unit>
        <trans-unit id="c0e07d9c87aab6412703456e09b86450b239d9d4" translate="yes" xml:space="preserve">
          <source>Broadcasts the given tensors according to &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;Broadcasting semantics&lt;/a&gt;.</source>
          <target state="translated">브로드 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;캐스팅 의미론&lt;/a&gt; 에 따라 주어진 텐서를 브로드 캐스트합니다 .</target>
        </trans-unit>
        <trans-unit id="3ef2cb9c843900d814506b00e103fcfa6e5a1790" translate="yes" xml:space="preserve">
          <source>Broadcasts the tensor to the whole group with multiple GPU tensors per node.</source>
          <target state="translated">노드 당 여러 GPU 텐서를 사용하여 전체 그룹에 텐서를 브로드 캐스트합니다.</target>
        </trans-unit>
        <trans-unit id="8cd98b60116d9de3db535a744915ab8ea6c752e2" translate="yes" xml:space="preserve">
          <source>Broadcasts the tensor to the whole group.</source>
          <target state="translated">텐서를 전체 그룹에 브로드 캐스트합니다.</target>
        </trans-unit>
        <trans-unit id="9d0d383693b784a550085e06cd3563d333282d5c" translate="yes" xml:space="preserve">
          <source>Buffers can be accessed as attributes using given names.</source>
          <target state="translated">버퍼는 주어진 이름을 사용하여 속성으로 액세스 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="072f59c8f878e682443386c6e7570c90195dbc31" translate="yes" xml:space="preserve">
          <source>Built-in Functions and Modules</source>
          <target state="translated">내장 함수 및 모듈</target>
        </trans-unit>
        <trans-unit id="2faa9e18e8bd0f32f4bea2db29afed6e0ae12775" translate="yes" xml:space="preserve">
          <source>By default collectives operate on the default group (also called the world) and require all processes to enter the distributed function call. However, some workloads can benefit from more fine-grained communication. This is where distributed groups come into play. &lt;a href=&quot;#torch.distributed.new_group&quot;&gt;&lt;code&gt;new_group()&lt;/code&gt;&lt;/a&gt; function can be used to create new groups, with arbitrary subsets of all processes. It returns an opaque group handle that can be given as a &lt;code&gt;group&lt;/code&gt; argument to all collectives (collectives are distributed functions to exchange information in certain well-known programming patterns).</source>
          <target state="translated">기본적으로 집합체는 기본 그룹 (월드라고도 함)에서 작동하며 모든 프로세스가 분산 함수 호출을 입력해야합니다. 그러나 일부 워크로드는보다 세분화 된 통신의 이점을 누릴 수 있습니다. 이것은 분산 그룹이 작동하는 곳입니다. &lt;a href=&quot;#torch.distributed.new_group&quot;&gt; &lt;code&gt;new_group()&lt;/code&gt; &lt;/a&gt; 함수는 모든 프로세스의 임의의 하위 집합으로 새 그룹을 만드는 데 사용할 수 있습니다. 모든 집합체에 대한 &lt;code&gt;group&lt;/code&gt; 인수로 제공 될 수있는 불투명 한 그룹 핸들을 반환합니다 (집합은 잘 알려진 특정 프로그래밍 패턴에서 정보를 교환하는 분산 함수입니다).</target>
        </trans-unit>
        <trans-unit id="aa6f8066d9ec2654575e0a2dde8cde52f9a324b9" translate="yes" xml:space="preserve">
          <source>By default, &lt;code&gt;dim&lt;/code&gt; is the last dimension of the &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">기본적으로 &lt;code&gt;dim&lt;/code&gt; 은 &lt;code&gt;input&lt;/code&gt; 텐서 의 마지막 차원입니다 .</target>
        </trans-unit>
        <trans-unit id="fbd6c7669b0b5fa4b44c256ffe1ad2c51d96bac8" translate="yes" xml:space="preserve">
          <source>By default, all parameters to a TorchScript function are assumed to be Tensor. To specify that an argument to a TorchScript function is another type, it is possible to use MyPy-style type annotations using the types listed above.</source>
          <target state="translated">기본적으로 TorchScript 함수에 대한 모든 매개 변수는 Tensor로 간주됩니다. TorchScript 함수에 대한 인수가 다른 유형임을 지정하려면 위에 나열된 유형을 사용하여 MyPy 스타일 유형 주석을 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="3269c80ab5ee7ca74422e9e598672c0d5563eb7c" translate="yes" xml:space="preserve">
          <source>By default, both the NCCL and Gloo backends will try to find the right network interface to use. If the automatically detected interface is not correct, you can override it using the following environment variables (applicable to the respective backend):</source>
          <target state="translated">기본적으로 NCCL 및 Gloo 백엔드는 사용할 올바른 네트워크 인터페이스를 찾으려고합니다. 자동으로 감지 된 인터페이스가 올바르지 않은 경우 다음 환경 변수를 사용하여 재정의 할 수 있습니다 (각 백엔드에 적용 가능).</target>
        </trans-unit>
        <trans-unit id="a47868a7f38c8661f0bf65ea44d4c34b8a4bf540" translate="yes" xml:space="preserve">
          <source>By default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the &lt;code&gt;MAX_JOBS&lt;/code&gt; environment variable to a non-negative number.</source>
          <target state="translated">기본적으로 Ninja 백엔드는 #CPUS + 2 명의 작업자를 사용하여 확장 프로그램을 빌드합니다. 이것은 일부 시스템에서 너무 많은 자원을 사용할 수 있습니다. &lt;code&gt;MAX_JOBS&lt;/code&gt; 환경 변수를 음수가 아닌 숫자 로 설정하여 작업자 수를 제어 할 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="0d1c0495670db55861d9901a2985f3df3c09c53d" translate="yes" xml:space="preserve">
          <source>By default, the directory to which the build file is emitted and the resulting library compiled to is &lt;code&gt;&amp;lt;tmp&amp;gt;/torch_extensions/&amp;lt;name&amp;gt;&lt;/code&gt;, where &lt;code&gt;&amp;lt;tmp&amp;gt;&lt;/code&gt; is the temporary folder on the current platform and &lt;code&gt;&amp;lt;name&amp;gt;&lt;/code&gt; the name of the extension. This location can be overridden in two ways. First, if the &lt;code&gt;TORCH_EXTENSIONS_DIR&lt;/code&gt; environment variable is set, it replaces &lt;code&gt;&amp;lt;tmp&amp;gt;/torch_extensions&lt;/code&gt; and all extensions will be compiled into subfolders of this directory. Second, if the &lt;code&gt;build_directory&lt;/code&gt; argument to this function is supplied, it overrides the entire path, i.e. the library will be compiled into that folder directly.</source>
          <target state="translated">기본적으로 빌드 파일이 생성되고 결과 라이브러리가 컴파일되는 &lt;code&gt;&amp;lt;tmp&amp;gt;/torch_extensions/&amp;lt;name&amp;gt;&lt;/code&gt; 는 &amp;lt;tmp&amp;gt; / torch_extensions / &amp;lt;name&amp;gt;입니다 . 여기서 &lt;code&gt;&amp;lt;tmp&amp;gt;&lt;/code&gt; 는 현재 플랫폼의 임시 폴더이고 &lt;code&gt;&amp;lt;name&amp;gt;&lt;/code&gt; 은 확장의 이름입니다. . 이 위치는 두 가지 방법으로 재정의 할 수 있습니다. 먼저 &lt;code&gt;TORCH_EXTENSIONS_DIR&lt;/code&gt; 환경 변수가 설정되면 &lt;code&gt;&amp;lt;tmp&amp;gt;/torch_extensions&lt;/code&gt; 를 대체 하고 모든 확장이이 디렉토리의 하위 폴더로 컴파일됩니다. 둘째, 이 함수에 대한 &lt;code&gt;build_directory&lt;/code&gt; 인수가 제공되면 전체 경로를 덮어 씁니다. 즉, 라이브러리가 해당 폴더로 직접 컴파일됩니다.</target>
        </trans-unit>
        <trans-unit id="836a1bf0b7e01bddc495fceffa973ccc50f2cdd2" translate="yes" xml:space="preserve">
          <source>By default, this layer uses instance statistics computed from input data in both training and evaluation modes.</source>
          <target state="translated">기본적으로이 계층은 훈련 및 평가 모드 모두에서 입력 데이터에서 계산 된 인스턴스 통계를 사용합니다.</target>
        </trans-unit>
        <trans-unit id="260db48b6c19b12ad8aaa2c223fbe53f044c91eb" translate="yes" xml:space="preserve">
          <source>By default, we decode byte strings as &lt;code&gt;utf-8&lt;/code&gt;. This is to avoid a common error case &lt;code&gt;UnicodeDecodeError: 'ascii' codec can't decode byte 0x...&lt;/code&gt; when loading files saved by Python 2 in Python 3. If this default is incorrect, you may use an extra &lt;code&gt;encoding&lt;/code&gt; keyword argument to specify how these objects should be loaded, e.g., &lt;code&gt;encoding='latin1'&lt;/code&gt; decodes them to strings using &lt;code&gt;latin1&lt;/code&gt; encoding, and &lt;code&gt;encoding='bytes'&lt;/code&gt; keeps them as byte arrays which can be decoded later with &lt;code&gt;byte_array.decode(...)&lt;/code&gt;.</source>
          <target state="translated">기본적으로 바이트 문자열을 &lt;code&gt;utf-8&lt;/code&gt; 로 디코딩 합니다. 이것은 일반적인 오류 사례를 방지하기위한 것입니다. &lt;code&gt;UnicodeDecodeError: 'ascii' codec can't decode byte 0x...&lt;/code&gt; Python 2에서 Python 3에서 저장 한 파일을로드 할 때 ... 이 기본값이 올바르지 않으면 추가 &lt;code&gt;encoding&lt;/code&gt; 키워드 인수를 사용하여 지정할 수 있습니다. 이러한 객체를로드하는 방법, 예를 들어 &lt;code&gt;encoding='latin1'&lt;/code&gt; 은 &lt;code&gt;latin1&lt;/code&gt; 인코딩을 사용하여 문자열로 디코딩 하고 &lt;code&gt;encoding='bytes'&lt;/code&gt; 는 나중에 &lt;code&gt;byte_array.decode(...)&lt;/code&gt; 로 디코딩 할 수있는 바이트 배열로 유지합니다 .</target>
        </trans-unit>
        <trans-unit id="5b3e53bd55890d668be711af0b27d6b4e4085bb4" translate="yes" xml:space="preserve">
          <source>By default, we don&amp;rsquo;t clean up files after loading it. Hub uses the cache by default if it already exists in the directory returned by &lt;a href=&quot;#torch.hub.get_dir&quot;&gt;&lt;code&gt;get_dir()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">기본적으로 파일을로드 한 후에는 정리하지 않습니다. Hub는 &lt;a href=&quot;#torch.hub.get_dir&quot;&gt; &lt;code&gt;get_dir()&lt;/code&gt; &lt;/a&gt; 의해 반환 된 디렉토리에 이미있는 경우 기본적으로 캐시를 사용합니다 .</target>
        </trans-unit>
        <trans-unit id="4d31b2b62d5a7017f58599071410bf4ed9eb8d68" translate="yes" xml:space="preserve">
          <source>By default, with &lt;code&gt;dim=0&lt;/code&gt;, the norm is computed independently per output channel/plane. To compute a norm over the entire weight tensor, use &lt;code&gt;dim=None&lt;/code&gt;.</source>
          <target state="translated">기본적으로 &lt;code&gt;dim=0&lt;/code&gt; 을 사용하면 표준이 출력 채널 / 평면별로 독립적으로 계산됩니다. 전체 가중치 텐서에 대한 노름을 계산하려면 &lt;code&gt;dim=None&lt;/code&gt; 을 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="32096c2e0eff33d844ee6d675407ace18289357d" translate="yes" xml:space="preserve">
          <source>C</source>
          <target state="translated">C</target>
        </trans-unit>
        <trans-unit id="eaef64b3192bf8d5502eee309124b228be2379ec" translate="yes" xml:space="preserve">
          <source>C = \log(\pi) \times \frac{p (p - 1)}{4}</source>
          <target state="translated">C = \ log (\ pi) \ times \ frac {p (p-1)} {4}</target>
        </trans-unit>
        <trans-unit id="4ee9f8327170df1bedff8fda035491204285f3c3" translate="yes" xml:space="preserve">
          <source>C = \text{number of classes (including blank)}</source>
          <target state="translated">C = \ text {클래스 수 (공백 포함)}</target>
        </trans-unit>
        <trans-unit id="f46c4b42af8b2196cc344f07319e812268c3c0dd" translate="yes" xml:space="preserve">
          <source>C \times \prod(\text{kernel\_size})</source>
          <target state="translated">C \ times \ prod (\ text {커널 \ _size})</target>
        </trans-unit>
        <trans-unit id="fc2b4216164cfb01ac45112054b3fedda8b56c86" translate="yes" xml:space="preserve">
          <source>C++</source>
          <target state="translated">C++</target>
        </trans-unit>
        <trans-unit id="a3d883aa22c9b3cbe1562cc3d62c063468b9a196" translate="yes" xml:space="preserve">
          <source>C=\text{num\_channels}</source>
          <target state="translated">C=\text{num\_channels}</target>
        </trans-unit>
        <trans-unit id="798a57343d5fc6a0cf122924f6ca62852b64f4a6" translate="yes" xml:space="preserve">
          <source>CELU</source>
          <target state="translated">CELU</target>
        </trans-unit>
        <trans-unit id="ff221d4752ce05f5a91bbf1d28b78a7bf7e2ddaa" translate="yes" xml:space="preserve">
          <source>CPU</source>
          <target state="translated">CPU</target>
        </trans-unit>
        <trans-unit id="d2b3baf18b41b52a701d4019f7ffaa284e02f53a" translate="yes" xml:space="preserve">
          <source>CPU hosts with Ethernet interconnect</source>
          <target state="translated">이더넷 상호 연결이있는 CPU 호스트</target>
        </trans-unit>
        <trans-unit id="bfea39d4ef79843c8c035774adc370d11a93b4e8" translate="yes" xml:space="preserve">
          <source>CPU hosts with InfiniBand interconnect</source>
          <target state="translated">InfiniBand 상호 연결이있는 CPU 호스트</target>
        </trans-unit>
        <trans-unit id="aca0030d1b8e86f8e968a622d4b61c5e238ad1fa" translate="yes" xml:space="preserve">
          <source>CPU tensor</source>
          <target state="translated">CPU 텐서</target>
        </trans-unit>
        <trans-unit id="1adcc1c5aae9ba0bc68cde14e7017456258e8921" translate="yes" xml:space="preserve">
          <source>CPU threading and TorchScript inference</source>
          <target state="translated">CPU 스레딩 및 TorchScript 추론</target>
        </trans-unit>
        <trans-unit id="5c435b722f2bc9152d11b1225cc3099efa162504" translate="yes" xml:space="preserve">
          <source>CTCLoss</source>
          <target state="translated">CTCLoss</target>
        </trans-unit>
        <trans-unit id="1ab6d957380a70ab72c7926a9d4fae8cf48ecf35" translate="yes" xml:space="preserve">
          <source>CUDA semantics</source>
          <target state="translated">CUDA 의미론</target>
        </trans-unit>
        <trans-unit id="3f5da429aac783fd0a3a0da898da1913e428656f" translate="yes" xml:space="preserve">
          <source>CUDA support with mixed compilation is provided. Simply pass CUDA source files (&lt;code&gt;.cu&lt;/code&gt; or &lt;code&gt;.cuh&lt;/code&gt;) along with other sources. Such files will be detected and compiled with nvcc rather than the C++ compiler. This includes passing the CUDA lib64 directory as a library directory, and linking &lt;code&gt;cudart&lt;/code&gt;. You can pass additional flags to nvcc via &lt;code&gt;extra_cuda_cflags&lt;/code&gt;, just like with &lt;code&gt;extra_cflags&lt;/code&gt; for C++. Various heuristics for finding the CUDA install directory are used, which usually work fine. If not, setting the &lt;code&gt;CUDA_HOME&lt;/code&gt; environment variable is the safest option.</source>
          <target state="translated">혼합 컴파일을 통한 CUDA 지원이 제공됩니다. CUDA 소스 파일 ( &lt;code&gt;.cu&lt;/code&gt; 또는 &lt;code&gt;.cuh&lt;/code&gt; )을 다른 소스와 함께 전달하기 만하면 됩니다. 이러한 파일은 C ++ 컴파일러가 아닌 nvcc로 감지되고 컴파일됩니다. 여기에는 CUDA lib64 디렉토리를 라이브러리 디렉토리로 전달하고 &lt;code&gt;cudart&lt;/code&gt; 연결이 포함 됩니다 . 당신은을 통해 NVCC에 추가 플래그를 전달할 수 있습니다 &lt;code&gt;extra_cuda_cflags&lt;/code&gt; 단지와 마찬가지로, &lt;code&gt;extra_cflags&lt;/code&gt; C ++합니다. CUDA 설치 디렉토리를 찾기위한 다양한 휴리스틱이 사용되며 일반적으로 잘 작동합니다. 그렇지 않은 경우 &lt;code&gt;CUDA_HOME&lt;/code&gt; 환경 변수를 설정하는 것이 가장 안전한 옵션입니다.</target>
        </trans-unit>
        <trans-unit id="5abf962ed164e31df7bbd04bfe694578c153b51d" translate="yes" xml:space="preserve">
          <source>Caching logic</source>
          <target state="translated">캐싱 로직</target>
        </trans-unit>
        <trans-unit id="e8a73f7e1b281ff72f0cd1f11baf202568648a12" translate="yes" xml:space="preserve">
          <source>Calculates determinant of a square matrix or batches of square matrices.</source>
          <target state="translated">정사각형 행렬의 행렬식 또는 정사각형 행렬의 배치를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="b469ab85777119e331c0d2b40b68cd19ac71da49" translate="yes" xml:space="preserve">
          <source>Calculates log determinant of a square matrix or batches of square matrices.</source>
          <target state="translated">정사각형 행렬 또는 정사각형 행렬의 배치의 로그 행렬식을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="dbc9fd03c3e295de099741d45013bec1bd5703f7" translate="yes" xml:space="preserve">
          <source>Calculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the probability of possible alignments of input to target, producing a loss value which is differentiable with respect to each input node. The alignment of input to target is assumed to be &amp;ldquo;many-to-one&amp;rdquo;, which limits the length of the target sequence such that it must be</source>
          <target state="translated">연속 (분할되지 않은) 시계열과 대상 시퀀스 간의 손실을 계산합니다. CTCLoss는 대상에 대한 입력 정렬 가능성을 합산하여 각 입력 노드에 대해 미분 할 수있는 손실 값을 생성합니다. 타겟에 대한 입력의 정렬은&amp;ldquo;다 대일&amp;rdquo;로 가정되며, 이는 타겟 시퀀스의 길이를 제한하여</target>
        </trans-unit>
        <trans-unit id="8b95dc9e720785341df91a709dd18edb57e422e8" translate="yes" xml:space="preserve">
          <source>Calculates pointwise</source>
          <target state="translated">포인트 단위로 계산</target>
        </trans-unit>
        <trans-unit id="3f73e5660ccb505df5953e1fbed92e6c8a064be1" translate="yes" xml:space="preserve">
          <source>Calculates the pseudo-inverse (also known as the Moore-Penrose inverse) of a 2D tensor.</source>
          <target state="translated">2D 텐서의 의사 역 (무어-펜로즈 역이라고도 함)을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="afbe6768e28de20bd0669bfb42db4b0e5335e016" translate="yes" xml:space="preserve">
          <source>Calculates the pseudo-inverse (also known as the Moore-Penrose inverse) of a 2D tensor. Please look at &lt;a href=&quot;https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse&quot;&gt;Moore-Penrose inverse&lt;/a&gt; for more details</source>
          <target state="translated">2D 텐서의 의사 역 (무어-펜로즈 역이라고도 함)을 계산합니다. 자세한 내용 은 &lt;a href=&quot;https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse&quot;&gt;Moore-Penrose inverse&lt;/a&gt; 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="bf3fa1ed9287f3a31249968ae80488bbbc90709d" translate="yes" xml:space="preserve">
          <source>Calculates the sign and log absolute value of the determinant(s) of a square matrix or batches of square matrices.</source>
          <target state="translated">정사각형 행렬 또는 정사각형 행렬 배치의 행렬식의 부호 및 로그 절대 값을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="bc5cae9078b78a08c10f30abb3ba9b5b22cfef54" translate="yes" xml:space="preserve">
          <source>Callables prefixed with underscore are considered as helper functions which won&amp;rsquo;t show up in &lt;a href=&quot;#torch.hub.list&quot;&gt;&lt;code&gt;torch.hub.list()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">밑줄로 시작하는 &lt;a href=&quot;#torch.hub.list&quot;&gt; &lt;code&gt;torch.hub.list()&lt;/code&gt; &lt;/a&gt; 은 torch.hub.list () 에 표시되지 않는 도우미 함수로 간주됩니다 .</target>
        </trans-unit>
        <trans-unit id="9c42beef7db382c9ae158419c3ebc5c8310023f1" translate="yes" xml:space="preserve">
          <source>Calling &lt;code&gt;hub.set_dir(&amp;lt;PATH_TO_HUB_DIR&amp;gt;)&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;hub.set_dir(&amp;lt;PATH_TO_HUB_DIR&amp;gt;)&lt;/code&gt; 호출</target>
        </trans-unit>
        <trans-unit id="265655dd73dcd39520c5cd9a954e45b04d35e8a1" translate="yes" xml:space="preserve">
          <source>Calling &lt;code&gt;torch.kaiser_window(L, B, periodic=True)&lt;/code&gt; is equivalent to calling &lt;code&gt;torch.kaiser_window(L + 1, B, periodic=False)[:-1])&lt;/code&gt;. The &lt;code&gt;periodic&lt;/code&gt; argument is intended as a helpful shorthand to produce a periodic window as input to functions like &lt;a href=&quot;torch.stft#torch.stft&quot;&gt;&lt;code&gt;torch.stft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">호출 &lt;code&gt;torch.kaiser_window(L, B, periodic=True)&lt;/code&gt; 호출하는 것과 &lt;code&gt;torch.kaiser_window(L + 1, B, periodic=False)[:-1])&lt;/code&gt; . &lt;code&gt;periodic&lt;/code&gt; 인수 같은 함수에 대한 입력으로 주기적 윈도우 제조하는 유용한 속기 마련된다 &lt;a href=&quot;torch.stft#torch.stft&quot;&gt; &lt;code&gt;torch.stft()&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="0d622de7b9cec6af9b50c591c0762f95f6cf23f1" translate="yes" xml:space="preserve">
          <source>Calling a submodule directly (e.g. &lt;code&gt;self.resnet(input)&lt;/code&gt;) is equivalent to calling its &lt;code&gt;forward&lt;/code&gt; method (e.g. &lt;code&gt;self.resnet.forward(input)&lt;/code&gt;).</source>
          <target state="translated">서브 모듈을 직접 호출하는 것은 (예 : &lt;code&gt;self.resnet(input)&lt;/code&gt; ) &lt;code&gt;forward&lt;/code&gt; 메소드 를 호출하는 것과 같습니다 (예 : &lt;code&gt;self.resnet.forward(input)&lt;/code&gt; ).</target>
        </trans-unit>
        <trans-unit id="1b7d84e5b4532eb537ef53414226540774bd94f3" translate="yes" xml:space="preserve">
          <source>Calling the backward transform (&lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">동일한 정규화 모드로 역방향 변환 ( &lt;a href=&quot;#torch.fft.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt; )을 호출하면 두 변환간에 &lt;code&gt;1/n&lt;/code&gt; 의 전체 정규화가 적용됩니다 . 이것은 &lt;a href=&quot;#torch.fft.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt; 를 정확한 역 으로 만들기 위해 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="57076e248ecf298e1219a0fa6ecf299084bfedd8" translate="yes" xml:space="preserve">
          <source>Calling the backward transform (&lt;a href=&quot;#torch.fft.ihfft&quot;&gt;&lt;code&gt;ihfft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ihfft&quot;&gt;&lt;code&gt;ihfft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">동일한 정규화 모드로 역방향 변환 ( &lt;a href=&quot;#torch.fft.ihfft&quot;&gt; &lt;code&gt;ihfft()&lt;/code&gt; &lt;/a&gt; )을 호출하면 두 변환간에 &lt;code&gt;1/n&lt;/code&gt; 의 전체 정규화가 적용됩니다 . 이것은 &lt;a href=&quot;#torch.fft.ihfft&quot;&gt; &lt;code&gt;ihfft()&lt;/code&gt; &lt;/a&gt; 를 정확한 역 으로 만들기 위해 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="6550668f3134865d4fd7ecb672c8b13821a60616" translate="yes" xml:space="preserve">
          <source>Calling the backward transform (&lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">동일한 정규화 모드로 역방향 변환 ( &lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt; )을 호출하면 두 변환간에 &lt;code&gt;1/n&lt;/code&gt; 의 전체 정규화가 적용됩니다 . 이것은 &lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt; 를 정확한 역 으로 만드는 데 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="393935e7b8f54e6c891a458a9a0a5b90620581df" translate="yes" xml:space="preserve">
          <source>Calling the forward transform (&lt;a href=&quot;#torch.fft.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">동일한 정규화 모드로 순방향 변환 ( &lt;a href=&quot;#torch.fft.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt; )을 호출하면 두 변환간에 &lt;code&gt;1/n&lt;/code&gt; 의 전체 정규화가 적용됩니다 . 이것은 &lt;a href=&quot;#torch.fft.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt; 를 정확한 역 으로 만들기 위해 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="3c4696c3c77b6fbb0767ee074a0b08401ad5fd2b" translate="yes" xml:space="preserve">
          <source>Calling the forward transform (&lt;a href=&quot;#torch.fft.hfft&quot;&gt;&lt;code&gt;hfft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.ihfft&quot;&gt;&lt;code&gt;ihfft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">동일한 정규화 모드로 순방향 변환 ( &lt;a href=&quot;#torch.fft.hfft&quot;&gt; &lt;code&gt;hfft()&lt;/code&gt; &lt;/a&gt; )을 호출하면 두 변환간에 &lt;code&gt;1/n&lt;/code&gt; 의 전체 정규화가 적용됩니다 . 이것은 &lt;a href=&quot;#torch.fft.ihfft&quot;&gt; &lt;code&gt;ihfft()&lt;/code&gt; &lt;/a&gt; 를 정확한 역 으로 만들기 위해 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="a0f807127c82b5c379fa18d9be9a714272f46e17" translate="yes" xml:space="preserve">
          <source>Calling the forward transform (&lt;a href=&quot;#torch.fft.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;) with the same normalization mode will apply an overall normalization of &lt;code&gt;1/n&lt;/code&gt; between the two transforms. This is required to make &lt;a href=&quot;#torch.fft.irfft&quot;&gt;&lt;code&gt;irfft()&lt;/code&gt;&lt;/a&gt; the exact inverse.</source>
          <target state="translated">동일한 정규화 모드로 순방향 변환 ( &lt;a href=&quot;#torch.fft.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt; )을 호출하면 두 변환간에 &lt;code&gt;1/n&lt;/code&gt; 의 전체 정규화가 적용됩니다 . 이것은 &lt;a href=&quot;#torch.fft.irfft&quot;&gt; &lt;code&gt;irfft()&lt;/code&gt; &lt;/a&gt; 를 정확한 역 으로 만드는 데 필요합니다 .</target>
        </trans-unit>
        <trans-unit id="76582af9585743776e20d4bdf66734ecbe7e7ff9" translate="yes" xml:space="preserve">
          <source>Calls to &lt;code&gt;builtin functions&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;builtin functions&lt;/code&gt; 호출</target>
        </trans-unit>
        <trans-unit id="eb07741ad617617e9abda7e3d52fee63a305a121" translate="yes" xml:space="preserve">
          <source>Calls to methods of builtin types like tensor: &lt;code&gt;x.mm(y)&lt;/code&gt;</source>
          <target state="translated">텐서와 같은 내장 유형의 메서드 호출 : &lt;code&gt;x.mm(y)&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="fc7e70542d9259610f72ca817bd3eb6584edd2c8" translate="yes" xml:space="preserve">
          <source>Calls to other script functions:</source>
          <target state="translated">다른 스크립트 함수 호출 :</target>
        </trans-unit>
        <trans-unit id="b6867b70db2065294481ad42aed53ba49e6355b8" translate="yes" xml:space="preserve">
          <source>Can also be used for higher dimension inputs, such as 2D images, by providing an input of size</source>
          <target state="translated">크기 입력을 제공하여 2D 이미지와 같은 고차원 입력에도 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="221c9205ab30df3bd1004805355ca477c455a15b" translate="yes" xml:space="preserve">
          <source>Can only be called once and before any inter-op parallel work is started (e.g. JIT execution).</source>
          <target state="translated">inter-op 병렬 작업이 시작되기 전에 한 번만 호출 할 수 있습니다 (예 : JIT 실행).</target>
        </trans-unit>
        <trans-unit id="3805fd56c2af8071d51bc53ae0a92fdcb58807db" translate="yes" xml:space="preserve">
          <source>Casting Examples:</source>
          <target state="translated">캐스팅 예 :</target>
        </trans-unit>
        <trans-unit id="e7500c883cdd17fa4172ea83911ebf91a32625de" translate="yes" xml:space="preserve">
          <source>Casts</source>
          <target state="translated">Casts</target>
        </trans-unit>
        <trans-unit id="26538798d0973ae83ca7715b676794ed3b172f33" translate="yes" xml:space="preserve">
          <source>Casts all floating point parameters and buffers to &lt;code&gt;bfloat16&lt;/code&gt; datatype.</source>
          <target state="translated">모든 부동 소수점 매개 변수와 버퍼를 &lt;code&gt;bfloat16&lt;/code&gt; 데이터 유형으로 캐스트합니다 .</target>
        </trans-unit>
        <trans-unit id="b6c764881902912eb6ba271fb55ce5179af34673" translate="yes" xml:space="preserve">
          <source>Casts all floating point parameters and buffers to &lt;code&gt;double&lt;/code&gt; datatype.</source>
          <target state="translated">모든 부동 소수점 매개 변수와 버퍼를 &lt;code&gt;double&lt;/code&gt; 데이터 유형으로 캐스트합니다 .</target>
        </trans-unit>
        <trans-unit id="78461ae0a43c2d54b7c9efa684628bba9ad1c1ed" translate="yes" xml:space="preserve">
          <source>Casts all floating point parameters and buffers to &lt;code&gt;half&lt;/code&gt; datatype.</source>
          <target state="translated">모든 부동 소수점 매개 변수와 버퍼를 &lt;code&gt;half&lt;/code&gt; 데이터 유형으로 캐스트합니다 .</target>
        </trans-unit>
        <trans-unit id="661d47897b8e53cbd52a45424e86044636652304" translate="yes" xml:space="preserve">
          <source>Casts all floating point parameters and buffers to float datatype.</source>
          <target state="translated">모든 부동 소수점 매개 변수 및 버퍼를 부동 데이터 유형으로 캐스트합니다.</target>
        </trans-unit>
        <trans-unit id="4724f395a64ded08f676d2e9e0393dcb0f712246" translate="yes" xml:space="preserve">
          <source>Casts all parameters and buffers to &lt;code&gt;dst_type&lt;/code&gt;.</source>
          <target state="translated">모든 매개 변수와 버퍼를 &lt;code&gt;dst_type&lt;/code&gt; 으로 캐스트합니다 .</target>
        </trans-unit>
        <trans-unit id="79f02a31265abcb1bb988f26d0c498d397fdd789" translate="yes" xml:space="preserve">
          <source>Casts this storage to bfloat16 type</source>
          <target state="translated">이 스토리지를 bfloat16 유형으로 캐스트합니다.</target>
        </trans-unit>
        <trans-unit id="bd487dabbd2c7187cd079ec504def8aaf3a7b571" translate="yes" xml:space="preserve">
          <source>Casts this storage to bool type</source>
          <target state="translated">이 저장소를 bool 유형으로 캐스팅합니다.</target>
        </trans-unit>
        <trans-unit id="6694cf255316b894b3ad2697cc3b38ec9452ca42" translate="yes" xml:space="preserve">
          <source>Casts this storage to byte type</source>
          <target state="translated">이 스토리지를 바이트 유형으로 캐스트</target>
        </trans-unit>
        <trans-unit id="151ff3570c6883e5b0af6409a5ae071d0e154dbc" translate="yes" xml:space="preserve">
          <source>Casts this storage to char type</source>
          <target state="translated">이 스토리지를 char 유형으로 캐스트합니다.</target>
        </trans-unit>
        <trans-unit id="d3e9e6a46615473165fa7e0ec3e61fb30d1e8866" translate="yes" xml:space="preserve">
          <source>Casts this storage to complex double type</source>
          <target state="translated">이 저장소를 복잡한 이중 유형으로 캐스팅합니다.</target>
        </trans-unit>
        <trans-unit id="510b5fa8a83cea216c27cd3b246f5ff67c971a64" translate="yes" xml:space="preserve">
          <source>Casts this storage to complex float type</source>
          <target state="translated">이 저장소를 복잡한 플로트 유형으로 캐스팅합니다.</target>
        </trans-unit>
        <trans-unit id="2e339cd7fb3f62430cb2f45b27e9e325c9fd432f" translate="yes" xml:space="preserve">
          <source>Casts this storage to double type</source>
          <target state="translated">이 저장소를 더블 유형으로 캐스팅합니다.</target>
        </trans-unit>
        <trans-unit id="46bf8db49512235eab20518ddd006bc28b925a2c" translate="yes" xml:space="preserve">
          <source>Casts this storage to float type</source>
          <target state="translated">이 저장소를 float 유형으로 캐스팅합니다.</target>
        </trans-unit>
        <trans-unit id="63e25d02d152bfc3b6809c0d00d52a9786a45699" translate="yes" xml:space="preserve">
          <source>Casts this storage to half type</source>
          <target state="translated">이 스토리지를 하프 유형으로 캐스팅합니다.</target>
        </trans-unit>
        <trans-unit id="8dd4507b920a808cec77926ae75ebeb4c2c63230" translate="yes" xml:space="preserve">
          <source>Casts this storage to int type</source>
          <target state="translated">이 저장소를 int 유형으로 캐스팅합니다.</target>
        </trans-unit>
        <trans-unit id="afab2e0bade340b8cecdf6e13b386611e22e2730" translate="yes" xml:space="preserve">
          <source>Casts this storage to long type</source>
          <target state="translated">이 저장소를 긴 유형으로 캐스팅합니다.</target>
        </trans-unit>
        <trans-unit id="d4ce7dd1afcf534c203c4afd8fc54fe512dfe75d" translate="yes" xml:space="preserve">
          <source>Casts this storage to short type</source>
          <target state="translated">이 저장소를 짧은 유형으로 캐스팅합니다.</target>
        </trans-unit>
        <trans-unit id="795a0c324ff1f130803359d377842d67e3339ceb" translate="yes" xml:space="preserve">
          <source>Change if autograd should record operations on parameters in this module.</source>
          <target state="translated">autograd에서이 모듈의 매개 변수에 대한 작업을 기록해야하는지 변경합니다.</target>
        </trans-unit>
        <trans-unit id="771633aa9e4ffd6ed00dfc406fa17e09d36a9380" translate="yes" xml:space="preserve">
          <source>Change if autograd should record operations on this tensor: sets this tensor&amp;rsquo;s &lt;a href=&quot;autograd#torch.Tensor.requires_grad&quot;&gt;&lt;code&gt;requires_grad&lt;/code&gt;&lt;/a&gt; attribute in-place. Returns this tensor.</source>
          <target state="translated">autograd가이 텐서에서 작업을 기록해야하는지 변경 :이 텐서의 &lt;a href=&quot;autograd#torch.Tensor.requires_grad&quot;&gt; &lt;code&gt;requires_grad&lt;/code&gt; &lt;/a&gt; 속성을 제자리에 설정합니다. 이 텐서를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="df974d2905d5cd175e8ce062cca16506fc69a1da" translate="yes" xml:space="preserve">
          <source>Channel dim is the 2nd dim of input. When input has dims &amp;lt; 2, then there is no channel dim and the number of channels = 1.</source>
          <target state="translated">Channel dim은 입력의 두 번째 dim입니다. 입력이 희미한 2 미만이면 채널이 희미 해지고 채널 수가 1입니다.</target>
        </trans-unit>
        <trans-unit id="56ff021269456a017773120e4ed3d4af95c8ae59" translate="yes" xml:space="preserve">
          <source>Check whether &lt;code&gt;module&lt;/code&gt; is pruned by looking for &lt;code&gt;forward_pre_hooks&lt;/code&gt; in its modules that inherit from the &lt;a href=&quot;torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod&quot;&gt;&lt;code&gt;BasePruningMethod&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod&quot;&gt; &lt;code&gt;BasePruningMethod&lt;/code&gt; &lt;/a&gt; 에서 상속 된 모듈에서 &lt;code&gt;forward_pre_hooks&lt;/code&gt; 를 찾아 &lt;code&gt;module&lt;/code&gt; 이 정리 되었는지 확인합니다 .</target>
        </trans-unit>
        <trans-unit id="e4c8690ed4ddc9de7d901e88e48d1d54b0026ac9" translate="yes" xml:space="preserve">
          <source>Check whether &lt;code&gt;module&lt;/code&gt; is pruned by looking for &lt;code&gt;forward_pre_hooks&lt;/code&gt; in its modules that inherit from the &lt;code&gt;BasePruningMethod&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;BasePruningMethod&lt;/code&gt; 에서 상속 된 모듈에서 &lt;code&gt;forward_pre_hooks&lt;/code&gt; 를 찾아 &lt;code&gt;module&lt;/code&gt; 이 정리 되었는지 확인합니다 .</target>
        </trans-unit>
        <trans-unit id="b3a48ae8d9eebbbcfe68f114073dd271c96bf314" translate="yes" xml:space="preserve">
          <source>Check whether it&amp;rsquo;s in the middle of the ONNX export. This function returns True in the middle of torch.onnx.export(). torch.onnx.export should be executed with single thread.</source>
          <target state="translated">ONNX 내보내기 중간에 있는지 확인하십시오. 이 함수는 torch.onnx.export () 중간에 True를 반환합니다. torch.onnx.export는 단일 스레드로 실행해야합니다.</target>
        </trans-unit>
        <trans-unit id="e151686a765ce214247b375cc4d7aa1c97e4e14f" translate="yes" xml:space="preserve">
          <source>Checking if the default process group has been initialized</source>
          <target state="translated">기본 프로세스 그룹이 초기화되었는지 확인</target>
        </trans-unit>
        <trans-unit id="b37b3cf9158406adf7b90792ec9193ec0e0c1df6" translate="yes" xml:space="preserve">
          <source>Checkpoint a model or part of the model</source>
          <target state="translated">모델 또는 모델의 일부를 체크 포인트</target>
        </trans-unit>
        <trans-unit id="95fac7ec70656fe8c623fe8fbf3c5c04c9b16115" translate="yes" xml:space="preserve">
          <source>Checkpointing doesn&amp;rsquo;t work with &lt;a href=&quot;autograd#torch.autograd.grad&quot;&gt;&lt;code&gt;torch.autograd.grad()&lt;/code&gt;&lt;/a&gt;, but only with &lt;a href=&quot;autograd#torch.autograd.backward&quot;&gt;&lt;code&gt;torch.autograd.backward()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">검사 점은 작동하지 않습니다 &lt;a href=&quot;autograd#torch.autograd.grad&quot;&gt; &lt;code&gt;torch.autograd.grad()&lt;/code&gt; &lt;/a&gt; , 만에 &lt;a href=&quot;autograd#torch.autograd.backward&quot;&gt; &lt;code&gt;torch.autograd.backward()&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="f51f71651c6e78eb334ceb82bf5f372ed3b9e12e" translate="yes" xml:space="preserve">
          <source>Checkpointing is implemented by rerunning a forward-pass segment for each checkpointed segment during backward. This can cause persistent states like the RNG state to be advanced than they would without checkpointing. By default, checkpointing includes logic to juggle the RNG state such that checkpointed passes making use of RNG (through dropout for example) have deterministic output as compared to non-checkpointed passes. The logic to stash and restore RNG states can incur a moderate performance hit depending on the runtime of checkpointed operations. If deterministic output compared to non-checkpointed passes is not required, supply &lt;code&gt;preserve_rng_state=False&lt;/code&gt; to &lt;code&gt;checkpoint&lt;/code&gt; or &lt;code&gt;checkpoint_sequential&lt;/code&gt; to omit stashing and restoring the RNG state during each checkpoint.</source>
          <target state="translated">체크 포인트는 역방향 동안 각 체크 포인트 세그먼트에 대해 순방향 통과 세그먼트를 다시 실행하여 구현됩니다. 이로 인해 RNG 상태와 같은 지속적인 상태가 체크 포인트없이 진행될 수 있습니다. 기본적으로 체크 포인팅에는 RNG를 사용하는 체크 포인트 패스 (예 : 드롭 아웃을 통해)가 비 체크 포인트 패스와 비교하여 결정적 출력을 갖도록 RNG 상태를 저글링하는 로직이 포함됩니다. RNG 상태를 숨기고 복원하는 논리는 검사 점 작업의 런타임에 따라 중간 성능 저하를 초래할 수 있습니다. 체크 포인트되지 않은 패스와 비교 한 결정적 출력이 필요하지 않은 경우 &lt;code&gt;preserve_rng_state=False&lt;/code&gt; 를 &lt;code&gt;checkpoint&lt;/code&gt; 또는 &lt;code&gt;checkpoint_sequential&lt;/code&gt; 에 제공하십시오. 각 체크 포인트 동안 은닉 및 RNG 상태 복원을 생략합니다.</target>
        </trans-unit>
        <trans-unit id="332b6971e8e3a29969ff874b4073e4272d2a843f" translate="yes" xml:space="preserve">
          <source>Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does &lt;strong&gt;not&lt;/strong&gt; save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model.</source>
          <target state="translated">체크 포인트는 컴퓨팅을 메모리와 거래하여 작동합니다. 역방향 계산을 위해 전체 계산 그래프의 모든 중간 활성화를 저장하는 대신 체크 포인트 부분은 중간 활성화를 저장 하지 &lt;strong&gt;않고&lt;/strong&gt; 대신 역방향 패스에서 다시 계산합니다. 모델의 모든 부분에 적용 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="39f6e80dd8c8ac31e71117491def641d00a288d1" translate="yes" xml:space="preserve">
          <source>Checks if tensor is in shared memory.</source>
          <target state="translated">텐서가 공유 메모리에 있는지 확인합니다.</target>
        </trans-unit>
        <trans-unit id="19004c9ecdb08358c5474afc574d3f10f8cf3c71" translate="yes" xml:space="preserve">
          <source>Checks if the MPI backend is available.</source>
          <target state="translated">MPI 백엔드를 사용할 수 있는지 확인합니다.</target>
        </trans-unit>
        <trans-unit id="f2d9da2e3f6bd1dd91d87846cd5b419112d24146" translate="yes" xml:space="preserve">
          <source>Checks if the NCCL backend is available.</source>
          <target state="translated">NCCL 백엔드를 사용할 수 있는지 확인합니다.</target>
        </trans-unit>
        <trans-unit id="56268a6e36c10e47601c6c33d1ee9618154a6c67" translate="yes" xml:space="preserve">
          <source>Choosing the network interface to use</source>
          <target state="translated">사용할 네트워크 인터페이스 선택</target>
        </trans-unit>
        <trans-unit id="d9eaceff91c313f7cab180625e8b4dd09baebd23" translate="yes" xml:space="preserve">
          <source>Clamp all elements in &lt;code&gt;input&lt;/code&gt; into the range &lt;code&gt;[&lt;/code&gt;&lt;a href=&quot;generated/torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;generated/torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt;&lt;code&gt;]&lt;/code&gt; and return a resulting tensor:</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 모든 요소를 &lt;code&gt;[&lt;/code&gt; &lt;a href=&quot;generated/torch.min#torch.min&quot;&gt; &lt;code&gt;min&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;generated/torch.max#torch.max&quot;&gt; &lt;code&gt;max&lt;/code&gt; &lt;/a&gt; &lt;code&gt;]&lt;/code&gt; 범위 로 고정하고 결과 텐서를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="378e42a306697a4dd63aa3dd31b3c55c600db5d2" translate="yes" xml:space="preserve">
          <source>Clamp all elements in &lt;code&gt;input&lt;/code&gt; into the range &lt;code&gt;[&lt;/code&gt;&lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt;&lt;code&gt;]&lt;/code&gt; and return a resulting tensor:</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 모든 요소를 &lt;code&gt;[&lt;/code&gt; &lt;a href=&quot;torch.min#torch.min&quot;&gt; &lt;code&gt;min&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;torch.max#torch.max&quot;&gt; &lt;code&gt;max&lt;/code&gt; &lt;/a&gt; &lt;code&gt;]&lt;/code&gt; 범위 로 고정하고 결과 텐서를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="cfe592b543afcbd470b6fcfa84bfb4061fbeb434" translate="yes" xml:space="preserve">
          <source>Clamps all elements in &lt;code&gt;input&lt;/code&gt; to be larger or equal &lt;a href=&quot;torch.min#torch.min&quot;&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 모든 요소를 &lt;a href=&quot;torch.min#torch.min&quot;&gt; &lt;code&gt;min&lt;/code&gt; &lt;/a&gt; 보다 크거나 같도록 클램프합니다 .</target>
        </trans-unit>
        <trans-unit id="e7bab3a95a3c2fe6a5a776d67024bda46c058f02" translate="yes" xml:space="preserve">
          <source>Clamps all elements in &lt;code&gt;input&lt;/code&gt; to be smaller or equal &lt;a href=&quot;torch.max#torch.max&quot;&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 모든 요소를 &lt;a href=&quot;torch.max#torch.max&quot;&gt; &lt;code&gt;max&lt;/code&gt; &lt;/a&gt; 보다 작거나 같게 고정 합니다.</target>
        </trans-unit>
        <trans-unit id="a8943ee6c6160fec8979faa9096fac2e19ce7bfd" translate="yes" xml:space="preserve">
          <source>Classes must be new-style classes, as we use &lt;code&gt;__new__()&lt;/code&gt; to construct them with pybind11.</source>
          <target state="translated">&lt;code&gt;__new__()&lt;/code&gt; 를 사용하여 pybind11 로 구성하므로 클래스는 새로운 스타일의 클래스 여야합니다 .</target>
        </trans-unit>
        <trans-unit id="9b6b05a782b6d150aaf1e98e0fdbbcbc865ca9be" translate="yes" xml:space="preserve">
          <source>Classes that inherit from &lt;code&gt;torch.jit.ScriptModule&lt;/code&gt;</source>
          <target state="translated">&lt;code&gt;torch.jit.ScriptModule&lt;/code&gt; 에서 상속되는 클래스</target>
        </trans-unit>
        <trans-unit id="94c2a3189e7f7885455350c4c7a8df2d0d6ad1d1" translate="yes" xml:space="preserve">
          <source>Classification</source>
          <target state="translated">Classification</target>
        </trans-unit>
        <trans-unit id="140c2943a67faf9ddd529fa9ef19fca05ebb9209" translate="yes" xml:space="preserve">
          <source>Clears the cuFFT plan cache.</source>
          <target state="translated">cuFFT 계획 캐시를 지 웁니다.</target>
        </trans-unit>
        <trans-unit id="026a5307ef4b381234e9524521b9341950a6f9cf" translate="yes" xml:space="preserve">
          <source>Clip acc@1</source>
          <target state="translated">클립 acc @ 1</target>
        </trans-unit>
        <trans-unit id="a7ba9ed0f0f5ac74d1bfd4abe70aaeb95921f947" translate="yes" xml:space="preserve">
          <source>Clip acc@5</source>
          <target state="translated">클립 acc @ 5</target>
        </trans-unit>
        <trans-unit id="75ccb1e878d6b2fcd6f1473f8a7c353d6b2e4806" translate="yes" xml:space="preserve">
          <source>Clips gradient norm of an iterable of parameters.</source>
          <target state="translated">반복 가능한 매개 변수의 그라디언트 표준을 자릅니다.</target>
        </trans-unit>
        <trans-unit id="990a2e75895b3d0ccfe4b20a82d20496b3e8ee0f" translate="yes" xml:space="preserve">
          <source>Clips gradient of an iterable of parameters at specified value.</source>
          <target state="translated">지정된 값에서 반복 가능한 매개 변수의 그라디언트를 자릅니다.</target>
        </trans-unit>
        <trans-unit id="2b8f8ed2e1db909d591f68c446a99d83144eb890" translate="yes" xml:space="preserve">
          <source>Code running on Node 0</source>
          <target state="translated">노드 0에서 실행되는 코드</target>
        </trans-unit>
        <trans-unit id="4a4d07ff7beaaa363e77cbea9319ab7a99ad01d0" translate="yes" xml:space="preserve">
          <source>Code running on Node 1</source>
          <target state="translated">노드 1에서 실행되는 코드</target>
        </trans-unit>
        <trans-unit id="bfa7c14251111856333ef843853b74b24bdf052b" translate="yes" xml:space="preserve">
          <source>Collective functions</source>
          <target state="translated">집단 기능</target>
        </trans-unit>
        <trans-unit id="43092611315cdcde3137662c72b15d7a3a889e75" translate="yes" xml:space="preserve">
          <source>Collects the provided &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; objects into a single combined &lt;a href=&quot;#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; that is completed when all of the sub-futures are completed.</source>
          <target state="translated">제공된 &lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt; 객체를 모든 하위 &lt;a href=&quot;#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt; 가 완료 될 때 완료 되는 단일 결합 Future 로 수집합니다 .</target>
        </trans-unit>
        <trans-unit id="ce079595058d15f2351c808e0ae1b284bea0c578" translate="yes" xml:space="preserve">
          <source>Combines an array of sliding local blocks into a large containing tensor.</source>
          <target state="translated">슬라이딩 로컬 블록 배열을 큰 포함 텐서로 결합합니다.</target>
        </trans-unit>
        <trans-unit id="46488d0a373ab0b6babd0f93cccfb474321ae981" translate="yes" xml:space="preserve">
          <source>Combining Distributed DataParallel with Distributed RPC Framework</source>
          <target state="translated">Distributed DataParallel과 Distributed RPC 프레임 워크 결합</target>
        </trans-unit>
        <trans-unit id="c170217f39333026f4f3aa0323fa905552560ec9" translate="yes" xml:space="preserve">
          <source>Common environment variables</source>
          <target state="translated">공통 환경 변수</target>
        </trans-unit>
        <trans-unit id="086cb07d3ecde2b840e1f458bfa9fda481174a5f" translate="yes" xml:space="preserve">
          <source>Common linear algebra operations.</source>
          <target state="translated">일반적인 선형 대수 연산.</target>
        </trans-unit>
        <trans-unit id="b4f955c58ceb08791953e5d03b53e986ad69907d" translate="yes" xml:space="preserve">
          <source>Commonly used ones include the following for debugging purposes:</source>
          <target state="translated">일반적으로 사용되는 것은 디버깅 목적으로 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="bfd58ee3a270f3a931009900e1008d549bbd7453" translate="yes" xml:space="preserve">
          <source>Community</source>
          <target state="translated">Community</target>
        </trans-unit>
        <trans-unit id="e0c1faa1db1f49518cfef07ea955debfa4db607f" translate="yes" xml:space="preserve">
          <source>Compare against the full output from &lt;a href=&quot;#torch.fft.fft&quot;&gt;&lt;code&gt;fft()&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">&lt;a href=&quot;#torch.fft.fft&quot;&gt; &lt;code&gt;fft()&lt;/code&gt; &lt;/a&gt; 의 전체 출력과 비교하십시오 .</target>
        </trans-unit>
        <trans-unit id="0ccc8d4186f8b576455c6bf986c275fa4e1ca508" translate="yes" xml:space="preserve">
          <source>Compare against the full output from &lt;a href=&quot;#torch.fft.ifft&quot;&gt;&lt;code&gt;ifft()&lt;/code&gt;&lt;/a&gt;:</source>
          <target state="translated">&lt;a href=&quot;#torch.fft.ifft&quot;&gt; &lt;code&gt;ifft()&lt;/code&gt; &lt;/a&gt; 의 전체 출력과 비교하십시오 .</target>
        </trans-unit>
        <trans-unit id="d9194e4e84d8475d85c26bc6d3d0aabdb9a3cfba" translate="yes" xml:space="preserve">
          <source>Compared against the full output from &lt;a href=&quot;#torch.fft.fftn&quot;&gt;&lt;code&gt;fftn()&lt;/code&gt;&lt;/a&gt;, we have all elements up to the Nyquist frequency.</source>
          <target state="translated">&lt;a href=&quot;#torch.fft.fftn&quot;&gt; &lt;code&gt;fftn()&lt;/code&gt; &lt;/a&gt; 의 전체 출력과 비교 하면 Nyquist 주파수까지 모든 요소가 있습니다.</target>
        </trans-unit>
        <trans-unit id="25acda77b0bc6b058bd349dd6f4a271e2a967bfe" translate="yes" xml:space="preserve">
          <source>Comparison Operators</source>
          <target state="translated">비교 연산자</target>
        </trans-unit>
        <trans-unit id="64e945410bd38eb8bc541fa4fe0d1a5601a0a7ba" translate="yes" xml:space="preserve">
          <source>Comparison Ops</source>
          <target state="translated">비교 운영</target>
        </trans-unit>
        <trans-unit id="c73def212afdc811169afd7e77aebfbeecb5facc" translate="yes" xml:space="preserve">
          <source>Complex Numbers</source>
          <target state="translated">복소수</target>
        </trans-unit>
        <trans-unit id="258ac8f61c6506a35c0dfc4029a4818ba9555f2a" translate="yes" xml:space="preserve">
          <source>Complex values are infinite when their real or imaginary part is infinite.</source>
          <target state="translated">실수 또는 허수 부분이 무한 할 때 복잡한 값은 무한합니다.</target>
        </trans-unit>
        <trans-unit id="20602cb78c6ade6e08a8e69fafec22e07a7a725f" translate="yes" xml:space="preserve">
          <source>Complex-to-complex Discrete Fourier Transform.</source>
          <target state="translated">복합에서 복합 이산 푸리에 변환.</target>
        </trans-unit>
        <trans-unit id="a7075fa71ba5aa25f75795b49f1a3a1c5779af40" translate="yes" xml:space="preserve">
          <source>Complex-to-complex Inverse Discrete Fourier Transform.</source>
          <target state="translated">복소수에서 복소수 역 이산 푸리에 변환.</target>
        </trans-unit>
        <trans-unit id="cfb4fe1390cd2aa35fb926daaca343f331617fd7" translate="yes" xml:space="preserve">
          <source>Complex-to-real Inverse Discrete Fourier Transform.</source>
          <target state="translated">복소수에서 실제로의 역 이산 푸리에 변환.</target>
        </trans-unit>
        <trans-unit id="49a3dcb8dae28aaaa8933d10f9d1fc99995fdb01" translate="yes" xml:space="preserve">
          <source>Compute combinations of length</source>
          <target state="translated">길이 조합 계산</target>
        </trans-unit>
        <trans-unit id="ad88b5c709893b3c20a622a8af743326fc15e581" translate="yes" xml:space="preserve">
          <source>Computes</source>
          <target state="translated">Computes</target>
        </trans-unit>
        <trans-unit id="b29ddfa9b93c0c21e2cd9673bf929a7688cb085a" translate="yes" xml:space="preserve">
          <source>Computes &lt;code&gt;input&lt;/code&gt; divided by &lt;code&gt;other&lt;/code&gt;, elementwise, and rounds each quotient towards zero. Equivalently, it truncates the quotient(s):</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 값을 &lt;code&gt;other&lt;/code&gt; 로 나눈 값을 요소 별로 계산 하고 각 몫을 0으로 반올림합니다. 동등하게, 몫을 자릅니다.</target>
        </trans-unit>
        <trans-unit id="d3181f8cc8e8fa272eef8d4d3ff7058798e00f63" translate="yes" xml:space="preserve">
          <source>Computes a QR decomposition of &lt;code&gt;input&lt;/code&gt;, but without constructing</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 의 QR 분해를 계산 하지만 구성하지 않습니다.</target>
        </trans-unit>
        <trans-unit id="f69a85669ba4f546c3f6c75d8d8a225191c83e6a" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;a href=&quot;torch.nn.maxpool1d#torch.nn.MaxPool1d&quot;&gt;&lt;code&gt;MaxPool1d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.maxpool1d#torch.nn.MaxPool1d&quot;&gt; &lt;code&gt;MaxPool1d&lt;/code&gt; &lt;/a&gt; 의 부분 역을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="bf04c1ac66a51bee90d0426e3c8106a0e4185416" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;a href=&quot;torch.nn.maxpool2d#torch.nn.MaxPool2d&quot;&gt;&lt;code&gt;MaxPool2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.maxpool2d#torch.nn.MaxPool2d&quot;&gt; &lt;code&gt;MaxPool2d&lt;/code&gt; &lt;/a&gt; 의 부분 역을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="f360cd6902e1a0579d80b170c5497449c1afe016" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;a href=&quot;torch.nn.maxpool3d#torch.nn.MaxPool3d&quot;&gt;&lt;code&gt;MaxPool3d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.nn.maxpool3d#torch.nn.MaxPool3d&quot;&gt; &lt;code&gt;MaxPool3d&lt;/code&gt; &lt;/a&gt; 의 부분 역을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="26e6d25fb90ed6ad09263d6546b96e18a94d0c3e" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;code&gt;MaxPool1d&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;MaxPool1d&lt;/code&gt; 의 부분 역을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="18c31c026887e211a21e3dc7927a9d793872976e" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;code&gt;MaxPool2d&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;MaxPool2d&lt;/code&gt; 의 부분 역을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="85ab6bb517a0364e7f136e092fa61b3c016286ff" translate="yes" xml:space="preserve">
          <source>Computes a partial inverse of &lt;code&gt;MaxPool3d&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;MaxPool3d&lt;/code&gt; 의 부분 역을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="b7cc19759037b27ca2d7fb79591e62353c0937a7" translate="yes" xml:space="preserve">
          <source>Computes and returns a mask for the input tensor &lt;code&gt;t&lt;/code&gt;. Starting from a base &lt;code&gt;default_mask&lt;/code&gt; (which should be a mask of ones if the tensor has not been pruned yet), generate a mask to apply on top of the &lt;code&gt;default_mask&lt;/code&gt; by zeroing out the channels along the specified dim with the lowest Ln-norm.</source>
          <target state="translated">입력 텐서 &lt;code&gt;t&lt;/code&gt; 에 대한 마스크를 계산하고 반환합니다 . 기본 &lt;code&gt;default_mask&lt;/code&gt; (텐서가 아직 정리되지 않은 경우 1의 마스크 여야 함) 에서 시작 하여 &lt;code&gt;default_mask&lt;/code&gt; 노름이 가장 낮은 지정된 dim을 따라 채널을 0으로 제거하여 default_mask 위에 적용 할 마스크를 생성합니다 .</target>
        </trans-unit>
        <trans-unit id="40d80ee71e511c62ded894cfa5f39bc5b20875f0" translate="yes" xml:space="preserve">
          <source>Computes and returns a mask for the input tensor &lt;code&gt;t&lt;/code&gt;. Starting from a base &lt;code&gt;default_mask&lt;/code&gt; (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the &lt;code&gt;default_mask&lt;/code&gt; according to the specific pruning method recipe.</source>
          <target state="translated">입력 텐서 &lt;code&gt;t&lt;/code&gt; 에 대한 마스크를 계산하고 반환합니다 . 기본 &lt;code&gt;default_mask&lt;/code&gt; (텐서가 아직 프 루닝되지 않은 경우 마스크 여야 함)에서 시작 하여 특정 프 루닝 방법 레시피에 따라 &lt;code&gt;default_mask&lt;/code&gt; 위에 적용 할 임의 마스크를 생성합니다 .</target>
        </trans-unit>
        <trans-unit id="65434598b41da121b29ca65831d67382801ae0c7" translate="yes" xml:space="preserve">
          <source>Computes and returns a mask for the input tensor &lt;code&gt;t&lt;/code&gt;. Starting from a base &lt;code&gt;default_mask&lt;/code&gt; (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the &lt;code&gt;default_mask&lt;/code&gt; by randomly zeroing out channels along the specified dim of the tensor.</source>
          <target state="translated">입력 텐서 &lt;code&gt;t&lt;/code&gt; 에 대한 마스크를 계산하고 반환합니다 . 기본 &lt;code&gt;default_mask&lt;/code&gt; (텐서가 아직 정리되지 않은 경우 1의 마스크 여야 함) 에서 시작 하여 텐서 의 지정된 dim을 따라 채널을 무작위로 제로화 하여 &lt;code&gt;default_mask&lt;/code&gt; 위에 적용 할 임의 마스크를 생성합니다 .</target>
        </trans-unit>
        <trans-unit id="180bc7b0d8de7293424fe248fc89cd2c995f979c" translate="yes" xml:space="preserve">
          <source>Computes and returns a pruned version of input tensor &lt;code&gt;t&lt;/code&gt; according to the pruning rule specified in &lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod.compute_mask&quot;&gt;&lt;code&gt;compute_mask()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.utils.prune.BasePruningMethod.compute_mask&quot;&gt; &lt;code&gt;compute_mask()&lt;/code&gt; &lt;/a&gt; 지정된 정리 규칙에 따라 입력 텐서 &lt;code&gt;t&lt;/code&gt; 의 정리 된 버전을 계산하고 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="7a3fb50e3449e8908990f0c00a969ef20d83c411" translate="yes" xml:space="preserve">
          <source>Computes and returns a pruned version of input tensor &lt;code&gt;t&lt;/code&gt; according to the pruning rule specified in &lt;a href=&quot;#torch.nn.utils.prune.LnStructured.compute_mask&quot;&gt;&lt;code&gt;compute_mask()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.utils.prune.LnStructured.compute_mask&quot;&gt; &lt;code&gt;compute_mask()&lt;/code&gt; &lt;/a&gt; 지정된 정리 규칙에 따라 입력 텐서 &lt;code&gt;t&lt;/code&gt; 의 정리 된 버전을 계산하고 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="9efff4b9b21781ca7329ebc9ab81c9ec882d85a1" translate="yes" xml:space="preserve">
          <source>Computes and returns a pruned version of input tensor &lt;code&gt;t&lt;/code&gt; according to the pruning rule specified in &lt;a href=&quot;#torch.nn.utils.prune.PruningContainer.compute_mask&quot;&gt;&lt;code&gt;compute_mask()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.utils.prune.PruningContainer.compute_mask&quot;&gt; &lt;code&gt;compute_mask()&lt;/code&gt; &lt;/a&gt; 지정된 정리 규칙에 따라 입력 텐서 &lt;code&gt;t&lt;/code&gt; 의 정리 된 버전을 계산하고 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="f99800562bd522721e9ea1fa3102c89a24cca75f" translate="yes" xml:space="preserve">
          <source>Computes and returns a pruned version of input tensor &lt;code&gt;t&lt;/code&gt; according to the pruning rule specified in &lt;a href=&quot;#torch.nn.utils.prune.RandomStructured.compute_mask&quot;&gt;&lt;code&gt;compute_mask()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;#torch.nn.utils.prune.RandomStructured.compute_mask&quot;&gt; &lt;code&gt;compute_mask()&lt;/code&gt; &lt;/a&gt; 지정된 정리 규칙에 따라 입력 텐서 &lt;code&gt;t&lt;/code&gt; 의 정리 된 버전을 계산하고 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="28c95a2d472ee12c714c4dae6bc27475d57d2a6e" translate="yes" xml:space="preserve">
          <source>Computes and returns a pruned version of input tensor &lt;code&gt;t&lt;/code&gt; according to the pruning rule specified in &lt;code&gt;compute_mask()&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;compute_mask()&lt;/code&gt; 지정된 정리 규칙에 따라 입력 텐서 &lt;code&gt;t&lt;/code&gt; 의 정리 된 버전을 계산하고 반환합니다 .</target>
        </trans-unit>
        <trans-unit id="2ad7888f31836fca25fd59e8bf8deaf0775bd8bb" translate="yes" xml:space="preserve">
          <source>Computes batched the p-norm distance between each pair of the two collections of row vectors.</source>
          <target state="translated">두 행 벡터 모음의 각 쌍 사이에 배치 된 p- 노름 거리를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="f2e6dd93c4230ac985c6eeef22bb5e42799d351c" translate="yes" xml:space="preserve">
          <source>Computes element-wise equality</source>
          <target state="translated">요소 별 동등성을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="23c55c6419438b7d4003d50aab0fa866dc6b754c" translate="yes" xml:space="preserve">
          <source>Computes log probabilities for all</source>
          <target state="translated">모두에 대한 로그 확률을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="bffd347218e0b14edb14dc921db35164acedc246" translate="yes" xml:space="preserve">
          <source>Computes sums or means of &amp;lsquo;bags&amp;rsquo; of embeddings, without instantiating the intermediate embeddings.</source>
          <target state="translated">중간 임베딩을 인스턴스화하지 않고 임베딩 '백'의 합계 또는 수단을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="a7d9c1eb8347c9d1cc0af2b2b967716bd8c4ae77" translate="yes" xml:space="preserve">
          <source>Computes sums, means or maxes of &lt;code&gt;bags&lt;/code&gt; of embeddings, without instantiating the intermediate embeddings.</source>
          <target state="translated">중간 임베딩을 인스턴스화하지 않고 임베딩 &lt;code&gt;bags&lt;/code&gt; 합계, 평균 또는 최대 값을 계산 합니다.</target>
        </trans-unit>
        <trans-unit id="b71f4915a8b78a90bf71f4feb80ae5bf3e75096c" translate="yes" xml:space="preserve">
          <source>Computes the</source>
          <target state="translated">계산</target>
        </trans-unit>
        <trans-unit id="ff498bc657a9c74488608c0ce73fd38063feeafe" translate="yes" xml:space="preserve">
          <source>Computes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_gamma_function&quot;&gt;multivariate log-gamma function&lt;/a&gt;) with dimension</source>
          <target state="translated">차원을 사용 하여 &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_gamma_function&quot;&gt;다변량 로그 감마 함수&lt;/a&gt; )를 계산 합니다.</target>
        </trans-unit>
        <trans-unit id="76557cba93a25c6c86c33cc0d82d4b6080c0ca6f" translate="yes" xml:space="preserve">
          <source>Computes the Cholesky decomposition of a symmetric positive-definite matrix</source>
          <target state="translated">양의 정의 대칭 행렬의 촐레 스키 분해를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="81b53a3d90c63815ad24683a8c1c69a90d47d01e" translate="yes" xml:space="preserve">
          <source>Computes the Heaviside step function for each element in &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 의 각 요소에 대한 헤비 사이드 스텝 함수를 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="5d07d50a47d9ca8fe0d585f7aa291a49089343e4" translate="yes" xml:space="preserve">
          <source>Computes the Heaviside step function for each element in &lt;code&gt;input&lt;/code&gt;. The Heaviside step function is defined as:</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 의 각 요소에 대한 헤비 사이드 스텝 함수를 계산합니다 . 헤비 사이드 스텝 함수는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="275e74bec95b0ab85103e19b6d6479c703706e12" translate="yes" xml:space="preserve">
          <source>Computes the Kaiser window with window length &lt;code&gt;window_length&lt;/code&gt; and shape parameter &lt;code&gt;beta&lt;/code&gt;.</source>
          <target state="translated">창 길이 &lt;code&gt;window_length&lt;/code&gt; 및 모양 매개 변수 &lt;code&gt;beta&lt;/code&gt; 로 카이저 창을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="622fda98680f4721949b3e2bf9b78a14c34deb65" translate="yes" xml:space="preserve">
          <source>Computes the LU factorization of a matrix or batches of matrices &lt;code&gt;A&lt;/code&gt;.</source>
          <target state="translated">행렬의 LU 분해 또는 행렬 &lt;code&gt;A&lt;/code&gt; 의 배치를 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="8ec07270d301f5370c046b7066eb8663a8f402a1" translate="yes" xml:space="preserve">
          <source>Computes the LU factorization of a matrix or batches of matrices &lt;code&gt;A&lt;/code&gt;. Returns a tuple containing the LU factorization and pivots of &lt;code&gt;A&lt;/code&gt;. Pivoting is done if &lt;code&gt;pivot&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">행렬의 LU 분해 또는 행렬 &lt;code&gt;A&lt;/code&gt; 의 배치를 계산합니다 . &lt;code&gt;A&lt;/code&gt; 의 LU 분해 및 피벗을 포함하는 튜플을 반환합니다 . 피벗이 &lt;code&gt;True&lt;/code&gt; 로 설정 되면 &lt;code&gt;pivot&lt;/code&gt; 이 수행됩니다 .</target>
        </trans-unit>
        <trans-unit id="0c3fba3f6161937ff4ae86bdb0e983d0c0fdda0b" translate="yes" xml:space="preserve">
          <source>Computes the N dimensional discrete Fourier transform of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 의 N 차원 이산 푸리에 변환을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="5852120d66c32f7c318de497ae26eeb74f1de763" translate="yes" xml:space="preserve">
          <source>Computes the N dimensional inverse discrete Fourier transform of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 의 N 차원 역 이산 푸리에 변환을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="efd57d1e36575205366c8a3b4753ee576bd1a231" translate="yes" xml:space="preserve">
          <source>Computes the N-dimensional discrete Fourier transform of real &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">실수 &lt;code&gt;input&lt;/code&gt; 의 N 차원 이산 푸리에 변환을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="a21392e07d8e832fdaaf1fc616e555a310a70752" translate="yes" xml:space="preserve">
          <source>Computes the QR decomposition of a matrix or a batch of matrices &lt;code&gt;input&lt;/code&gt;, and returns a namedtuple (Q, R) of tensors such that</source>
          <target state="translated">행렬의 QR 분해 또는 행렬 &lt;code&gt;input&lt;/code&gt; 의 배치를 계산하고 다음 과 같은 텐서의 명명 된 튜플 (Q, R)을 반환합니다.</target>
        </trans-unit>
        <trans-unit id="d476f44296e977553528b2efe494b3789b4d6cc8" translate="yes" xml:space="preserve">
          <source>Computes the absolute value of each element in &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 에있는 각 요소의 절대 값을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="da0e0333ca7d69a3bf51d86eddafcf5537f32ee1" translate="yes" xml:space="preserve">
          <source>Computes the base two exponential function of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 의 밑이 2 인 지수 함수를 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="833d30840f6f79f8ac884e63624d72a29fe2aedb" translate="yes" xml:space="preserve">
          <source>Computes the batchwise pairwise distance between vectors</source>
          <target state="translated">벡터 사이의 배치 별 쌍별 거리를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="b81b9d06afde312e3c1de6fa3003c1826f20a144" translate="yes" xml:space="preserve">
          <source>Computes the bitwise AND of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;other&lt;/code&gt; 의 비트 AND를 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="724c0661558717da97da9f13301714db62123db8" translate="yes" xml:space="preserve">
          <source>Computes the bitwise AND of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical AND.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;other&lt;/code&gt; 의 비트 AND를 계산합니다 . 입력 텐서는 정수 또는 부울 유형이어야합니다. 부울 텐서의 경우 논리 AND를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="4c385fff8361fd76237dd3d0d310601e4ded675d" translate="yes" xml:space="preserve">
          <source>Computes the bitwise NOT of the given input tensor.</source>
          <target state="translated">주어진 입력 텐서의 비트 단위 NOT을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="d34002186774eda6ff1544d9bb235cb5b5e464f1" translate="yes" xml:space="preserve">
          <source>Computes the bitwise NOT of the given input tensor. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical NOT.</source>
          <target state="translated">주어진 입력 텐서의 비트 단위 NOT을 계산합니다. 입력 텐서는 정수 또는 부울 유형이어야합니다. 부울 텐서의 경우 논리 NOT을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="1676d4cae5deb51721032154a09c9a5c26471fe8" translate="yes" xml:space="preserve">
          <source>Computes the bitwise OR of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;other&lt;/code&gt; 의 비트 단위 OR을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="db7eb5bd0ae610915cc8f5aaa985b610903472f9" translate="yes" xml:space="preserve">
          <source>Computes the bitwise OR of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical OR.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;other&lt;/code&gt; 의 비트 단위 OR을 계산합니다 . 입력 텐서는 정수 또는 부울 유형이어야합니다. 부울 텐서의 경우 논리 OR을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="2a7aa82e188d57e1be91dd1e59e101918adf2efe" translate="yes" xml:space="preserve">
          <source>Computes the bitwise XOR of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;other&lt;/code&gt; 의 비트 단위 XOR을 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="88579c44bdc54edf5679b9534b4247fa5a02a043" translate="yes" xml:space="preserve">
          <source>Computes the bitwise XOR of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical XOR.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;other&lt;/code&gt; 의 비트 단위 XOR을 계산합니다 . 입력 텐서는 정수 또는 부울 유형이어야합니다. 부울 텐서의 경우 논리 XOR을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="8a3e45b925774a34d85da6c0c7b80c7c01e6ba8d" translate="yes" xml:space="preserve">
          <source>Computes the complementary error function of each element of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 의 각 요소에 대한 보완 오차 함수를 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="159a12e467808284be4f45386c2f19ec971818e9" translate="yes" xml:space="preserve">
          <source>Computes the complementary error function of each element of &lt;code&gt;input&lt;/code&gt;. The complementary error function is defined as follows:</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 의 각 요소에 대한 보완 오차 함수를 계산합니다 . 보완 오류 함수는 다음과 같이 정의됩니다.</target>
        </trans-unit>
        <trans-unit id="1983c92beb407d56255dcc00b5fe7b66872fc6d6" translate="yes" xml:space="preserve">
          <source>Computes the dot product (inner product) of two tensors.</source>
          <target state="translated">두 텐서의 내적 (내적)을 계산합니다.</target>
        </trans-unit>
        <trans-unit id="9a5b212f2d4dd5a8260512297de68c3095e68cab" translate="yes" xml:space="preserve">
          <source>Computes the dot product (inner product) of two tensors. The vdot(a, b) function handles complex numbers differently than dot(a, b). If the first argument is complex, the complex conjugate of the first argument is used for the calculation of the dot product.</source>
          <target state="translated">두 텐서의 내적 (내적)을 계산합니다. vdot (a, b) 함수는 dot (a, b)와는 다르게 복소수를 처리합니다. 첫 번째 인수가 복소수이면 첫 번째 인수의 켤레 복소수가 내적 계산에 사용됩니다.</target>
        </trans-unit>
        <trans-unit id="e16561fbcf4a6c2f2dd9afb54001a93a98731fdc" translate="yes" xml:space="preserve">
          <source>Computes the eigenvalues and eigenvectors of a real square matrix.</source>
          <target state="translated">실수 제곱 행렬의 고유 값과 고유 벡터를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="52f9bd141b84eda92242b06f7f6d290a253966b4" translate="yes" xml:space="preserve">
          <source>Computes the element-wise angle (in radians) of the given &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">주어진 &lt;code&gt;input&lt;/code&gt; 텐서 의 요소 별 각도 (라디안 단위)를 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="47dc21f5f4986e27abb977788d16b590a07a71d6" translate="yes" xml:space="preserve">
          <source>Computes the element-wise conjugate of the given &lt;code&gt;input&lt;/code&gt; tensor.</source>
          <target state="translated">주어진 &lt;code&gt;input&lt;/code&gt; 텐서 의 요소 별 켤레를 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="9a964097a08f63a42878b22585751fd92a8682df" translate="yes" xml:space="preserve">
          <source>Computes the element-wise conjugate of the given &lt;code&gt;input&lt;/code&gt; tensor. If :attr`input` has a non-complex dtype, this function just returns &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">주어진 &lt;code&gt;input&lt;/code&gt; 텐서 의 요소 별 켤레를 계산합니다 . : attr`input`에 복잡하지 않은 dtype이 있으면이 함수는 &lt;code&gt;input&lt;/code&gt; 을 반환 합니다 .</target>
        </trans-unit>
        <trans-unit id="8e315d5a6fd3a46bc729f0eda3f3fb92b601c50b" translate="yes" xml:space="preserve">
          <source>Computes the element-wise greatest common divisor (GCD) of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;other&lt;/code&gt; 의 요소 별 최대 공약수 (GCD)를 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="91ee032a4ff000ec84d35b04317442f4984e6937" translate="yes" xml:space="preserve">
          <source>Computes the element-wise least common multiple (LCM) of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 과 &lt;code&gt;other&lt;/code&gt; 의 요소 별 최소 공배수 (LCM)를 계산합니다 .</target>
        </trans-unit>
        <trans-unit id="8e7b021553644e46362536880a5431b72a0587b9" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical AND of the given input tensors.</source>
          <target state="translated">Computes the element-wise logical AND of the given input tensors.</target>
        </trans-unit>
        <trans-unit id="68b2bf2409a8b5d2be8906244e0a49294d2e9679" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical AND of the given input tensors. Zeros are treated as &lt;code&gt;False&lt;/code&gt; and nonzeros are treated as &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">Computes the element-wise logical AND of the given input tensors. Zeros are treated as &lt;code&gt;False&lt;/code&gt; and nonzeros are treated as &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="a94b7ab9f10124804e19d264d7207840dc6728d2" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical NOT of the given input tensor.</source>
          <target state="translated">Computes the element-wise logical NOT of the given input tensor.</target>
        </trans-unit>
        <trans-unit id="117610e848f679548feba89108c832e6e9a20124" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical NOT of the given input tensor. If not specified, the output tensor will have the bool dtype. If the input tensor is not a bool tensor, zeros are treated as &lt;code&gt;False&lt;/code&gt; and non-zeros are treated as &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">Computes the element-wise logical NOT of the given input tensor. If not specified, the output tensor will have the bool dtype. If the input tensor is not a bool tensor, zeros are treated as &lt;code&gt;False&lt;/code&gt; and non-zeros are treated as &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="9572b2dd91b0d03ca02c1394c350df1174a646b0" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical OR of the given input tensors.</source>
          <target state="translated">Computes the element-wise logical OR of the given input tensors.</target>
        </trans-unit>
        <trans-unit id="256665a803b61ee6a430a86e174d4bca0ee69abe" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical OR of the given input tensors. Zeros are treated as &lt;code&gt;False&lt;/code&gt; and nonzeros are treated as &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">Computes the element-wise logical OR of the given input tensors. Zeros are treated as &lt;code&gt;False&lt;/code&gt; and nonzeros are treated as &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c172a20dd69ad22bb7b0a4de9500f73f3c3f98b9" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical XOR of the given input tensors.</source>
          <target state="translated">Computes the element-wise logical XOR of the given input tensors.</target>
        </trans-unit>
        <trans-unit id="c8837d2e60640bc8199344497981535b4ce62699" translate="yes" xml:space="preserve">
          <source>Computes the element-wise logical XOR of the given input tensors. Zeros are treated as &lt;code&gt;False&lt;/code&gt; and nonzeros are treated as &lt;code&gt;True&lt;/code&gt;.</source>
          <target state="translated">Computes the element-wise logical XOR of the given input tensors. Zeros are treated as &lt;code&gt;False&lt;/code&gt; and nonzeros are treated as &lt;code&gt;True&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b73dd1c77581f85619be2c0a672fa472582b7ef0" translate="yes" xml:space="preserve">
          <source>Computes the element-wise maximum of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">Computes the element-wise maximum of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="c821156c5be7a7c6ec010c82496a8862539fc670" translate="yes" xml:space="preserve">
          <source>Computes the element-wise minimum of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">Computes the element-wise minimum of &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;other&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="6b3693910cf57f242c4267e044d56aef8263e01c" translate="yes" xml:space="preserve">
          <source>Computes the element-wise remainder of division.</source>
          <target state="translated">Computes the element-wise remainder of division.</target>
        </trans-unit>
        <trans-unit id="9003dd6b7c6711ac836cacfed24f4e0435a56aec" translate="yes" xml:space="preserve">
          <source>Computes the error function of each element.</source>
          <target state="translated">Computes the error function of each element.</target>
        </trans-unit>
        <trans-unit id="29774d569920339acd042c100fd75031d60d53ac" translate="yes" xml:space="preserve">
          <source>Computes the error function of each element. The error function is defined as follows:</source>
          <target state="translated">Computes the error function of each element. The error function is defined as follows:</target>
        </trans-unit>
        <trans-unit id="2434be214f6e5985b87d4c052539ff329a7f0314" translate="yes" xml:space="preserve">
          <source>Computes the fractional portion of each element in &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the fractional portion of each element in &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cb83638df4a6cb7d822fc5fb54ba573a3cd29240" translate="yes" xml:space="preserve">
          <source>Computes the gradient of current tensor w.r.t. graph leaves.</source>
          <target state="translated">Computes the gradient of current tensor w.r.t. graph leaves.</target>
        </trans-unit>
        <trans-unit id="ce4470aa5bf338d4ad1d1c606fbb7a81df3b9429" translate="yes" xml:space="preserve">
          <source>Computes the histogram of a tensor.</source>
          <target state="translated">Computes the histogram of a tensor.</target>
        </trans-unit>
        <trans-unit id="28063e348dc010ab497d4b723f1ef5afa57d8d1a" translate="yes" xml:space="preserve">
          <source>Computes the inverse cosine of each element in &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the inverse cosine of each element in &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="a8f61c7d805f4a069d9c46e57aa8a4eda42a714f" translate="yes" xml:space="preserve">
          <source>Computes the inverse error function of each element of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the inverse error function of each element of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="ac7da6502603da8eebb2a12646d0b0cbe100d072" translate="yes" xml:space="preserve">
          <source>Computes the inverse error function of each element of &lt;code&gt;input&lt;/code&gt;. The inverse error function is defined in the range</source>
          <target state="translated">Computes the inverse error function of each element of &lt;code&gt;input&lt;/code&gt; . The inverse error function is defined in the range</target>
        </trans-unit>
        <trans-unit id="e0fc6a3f3385283ab40e97e09d370d8da0903b9c" translate="yes" xml:space="preserve">
          <source>Computes the inverse of &lt;a href=&quot;#torch.fft.hfft&quot;&gt;&lt;code&gt;hfft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Computes the inverse of &lt;a href=&quot;#torch.fft.hfft&quot;&gt; &lt;code&gt;hfft()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="f42895543a1b54cd19094b1bf3f3033fd72a6293" translate="yes" xml:space="preserve">
          <source>Computes the inverse of &lt;a href=&quot;#torch.fft.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Computes the inverse of &lt;a href=&quot;#torch.fft.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="8157bbb847ce3db3aa43a43004aeed9105153b04" translate="yes" xml:space="preserve">
          <source>Computes the inverse of &lt;a href=&quot;#torch.fft.rfftn&quot;&gt;&lt;code&gt;rfftn()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Computes the inverse of &lt;a href=&quot;#torch.fft.rfftn&quot;&gt; &lt;code&gt;rfftn()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="751c20273a8826ac2930fa4c66f27dae7e42581e" translate="yes" xml:space="preserve">
          <source>Computes the inverse of a symmetric positive-definite matrix</source>
          <target state="translated">Computes the inverse of a symmetric positive-definite matrix</target>
        </trans-unit>
        <trans-unit id="5ad2395a52ab42ac3844b6eab85a745737496c2b" translate="yes" xml:space="preserve">
          <source>Computes the logarithm of the gamma function on &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the logarithm of the gamma function on &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="0ab463de8fae2192880ba9572b3595b33254233d" translate="yes" xml:space="preserve">
          <source>Computes the logarithmic derivative of the gamma function on &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the logarithmic derivative of the gamma function on &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="53d62470ba220fb7afc0e820b9e1d137675f35a7" translate="yes" xml:space="preserve">
          <source>Computes the one dimensional Fourier transform of real-valued &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the one dimensional Fourier transform of real-valued &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="df866b02f2fffb4ec82fb366d91cc209748d5c59" translate="yes" xml:space="preserve">
          <source>Computes the one dimensional discrete Fourier transform of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the one dimensional discrete Fourier transform of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="e2bd9739508dd66799ff1e51b1381c47e1c1143b" translate="yes" xml:space="preserve">
          <source>Computes the one dimensional discrete Fourier transform of a Hermitian symmetric &lt;code&gt;input&lt;/code&gt; signal.</source>
          <target state="translated">Computes the one dimensional discrete Fourier transform of a Hermitian symmetric &lt;code&gt;input&lt;/code&gt; signal.</target>
        </trans-unit>
        <trans-unit id="1c827fe5b069aea41e126efde791986643cae21d" translate="yes" xml:space="preserve">
          <source>Computes the one dimensional inverse discrete Fourier transform of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the one dimensional inverse discrete Fourier transform of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b12e2ad844db4fa1d8dc61bc76b5cc7fede08ebb" translate="yes" xml:space="preserve">
          <source>Computes the orthogonal matrix &lt;code&gt;Q&lt;/code&gt; of a QR factorization, from the &lt;code&gt;(input, input2)&lt;/code&gt; tuple returned by &lt;a href=&quot;generated/torch.geqrf#torch.geqrf&quot;&gt;&lt;code&gt;torch.geqrf()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Computes the orthogonal matrix &lt;code&gt;Q&lt;/code&gt; of a QR factorization, from the &lt;code&gt;(input, input2)&lt;/code&gt; tuple returned by &lt;a href=&quot;generated/torch.geqrf#torch.geqrf&quot;&gt; &lt;code&gt;torch.geqrf()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="9a50e0c97fa82cb476bece9bdcb5b778a06bbe5b" translate="yes" xml:space="preserve">
          <source>Computes the orthogonal matrix &lt;code&gt;Q&lt;/code&gt; of a QR factorization, from the &lt;code&gt;(input, input2)&lt;/code&gt; tuple returned by &lt;a href=&quot;torch.geqrf#torch.geqrf&quot;&gt;&lt;code&gt;torch.geqrf()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Computes the orthogonal matrix &lt;code&gt;Q&lt;/code&gt; of a QR factorization, from the &lt;code&gt;(input, input2)&lt;/code&gt; tuple returned by &lt;a href=&quot;torch.geqrf#torch.geqrf&quot;&gt; &lt;code&gt;torch.geqrf()&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="c4aeca69a4a2b0bea72290f1b36d08fba25dfffc" translate="yes" xml:space="preserve">
          <source>Computes the p-norm distance between every pair of row vectors in the input. This is identical to the upper triangular portion, excluding the diagonal, of &lt;code&gt;torch.norm(input[:, None] - input, dim=2, p=p)&lt;/code&gt;. This function will be faster if the rows are contiguous.</source>
          <target state="translated">Computes the p-norm distance between every pair of row vectors in the input. This is identical to the upper triangular portion, excluding the diagonal, of &lt;code&gt;torch.norm(input[:, None] - input, dim=2, p=p)&lt;/code&gt; . This function will be faster if the rows are contiguous.</target>
        </trans-unit>
        <trans-unit id="52fb62cb631b335790a2488186d42d7739c05544" translate="yes" xml:space="preserve">
          <source>Computes the solution to the least squares and least norm problems for a full rank matrix</source>
          <target state="translated">Computes the solution to the least squares and least norm problems for a full rank matrix</target>
        </trans-unit>
        <trans-unit id="cf99caa72d6917de8396ddac94469751042f2b03" translate="yes" xml:space="preserve">
          <source>Computes the zeroth order modified Bessel function of the first kind for each element of &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">Computes the zeroth order modified Bessel function of the first kind for each element of &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="cbcbb3162b2e52eeda82ef5d679b6c19cdfaf1b5" translate="yes" xml:space="preserve">
          <source>Computing dependencies</source>
          <target state="translated">Computing dependencies</target>
        </trans-unit>
        <trans-unit id="6ad79ab6353b1eee8ebbc085e10d17c4fcfb024f" translate="yes" xml:space="preserve">
          <source>Concat</source>
          <target state="translated">Concat</target>
        </trans-unit>
        <trans-unit id="4ddddf59160aed751a5f07007a03c044d9753ba8" translate="yes" xml:space="preserve">
          <source>Concatenates a sequence of tensors along a new dimension.</source>
          <target state="translated">Concatenates a sequence of tensors along a new dimension.</target>
        </trans-unit>
        <trans-unit id="1b32473fe5755da0d831ffcfc10b7cdd982c9092" translate="yes" xml:space="preserve">
          <source>Concatenates the given sequence of &lt;code&gt;seq&lt;/code&gt; tensors in the given dimension.</source>
          <target state="translated">Concatenates the given sequence of &lt;code&gt;seq&lt;/code&gt; tensors in the given dimension.</target>
        </trans-unit>
        <trans-unit id="a9ec200f382eb7e2b6f899132d934e4d406c3bcb" translate="yes" xml:space="preserve">
          <source>Concatenates the given sequence of &lt;code&gt;seq&lt;/code&gt; tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.</source>
          <target state="translated">Concatenates the given sequence of &lt;code&gt;seq&lt;/code&gt; tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.</target>
        </trans-unit>
        <trans-unit id="1f8d364de32d6d9b96e9b370a225d6dab6d594c7" translate="yes" xml:space="preserve">
          <source>Concurrent calls to &lt;a href=&quot;#torch.distributed.optim.DistributedOptimizer.step&quot;&gt;&lt;code&gt;step()&lt;/code&gt;&lt;/a&gt;, either from the same or different clients, will be serialized on each worker &amp;ndash; as each worker&amp;rsquo;s optimizer can only work on one set of gradients at a time. However, there is no guarantee that the full forward-backward-optimizer sequence will execute for one client at a time. This means that the gradients being applied may not correspond to the latest forward pass executed on a given worker. Also, there is no guaranteed ordering across workers.</source>
          <target state="translated">Concurrent calls to &lt;a href=&quot;#torch.distributed.optim.DistributedOptimizer.step&quot;&gt; &lt;code&gt;step()&lt;/code&gt; &lt;/a&gt;, either from the same or different clients, will be serialized on each worker &amp;ndash; as each worker&amp;rsquo;s optimizer can only work on one set of gradients at a time. However, there is no guarantee that the full forward-backward-optimizer sequence will execute for one client at a time. This means that the gradients being applied may not correspond to the latest forward pass executed on a given worker. Also, there is no guaranteed ordering across workers.</target>
        </trans-unit>
        <trans-unit id="30e59e381077a379cb9607bde3a4a42eb3f45ab9" translate="yes" xml:space="preserve">
          <source>Consider a batched &lt;code&gt;input&lt;/code&gt; tensor containing sliding local blocks, e.g., patches of images, of shape</source>
          <target state="translated">Consider a batched &lt;code&gt;input&lt;/code&gt; tensor containing sliding local blocks, e.g., patches of images, of shape</target>
        </trans-unit>
        <trans-unit id="7f6c99c148a673c2e06016f795e6fb113b6d25b3" translate="yes" xml:space="preserve">
          <source>Consider a batched &lt;code&gt;input&lt;/code&gt; tensor of shape</source>
          <target state="translated">Consider a batched &lt;code&gt;input&lt;/code&gt; tensor of shape</target>
        </trans-unit>
        <trans-unit id="a3d8f578f82ef706b4638b32cc8ac9d832c7f7c7" translate="yes" xml:space="preserve">
          <source>ConstantPad1d</source>
          <target state="translated">ConstantPad1d</target>
        </trans-unit>
        <trans-unit id="998c295145e82549d2f17c1a6ba6c23bef09837b" translate="yes" xml:space="preserve">
          <source>ConstantPad2d</source>
          <target state="translated">ConstantPad2d</target>
        </trans-unit>
        <trans-unit id="4a15d68828e68d36dcfe86ffe64b4e4f826c7f8a" translate="yes" xml:space="preserve">
          <source>ConstantPad3d</source>
          <target state="translated">ConstantPad3d</target>
        </trans-unit>
        <trans-unit id="0a41b38808acdf43af00c5eb932dd87575946224" translate="yes" xml:space="preserve">
          <source>ConstantPadNd</source>
          <target state="translated">ConstantPadNd</target>
        </trans-unit>
        <trans-unit id="0f386d7e7881b32fa39cb7b62bdb15c0f3a4c0e1" translate="yes" xml:space="preserve">
          <source>Constants</source>
          <target state="translated">Constants</target>
        </trans-unit>
        <trans-unit id="cba7185d08d214544898ab239c21fb5414c4fc69" translate="yes" xml:space="preserve">
          <source>Constants can be marked with a &lt;code&gt;Final&lt;/code&gt; class annotation instead of adding the name of the member to &lt;code&gt;__constants__&lt;/code&gt;.</source>
          <target state="translated">Constants can be marked with a &lt;code&gt;Final&lt;/code&gt; class annotation instead of adding the name of the member to &lt;code&gt;__constants__&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="3e93ee8b7e4549e7e136ce0f0147ddef90f905dc" translate="yes" xml:space="preserve">
          <source>Construct 18 layer Resnet3D model as in &lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248&lt;/a&gt;</source>
          <target state="translated">Construct 18 layer Resnet3D model as in &lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="14faa516e57e5e69781808c4221cf6c62dc83c10" translate="yes" xml:space="preserve">
          <source>Constructor for 18 layer Mixed Convolution network as in &lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248&lt;/a&gt;</source>
          <target state="translated">Constructor for 18 layer Mixed Convolution network as in &lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="243abd8ffcbf23837c06014609b82a14c78192f1" translate="yes" xml:space="preserve">
          <source>Constructor for the 18 layer deep R(2+1)D network as in &lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248&lt;/a&gt;</source>
          <target state="translated">Constructor for the 18 layer deep R(2+1)D network as in &lt;a href=&quot;https://arxiv.org/abs/1711.11248&quot;&gt;https://arxiv.org/abs/1711.11248&lt;/a&gt;</target>
        </trans-unit>
        <trans-unit id="25ef738c9d86e2fc8c4436c90a8dc11de37e7492" translate="yes" xml:space="preserve">
          <source>Constructor, forward method, and differentiation of the output (or a function of the output of this module) are distributed synchronization points. Take that into account in case different processes might be executing different code.</source>
          <target state="translated">Constructor, forward method, and differentiation of the output (or a function of the output of this module) are distributed synchronization points. Take that into account in case different processes might be executing different code.</target>
        </trans-unit>
        <trans-unit id="e3591588e6daf80e91783781c9de4873426a6c50" translate="yes" xml:space="preserve">
          <source>Constructs a DeepLabV3 model with a ResNet-101 backbone.</source>
          <target state="translated">Constructs a DeepLabV3 model with a ResNet-101 backbone.</target>
        </trans-unit>
        <trans-unit id="0b9d4f7f98afc347ec7829b73a659d20e47f4656" translate="yes" xml:space="preserve">
          <source>Constructs a DeepLabV3 model with a ResNet-50 backbone.</source>
          <target state="translated">Constructs a DeepLabV3 model with a ResNet-50 backbone.</target>
        </trans-unit>
        <trans-unit id="a8efeef3cc601befcf295dbb51e707b10b91e402" translate="yes" xml:space="preserve">
          <source>Constructs a Faster R-CNN model with a ResNet-50-FPN backbone.</source>
          <target state="translated">Constructs a Faster R-CNN model with a ResNet-50-FPN backbone.</target>
        </trans-unit>
        <trans-unit id="63a7411b909e98e9e117a9221e16319404fd9e0b" translate="yes" xml:space="preserve">
          <source>Constructs a Fully-Convolutional Network model with a ResNet-101 backbone.</source>
          <target state="translated">Constructs a Fully-Convolutional Network model with a ResNet-101 backbone.</target>
        </trans-unit>
        <trans-unit id="40a8ca8c7bf720c8a0f63021a990b502e7b406b0" translate="yes" xml:space="preserve">
          <source>Constructs a Fully-Convolutional Network model with a ResNet-50 backbone.</source>
          <target state="translated">Constructs a Fully-Convolutional Network model with a ResNet-50 backbone.</target>
        </trans-unit>
        <trans-unit id="c44ff1a6ea4f78ec6e55eb5a4dd0b0381511fd66" translate="yes" xml:space="preserve">
          <source>Constructs a Keypoint R-CNN model with a ResNet-50-FPN backbone.</source>
          <target state="translated">Constructs a Keypoint R-CNN model with a ResNet-50-FPN backbone.</target>
        </trans-unit>
        <trans-unit id="0bac8f59a0445e0c89aaa35b2ecaf7957f6cbc76" translate="yes" xml:space="preserve">
          <source>Constructs a Mask R-CNN model with a ResNet-50-FPN backbone.</source>
          <target state="translated">Constructs a Mask R-CNN model with a ResNet-50-FPN backbone.</target>
        </trans-unit>
        <trans-unit id="ed68461c0a66e54bb8577296b733b09ee8628e1f" translate="yes" xml:space="preserve">
          <source>Constructs a MobileNetV2 architecture from &lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;&gt;&amp;ldquo;MobileNetV2: Inverted Residuals and Linear Bottlenecks&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">Constructs a MobileNetV2 architecture from &lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;&gt;&amp;ldquo;MobileNetV2: Inverted Residuals and Linear Bottlenecks&amp;rdquo;&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="fa58d20a53a578ca582a51e6082babe2633522d9" translate="yes" xml:space="preserve">
          <source>Constructs a RetinaNet model with a ResNet-50-FPN backbone.</source>
          <target state="translated">Constructs a RetinaNet model with a ResNet-50-FPN backbone.</target>
        </trans-unit>
        <trans-unit id="c060b450d2f139169d01201742e2de6bd6813c33" translate="yes" xml:space="preserve">
          <source>Constructs a ShuffleNetV2 with 0.5x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">Constructs a ShuffleNetV2 with 0.5x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="c1b1411f7ad7f2a9785e3f797d1ed1dca55c0522" translate="yes" xml:space="preserve">
          <source>Constructs a ShuffleNetV2 with 1.0x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">Constructs a ShuffleNetV2 with 1.0x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="946788267a37fd704ae9ab9be9bb919471421254" translate="yes" xml:space="preserve">
          <source>Constructs a ShuffleNetV2 with 1.5x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">Constructs a ShuffleNetV2 with 1.5x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="c3079515cd84378d124e9d68610779cdd6cd48ea" translate="yes" xml:space="preserve">
          <source>Constructs a ShuffleNetV2 with 2.0x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</source>
          <target state="translated">Constructs a ShuffleNetV2 with 2.0x output channels, as described in &lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;&amp;ldquo;ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design&amp;rdquo;&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="17f894712d4813d2bb2bfed6f1251b3bc7ad7f28" translate="yes" xml:space="preserve">
          <source>Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value &lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt;&lt;code&gt;abs&lt;/code&gt;&lt;/a&gt; and angle &lt;a href=&quot;generated/torch.angle#torch.angle&quot;&gt;&lt;code&gt;angle&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value &lt;a href=&quot;generated/torch.abs#torch.abs&quot;&gt; &lt;code&gt;abs&lt;/code&gt; &lt;/a&gt; and angle &lt;a href=&quot;generated/torch.angle#torch.angle&quot;&gt; &lt;code&gt;angle&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="21791c63e7cf531ba91fa49b8e8790ebd26c8e28" translate="yes" xml:space="preserve">
          <source>Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value &lt;a href=&quot;torch.abs#torch.abs&quot;&gt;&lt;code&gt;abs&lt;/code&gt;&lt;/a&gt; and angle &lt;a href=&quot;torch.angle#torch.angle&quot;&gt;&lt;code&gt;angle&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value &lt;a href=&quot;torch.abs#torch.abs&quot;&gt; &lt;code&gt;abs&lt;/code&gt; &lt;/a&gt; and angle &lt;a href=&quot;torch.angle#torch.angle&quot;&gt; &lt;code&gt;angle&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="3f22df20de166bd700f665def840237abc20c0b1" translate="yes" xml:space="preserve">
          <source>Constructs a complex tensor with its real part equal to &lt;a href=&quot;generated/torch.real#torch.real&quot;&gt;&lt;code&gt;real&lt;/code&gt;&lt;/a&gt; and its imaginary part equal to &lt;a href=&quot;generated/torch.imag#torch.imag&quot;&gt;&lt;code&gt;imag&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Constructs a complex tensor with its real part equal to &lt;a href=&quot;generated/torch.real#torch.real&quot;&gt; &lt;code&gt;real&lt;/code&gt; &lt;/a&gt; and its imaginary part equal to &lt;a href=&quot;generated/torch.imag#torch.imag&quot;&gt; &lt;code&gt;imag&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="7cbc2c2e6f7677b79f4e49073177abc0eaf5c360" translate="yes" xml:space="preserve">
          <source>Constructs a complex tensor with its real part equal to &lt;a href=&quot;torch.real#torch.real&quot;&gt;&lt;code&gt;real&lt;/code&gt;&lt;/a&gt; and its imaginary part equal to &lt;a href=&quot;torch.imag#torch.imag&quot;&gt;&lt;code&gt;imag&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">Constructs a complex tensor with its real part equal to &lt;a href=&quot;torch.real#torch.real&quot;&gt; &lt;code&gt;real&lt;/code&gt; &lt;/a&gt; and its imaginary part equal to &lt;a href=&quot;torch.imag#torch.imag&quot;&gt; &lt;code&gt;imag&lt;/code&gt; &lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="f93f080d3b4b5a3e3dc0939299e73f9681c6cfa7" translate="yes" xml:space="preserve">
          <source>Constructs a sparse tensors in COO(rdinate) format with non-zero elements at the given &lt;code&gt;indices&lt;/code&gt; with the given &lt;code&gt;values&lt;/code&gt;.</source>
          <target state="translated">Constructs a sparse tensors in COO(rdinate) format with non-zero elements at the given &lt;code&gt;indices&lt;/code&gt; with the given &lt;code&gt;values&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="be72a7a131ec1a761dd654077e838e6f0e7ea737" translate="yes" xml:space="preserve">
          <source>Constructs a sparse tensors in COO(rdinate) format with non-zero elements at the given &lt;code&gt;indices&lt;/code&gt; with the given &lt;code&gt;values&lt;/code&gt;. A sparse tensor can be &lt;code&gt;uncoalesced&lt;/code&gt;, in that case, there are duplicate coordinates in the indices, and the value at that index is the sum of all duplicate value entries: &lt;a href=&quot;https://pytorch.org/docs/stable/sparse.html&quot;&gt;torch.sparse&lt;/a&gt;.</source>
          <target state="translated">Constructs a sparse tensors in COO(rdinate) format with non-zero elements at the given &lt;code&gt;indices&lt;/code&gt; with the given &lt;code&gt;values&lt;/code&gt; . A sparse tensor can be &lt;code&gt;uncoalesced&lt;/code&gt; , in that case, there are duplicate coordinates in the indices, and the value at that index is the sum of all duplicate value entries: &lt;a href=&quot;https://pytorch.org/docs/stable/sparse.html&quot;&gt;torch.sparse&lt;/a&gt;.</target>
        </trans-unit>
        <trans-unit id="0e69f365d19e40e598e945db7b7eaf81602d0597" translate="yes" xml:space="preserve">
          <source>Constructs a tensor with &lt;code&gt;data&lt;/code&gt;.</source>
          <target state="translated">Constructs a tensor with &lt;code&gt;data&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="26d1a380017a1ee5db1ae9f2be051aa31298d8a4" translate="yes" xml:space="preserve">
          <source>Container holding a sequence of pruning methods for iterative pruning.</source>
          <target state="translated">Container holding a sequence of pruning methods for iterative pruning.</target>
        </trans-unit>
        <trans-unit id="ecedca4e6711cc4546829104ac643201429f438a" translate="yes" xml:space="preserve">
          <source>Container holding a sequence of pruning methods for iterative pruning. Keeps track of the order in which pruning methods are applied and handles combining successive pruning calls.</source>
          <target state="translated">Container holding a sequence of pruning methods for iterative pruning. Keeps track of the order in which pruning methods are applied and handles combining successive pruning calls.</target>
        </trans-unit>
        <trans-unit id="e040a458f46532a90ec69fa0b4bfc33ba151c98b" translate="yes" xml:space="preserve">
          <source>Containers</source>
          <target state="translated">Containers</target>
        </trans-unit>
        <trans-unit id="a5f7ef3dcfb494670b1f12c053712004a30a5030" translate="yes" xml:space="preserve">
          <source>Containers are assumed to have type &lt;code&gt;Tensor&lt;/code&gt; and be non-optional (see &lt;code&gt;Default Types&lt;/code&gt; for more information). Previously, &lt;code&gt;torch.jit.annotate&lt;/code&gt; was used to tell the TorchScript compiler what the type should be. Python 3 style type hints are now supported.</source>
          <target state="translated">Containers are assumed to have type &lt;code&gt;Tensor&lt;/code&gt; and be non-optional (see &lt;code&gt;Default Types&lt;/code&gt; for more information). Previously, &lt;code&gt;torch.jit.annotate&lt;/code&gt; was used to tell the TorchScript compiler what the type should be. Python 3 style type hints are now supported.</target>
        </trans-unit>
        <trans-unit id="69cef769a134b0d4a56c19cba61f8a8ed316d60c" translate="yes" xml:space="preserve">
          <source>Context object to wrap forward and backward passes when using distributed autograd. The &lt;code&gt;context_id&lt;/code&gt; generated in the &lt;code&gt;with&lt;/code&gt; statement is required to uniquely identify a distributed backward pass on all workers. Each worker stores metadata associated with this &lt;code&gt;context_id&lt;/code&gt;, which is required to correctly execute a distributed autograd pass.</source>
          <target state="translated">Context object to wrap forward and backward passes when using distributed autograd. The &lt;code&gt;context_id&lt;/code&gt; generated in the &lt;code&gt;with&lt;/code&gt; statement is required to uniquely identify a distributed backward pass on all workers. Each worker stores metadata associated with this &lt;code&gt;context_id&lt;/code&gt; , which is required to correctly execute a distributed autograd pass.</target>
        </trans-unit>
        <trans-unit id="05fa9ad5bb0946657dbbab246393aea9c7883c54" translate="yes" xml:space="preserve">
          <source>Context-manager that disabled gradient calculation.</source>
          <target state="translated">Context-manager that disabled gradient calculation.</target>
        </trans-unit>
        <trans-unit id="98ceadd98235bb51cb8307cf83dd7c6d79e59720" translate="yes" xml:space="preserve">
          <source>Context-manager that enables gradient calculation.</source>
          <target state="translated">Context-manager that enables gradient calculation.</target>
        </trans-unit>
        <trans-unit id="8479c988b868dfa3b4c887d081e83a92acf0bb43" translate="yes" xml:space="preserve">
          <source>Context-manager that sets gradient calculation to on or off.</source>
          <target state="translated">Context-manager that sets gradient calculation to on or off.</target>
        </trans-unit>
        <trans-unit id="4fbe3836d5db9ff249ce993c595d3f0f979273c3" translate="yes" xml:space="preserve">
          <source>Conv</source>
          <target state="translated">Conv</target>
        </trans-unit>
        <trans-unit id="0f579280c2328a913d6d725180603d671f003e1c" translate="yes" xml:space="preserve">
          <source>Conv1d</source>
          <target state="translated">Conv1d</target>
        </trans-unit>
        <trans-unit id="40b3c3b8c860add6637a732716bb59fe0a472372" translate="yes" xml:space="preserve">
          <source>Conv2d</source>
          <target state="translated">Conv2d</target>
        </trans-unit>
        <trans-unit id="11775e3bcd7c158060f3c37b634846211f59b0a8" translate="yes" xml:space="preserve">
          <source>Conv3d</source>
          <target state="translated">Conv3d</target>
        </trans-unit>
        <trans-unit id="7bd21924a004fee82a9b59ee3da28ba6cf10e0cd" translate="yes" xml:space="preserve">
          <source>ConvBn1d</source>
          <target state="translated">ConvBn1d</target>
        </trans-unit>
        <trans-unit id="b49eb764a64698e64c0e874c6a40f0bd980f5854" translate="yes" xml:space="preserve">
          <source>ConvBn2d</source>
          <target state="translated">ConvBn2d</target>
        </trans-unit>
        <trans-unit id="f2a4cf1ba0285a54525c0f0bced1d81a716af040" translate="yes" xml:space="preserve">
          <source>ConvBnReLU1d</source>
          <target state="translated">ConvBnReLU1d</target>
        </trans-unit>
        <trans-unit id="9a62f54f222bf04d2c6e425b49c602b198a2f54e" translate="yes" xml:space="preserve">
          <source>ConvBnReLU2d</source>
          <target state="translated">ConvBnReLU2d</target>
        </trans-unit>
        <trans-unit id="02be55e7a33c6df1aae2e7f342fb637e9ac77c6b" translate="yes" xml:space="preserve">
          <source>ConvReLU1d</source>
          <target state="translated">ConvReLU1d</target>
        </trans-unit>
        <trans-unit id="ee43a211652ce7b140d6ac846ff3539f57bfe4ce" translate="yes" xml:space="preserve">
          <source>ConvReLU2d</source>
          <target state="translated">ConvReLU2d</target>
        </trans-unit>
        <trans-unit id="9c9f7992fc4e6c43d3ef3ba68725dbe4ae51d5e7" translate="yes" xml:space="preserve">
          <source>ConvReLU3d</source>
          <target state="translated">ConvReLU3d</target>
        </trans-unit>
        <trans-unit id="afca9fffa388b0f4407644f89fc3d2572519c33f" translate="yes" xml:space="preserve">
          <source>ConvTranspose1d</source>
          <target state="translated">ConvTranspose1d</target>
        </trans-unit>
        <trans-unit id="fc42d54df990ea8e2fd96576309fbe506686556e" translate="yes" xml:space="preserve">
          <source>ConvTranspose2d</source>
          <target state="translated">ConvTranspose2d</target>
        </trans-unit>
        <trans-unit id="aa70f25fef0f0928f03585abe233d18e1b7a43bf" translate="yes" xml:space="preserve">
          <source>ConvTranspose3d</source>
          <target state="translated">ConvTranspose3d</target>
        </trans-unit>
        <trans-unit id="bf6b6d744534af31ad6085d41b063c70edf02466" translate="yes" xml:space="preserve">
          <source>Convenience method that creates a &lt;code&gt;setuptools.Extension&lt;/code&gt; with the bare minimum (but often sufficient) arguments to build a C++ extension.</source>
          <target state="translated">Convenience method that creates a &lt;code&gt;setuptools.Extension&lt;/code&gt; with the bare minimum (but often sufficient) arguments to build a C++ extension.</target>
        </trans-unit>
        <trans-unit id="6e35177ef47547c230ba4dcc90eda6d927bb9e5a" translate="yes" xml:space="preserve">
          <source>Convenience method that creates a &lt;code&gt;setuptools.Extension&lt;/code&gt; with the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension. This includes the CUDA include path, library path and runtime library.</source>
          <target state="translated">Convenience method that creates a &lt;code&gt;setuptools.Extension&lt;/code&gt; with the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension. This includes the CUDA include path, library path and runtime library.</target>
        </trans-unit>
        <trans-unit id="5a0770a286742ba2629162d473ed7de8b93bc405" translate="yes" xml:space="preserve">
          <source>Convert one vector to the parameters</source>
          <target state="translated">Convert one vector to the parameters</target>
        </trans-unit>
        <trans-unit id="be41e2cca6d10c1b2760ba9429806df5479848bc" translate="yes" xml:space="preserve">
          <source>Convert parameters to one vector</source>
          <target state="translated">Convert parameters to one vector</target>
        </trans-unit>
        <trans-unit id="01733ec2af89cb05f3440f813cca55cd11cb2b18" translate="yes" xml:space="preserve">
          <source>Convert the data into a &lt;code&gt;torch.Tensor&lt;/code&gt;.</source>
          <target state="translated">Convert the data into a &lt;code&gt;torch.Tensor&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="6906a54aac4ab7e793a9985a2b6c90ca5f574d9a" translate="yes" xml:space="preserve">
          <source>Convert the data into a &lt;code&gt;torch.Tensor&lt;/code&gt;. If the data is already a &lt;code&gt;Tensor&lt;/code&gt; with the same &lt;code&gt;dtype&lt;/code&gt; and &lt;code&gt;device&lt;/code&gt;, no copy will be performed, otherwise a new &lt;code&gt;Tensor&lt;/code&gt; will be returned with computational graph retained if data &lt;code&gt;Tensor&lt;/code&gt; has &lt;code&gt;requires_grad=True&lt;/code&gt;. Similarly, if the data is an &lt;code&gt;ndarray&lt;/code&gt; of the corresponding &lt;code&gt;dtype&lt;/code&gt; and the &lt;code&gt;device&lt;/code&gt; is the cpu, no copy will be performed.</source>
          <target state="translated">Convert the data into a &lt;code&gt;torch.Tensor&lt;/code&gt; . If the data is already a &lt;code&gt;Tensor&lt;/code&gt; with the same &lt;code&gt;dtype&lt;/code&gt; and &lt;code&gt;device&lt;/code&gt; , no copy will be performed, otherwise a new &lt;code&gt;Tensor&lt;/code&gt; will be returned with computational graph retained if data &lt;code&gt;Tensor&lt;/code&gt; has &lt;code&gt;requires_grad=True&lt;/code&gt; . Similarly, if the data is an &lt;code&gt;ndarray&lt;/code&gt; of the corresponding &lt;code&gt;dtype&lt;/code&gt; and the &lt;code&gt;device&lt;/code&gt; is the cpu, no copy will be performed.</target>
        </trans-unit>
        <trans-unit id="5490b3bea11fc41ac534c3f667531da495228f6a" translate="yes" xml:space="preserve">
          <source>Converts a float tensor to a per-channel quantized tensor with given scales and zero points.</source>
          <target state="translated">Converts a float tensor to a per-channel quantized tensor with given scales and zero points.</target>
        </trans-unit>
        <trans-unit id="a8473a264e544b1bee1239b9dbf9ad1a93b701b5" translate="yes" xml:space="preserve">
          <source>Converts a float tensor to a quantized tensor with given scale and zero point.</source>
          <target state="translated">Converts a float tensor to a quantized tensor with given scale and zero point.</target>
        </trans-unit>
        <trans-unit id="7757dbf24e068e638ad997ae9c4d2e07adceb8c8" translate="yes" xml:space="preserve">
          <source>Convolution Layers</source>
          <target state="translated">Convolution Layers</target>
        </trans-unit>
        <trans-unit id="be084db8daa3740ea18a2c2e59749d4c0010cf94" translate="yes" xml:space="preserve">
          <source>Convolution functions</source>
          <target state="translated">Convolution functions</target>
        </trans-unit>
        <trans-unit id="989f24495d4e5092a0b9ea78596dc97ffbe13f9f" translate="yes" xml:space="preserve">
          <source>Conv{1,2,3}D</source>
          <target state="translated">Conv{1,2,3}D</target>
        </trans-unit>
        <trans-unit id="1737f5913c8956ac817ace6ab2658a389afc44d8" translate="yes" xml:space="preserve">
          <source>Copies elements from &lt;code&gt;source&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; tensor at positions where the &lt;code&gt;mask&lt;/code&gt; is True. The shape of &lt;code&gt;mask&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with the shape of the underlying tensor. The &lt;code&gt;source&lt;/code&gt; should have at least as many elements as the number of ones in &lt;code&gt;mask&lt;/code&gt;</source>
          <target state="translated">Copies elements from &lt;code&gt;source&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; tensor at positions where the &lt;code&gt;mask&lt;/code&gt; is True. The shape of &lt;code&gt;mask&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with the shape of the underlying tensor. The &lt;code&gt;source&lt;/code&gt; should have at least as many elements as the number of ones in &lt;code&gt;mask&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="0ffea1905ca53aaa0c2e1dd04c44cfd1d1d610e9" translate="yes" xml:space="preserve">
          <source>Copies parameters and buffers from &lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then the keys of &lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">Copies parameters and buffers from &lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , then the keys of &lt;a href=&quot;#torch.jit.ScriptModule.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt; function.</target>
        </trans-unit>
        <trans-unit id="b20573e90f9dd7fefb1d1ba5fccbebf327c4350e" translate="yes" xml:space="preserve">
          <source>Copies parameters and buffers from &lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then the keys of &lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">Copies parameters and buffers from &lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , then the keys of &lt;a href=&quot;#torch.nn.Flatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt; function.</target>
        </trans-unit>
        <trans-unit id="2aa7f8afed131bd721ebda279fdf8b30aaa83be9" translate="yes" xml:space="preserve">
          <source>Copies parameters and buffers from &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then the keys of &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">Copies parameters and buffers from &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , then the keys of &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt; function.</target>
        </trans-unit>
        <trans-unit id="3fbe05d7c19e34a099d5b27aecd7e50cc995055f" translate="yes" xml:space="preserve">
          <source>Copies parameters and buffers from &lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, then the keys of &lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt;&lt;code&gt;state_dict&lt;/code&gt;&lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt;&lt;code&gt;state_dict()&lt;/code&gt;&lt;/a&gt; function.</source>
          <target state="translated">Copies parameters and buffers from &lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; into this module and its descendants. If &lt;code&gt;strict&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt; , then the keys of &lt;a href=&quot;#torch.nn.Unflatten.state_dict&quot;&gt; &lt;code&gt;state_dict&lt;/code&gt; &lt;/a&gt; must exactly match the keys returned by this module&amp;rsquo;s &lt;a href=&quot;torch.nn.module#torch.nn.Module.state_dict&quot;&gt; &lt;code&gt;state_dict()&lt;/code&gt; &lt;/a&gt; function.</target>
        </trans-unit>
        <trans-unit id="c58511363c704a11c0530ab36e516cead0a26399" translate="yes" xml:space="preserve">
          <source>Copies the elements from &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; into the positions specified by indices. For the purpose of indexing, the &lt;code&gt;self&lt;/code&gt; tensor is treated as if it were a 1-D tensor.</source>
          <target state="translated">Copies the elements from &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; into the positions specified by indices. For the purpose of indexing, the &lt;code&gt;self&lt;/code&gt; tensor is treated as if it were a 1-D tensor.</target>
        </trans-unit>
        <trans-unit id="31514897cedc5e7c028b472351d6c9c6b226eacf" translate="yes" xml:space="preserve">
          <source>Copies the elements from &lt;code&gt;src&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; tensor and returns &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="translated">Copies the elements from &lt;code&gt;src&lt;/code&gt; into &lt;code&gt;self&lt;/code&gt; tensor and returns &lt;code&gt;self&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b302a58928c86985fd5405fb9b12e6f3b385a56d" translate="yes" xml:space="preserve">
          <source>Copies the elements of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; into the &lt;code&gt;self&lt;/code&gt; tensor by selecting the indices in the order given in &lt;code&gt;index&lt;/code&gt;. For example, if &lt;code&gt;dim == 0&lt;/code&gt; and &lt;code&gt;index[i] == j&lt;/code&gt;, then the &lt;code&gt;i&lt;/code&gt;th row of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt;&lt;code&gt;tensor&lt;/code&gt;&lt;/a&gt; is copied to the &lt;code&gt;j&lt;/code&gt;th row of &lt;code&gt;self&lt;/code&gt;.</source>
          <target state="translated">Copies the elements of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; into the &lt;code&gt;self&lt;/code&gt; tensor by selecting the indices in the order given in &lt;code&gt;index&lt;/code&gt; . For example, if &lt;code&gt;dim == 0&lt;/code&gt; and &lt;code&gt;index[i] == j&lt;/code&gt; , then the &lt;code&gt;i&lt;/code&gt; th row of &lt;a href=&quot;generated/torch.tensor#torch.tensor&quot;&gt; &lt;code&gt;tensor&lt;/code&gt; &lt;/a&gt; is copied to the &lt;code&gt;j&lt;/code&gt; th row of &lt;code&gt;self&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="b91c805903c6cb0b3510a18d72ca193bc741d244" translate="yes" xml:space="preserve">
          <source>Copies the storage to pinned memory, if it&amp;rsquo;s not already pinned.</source>
          <target state="translated">Copies the storage to pinned memory, if it&amp;rsquo;s not already pinned.</target>
        </trans-unit>
        <trans-unit id="278e4ebde33e7e777180bcca75800abdc23470b8" translate="yes" xml:space="preserve">
          <source>Copies the tensor to pinned memory, if it&amp;rsquo;s not already pinned.</source>
          <target state="translated">Copies the tensor to pinned memory, if it&amp;rsquo;s not already pinned.</target>
        </trans-unit>
        <trans-unit id="bbf794aba20b276c8548169896a15590fe109e14" translate="yes" xml:space="preserve">
          <source>CosineEmbeddingLoss</source>
          <target state="translated">CosineEmbeddingLoss</target>
        </trans-unit>
        <trans-unit id="38519b5b3e654f4e1b794c35fd7670bb326f9738" translate="yes" xml:space="preserve">
          <source>CosineSimilarity</source>
          <target state="translated">CosineSimilarity</target>
        </trans-unit>
        <trans-unit id="09bf9d0f08c3cee1a60852d335fb4cf2f77d1281" translate="yes" xml:space="preserve">
          <source>Count the frequency of each value in an array of non-negative ints.</source>
          <target state="translated">Count the frequency of each value in an array of non-negative ints.</target>
        </trans-unit>
        <trans-unit id="fa674450dd813dec8a084d834fc97c369ca9a763" translate="yes" xml:space="preserve">
          <source>Counts the number of non-zero values in the tensor &lt;code&gt;input&lt;/code&gt; along the given &lt;code&gt;dim&lt;/code&gt;.</source>
          <target state="translated">Counts the number of non-zero values in the tensor &lt;code&gt;input&lt;/code&gt; along the given &lt;code&gt;dim&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="e0da99e01ab08d505f8e3aacba31f722f60ab5af" translate="yes" xml:space="preserve">
          <source>Counts the number of non-zero values in the tensor &lt;code&gt;input&lt;/code&gt; along the given &lt;code&gt;dim&lt;/code&gt;. If no dim is specified then all non-zeros in the tensor are counted.</source>
          <target state="translated">Counts the number of non-zero values in the tensor &lt;code&gt;input&lt;/code&gt; along the given &lt;code&gt;dim&lt;/code&gt; . If no dim is specified then all non-zeros in the tensor are counted.</target>
        </trans-unit>
        <trans-unit id="8bd108ea71b4b5faec418d13c1eaaf112af7d5fc" translate="yes" xml:space="preserve">
          <source>Create a block diagonal matrix from provided tensors.</source>
          <target state="translated">Create a block diagonal matrix from provided tensors.</target>
        </trans-unit>
        <trans-unit id="661a797fdd966423deb47c56a841ce6d565626a3" translate="yes" xml:space="preserve">
          <source>Create a helper proxy to easily launch a &lt;code&gt;remote&lt;/code&gt; using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, &lt;code&gt;rref.remote().func_name(*args, **kwargs)&lt;/code&gt; is the same as the following:</source>
          <target state="translated">Create a helper proxy to easily launch a &lt;code&gt;remote&lt;/code&gt; using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, &lt;code&gt;rref.remote().func_name(*args, **kwargs)&lt;/code&gt; is the same as the following:</target>
        </trans-unit>
        <trans-unit id="b873bdb21fd500fbd503a0fa2ae31f161f0a0486" translate="yes" xml:space="preserve">
          <source>Create a helper proxy to easily launch an &lt;code&gt;rpc_async&lt;/code&gt; using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, &lt;code&gt;rref.rpc_async().func_name(*args, **kwargs)&lt;/code&gt; is the same as the following:</source>
          <target state="translated">Create a helper proxy to easily launch an &lt;code&gt;rpc_async&lt;/code&gt; using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, &lt;code&gt;rref.rpc_async().func_name(*args, **kwargs)&lt;/code&gt; is the same as the following:</target>
        </trans-unit>
        <trans-unit id="8b2c6a8315871636ff748acb6e7ca0ce33cac24a" translate="yes" xml:space="preserve">
          <source>Create a helper proxy to easily launch an &lt;code&gt;rpc_sync&lt;/code&gt; using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, &lt;code&gt;rref.rpc_sync().func_name(*args, **kwargs)&lt;/code&gt; is the same as the following:</source>
          <target state="translated">Create a helper proxy to easily launch an &lt;code&gt;rpc_sync&lt;/code&gt; using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, &lt;code&gt;rref.rpc_sync().func_name(*args, **kwargs)&lt;/code&gt; is the same as the following:</target>
        </trans-unit>
        <trans-unit id="8c6bb577e26b7162d7d6c68e8046a3fec50e1ed2" translate="yes" xml:space="preserve">
          <source>Create a qat module from a float module or qparams_dict</source>
          <target state="translated">float 모듈 또는 qparams_dict에서 qat 모듈 만들기</target>
        </trans-unit>
        <trans-unit id="608ebd71f80f5c1a22f515709f259847e91c3798" translate="yes" xml:space="preserve">
          <source>Create a quantized module from a float module or qparams_dict</source>
          <target state="translated">float 모듈 또는 qparams_dict에서 양자화 된 모듈 만들기</target>
        </trans-unit>
        <trans-unit id="25224bf405304e1ab57eac3f4ba2017fad81ac0a" translate="yes" xml:space="preserve">
          <source>Create a symbolic function named &lt;code&gt;symbolic&lt;/code&gt; in the corresponding Function class.</source>
          <target state="translated">해당 Function 클래스에 &lt;code&gt;symbolic&lt;/code&gt; 이라는 기호 함수를 만듭니다 .</target>
        </trans-unit>
        <trans-unit id="05e63a314a968dccb93df86ac820277d35c3b513" translate="yes" xml:space="preserve">
          <source>Create a view of an existing &lt;code&gt;torch.Tensor&lt;/code&gt;&lt;code&gt;input&lt;/code&gt; with specified &lt;code&gt;size&lt;/code&gt;, &lt;code&gt;stride&lt;/code&gt; and &lt;code&gt;storage_offset&lt;/code&gt;.</source>
          <target state="translated">지정된 &lt;code&gt;size&lt;/code&gt; , &lt;code&gt;stride&lt;/code&gt; 및 &lt;code&gt;storage_offset&lt;/code&gt; 로 기존 &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;code&gt;input&lt;/code&gt; 의보기를 만듭니다 .</target>
        </trans-unit>
        <trans-unit id="b77a1bd9ce30851f58a1b21fe4ac8853dde6d2a8" translate="yes" xml:space="preserve">
          <source>Create special chart by collecting charts tags in &amp;lsquo;scalars&amp;rsquo;. Note that this function can only be called once for each SummaryWriter() object. Because it only provides metadata to tensorboard, the function can be called before or after the training loop.</source>
          <target state="translated">'스칼라'에서 차트 태그를 수집하여 특별한 차트를 만듭니다. 이 함수는 각 SummaryWriter () 개체에 대해 한 번만 호출 할 수 있습니다. 텐서 보드에 메타 데이터 만 제공하므로 훈련 루프 전후에 함수를 호출 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="a4594b741f646db69f78e37c46423c0ff117e55e" translate="yes" xml:space="preserve">
          <source>Creates Embedding instance from given 2-dimensional FloatTensor.</source>
          <target state="translated">지정된 2 차원 FloatTensor에서 Embedding 인스턴스를 만듭니다.</target>
        </trans-unit>
        <trans-unit id="a6e25ee28f09c0ec7810655de85d0843eaddac29" translate="yes" xml:space="preserve">
          <source>Creates EmbeddingBag instance from given 2-dimensional FloatTensor.</source>
          <target state="translated">지정된 2 차원 FloatTensor에서 EmbeddingBag 인스턴스를 만듭니다.</target>
        </trans-unit>
        <trans-unit id="7ab70bdf8d978808ae2341155ab591efacf83cbe" translate="yes" xml:space="preserve">
          <source>Creates a &lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;&lt;code&gt;Tensor&lt;/code&gt;&lt;/a&gt; from a &lt;a href=&quot;https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray&quot;&gt;&lt;code&gt;numpy.ndarray&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray&quot;&gt; &lt;code&gt;numpy.ndarray&lt;/code&gt; &lt;/a&gt; 에서 &lt;a href=&quot;../tensors#torch.Tensor&quot;&gt; &lt;code&gt;Tensor&lt;/code&gt; &lt;/a&gt; 를 만듭니다 .</target>
        </trans-unit>
        <trans-unit id="380a5b530b50dcf602f5ae20b20833a1876b83b8" translate="yes" xml:space="preserve">
          <source>Creates a &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;Tensor&lt;/code&gt;&lt;/a&gt; from a &lt;a href=&quot;https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray&quot;&gt;&lt;code&gt;numpy.ndarray&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray&quot;&gt; &lt;code&gt;numpy.ndarray&lt;/code&gt; &lt;/a&gt; 에서 &lt;a href=&quot;tensors#torch.Tensor&quot;&gt; &lt;code&gt;Tensor&lt;/code&gt; &lt;/a&gt; 를 만듭니다 .</target>
        </trans-unit>
        <trans-unit id="8ff7109193ed0423f307bf24dbf6b88c15e440c1" translate="yes" xml:space="preserve">
          <source>Creates a &lt;code&gt;SummaryWriter&lt;/code&gt; that will write out events and summaries to the event file.</source>
          <target state="translated">이벤트 및 요약을 이벤트 파일에 기록 하는 &lt;code&gt;SummaryWriter&lt;/code&gt; 를 만듭니다 .</target>
        </trans-unit>
        <trans-unit id="ba6567ea492c95adff11560e78e65d6f2fa6dfe4" translate="yes" xml:space="preserve">
          <source>Creates a &lt;code&gt;setuptools.Extension&lt;/code&gt; for C++.</source>
          <target state="translated">C ++ 용 &lt;code&gt;setuptools.Extension&lt;/code&gt; 을 만듭니다 .</target>
        </trans-unit>
        <trans-unit id="b766aaed745dd6ce4f2529c6188fca66de086dd4" translate="yes" xml:space="preserve">
          <source>Creates a &lt;code&gt;setuptools.Extension&lt;/code&gt; for CUDA/C++.</source>
          <target state="translated">CUDA / C ++ 용 &lt;code&gt;setuptools.Extension&lt;/code&gt; 을 생성합니다 .</target>
        </trans-unit>
        <trans-unit id="0ccb6b54dc9399c2c85ab56d56ee522676eba886" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the Binary Cross Entropy between the target and the output:</source>
          <target state="translated">대상과 출력 사이의 이진 교차 엔트로피를 측정하는 기준을 만듭니다.</target>
        </trans-unit>
        <trans-unit id="3d34756a1f2c85c7f54f427b38b59a7f138b4091" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the loss given input tensors</source>
          <target state="translated">주어진 입력 텐서의 손실을 측정하는 기준을 만듭니다.</target>
        </trans-unit>
        <trans-unit id="b30adb8efde8fc483debf3f678fe0a50c0f3a700" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the loss given inputs</source>
          <target state="translated">주어진 입력 손실을 측정하는 기준을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="db68393122ce4917cd40e5341fb476ef56ce47be" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the mean absolute error (MAE) between each element in the input</source>
          <target state="translated">입력의 각 요소 간의 평균 절대 오차 (MAE)를 측정하는 기준을 만듭니다.</target>
        </trans-unit>
        <trans-unit id="dc0bceb74569c8f166528464397135db651de1e9" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input</source>
          <target state="translated">입력의 각 요소 사이의 평균 제곱 오차 (제곱 L2 표준)를 측정하는 기준을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="c567813437a56bca1c0ee05ac4bbbc7a9a0f0b86" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the triplet loss given an input tensors</source>
          <target state="translated">입력 텐서가 주어지면 삼중 선 손실을 측정하는 기준을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="54c0c219984f4a50b8cfc1212d879c396ddab80a" translate="yes" xml:space="preserve">
          <source>Creates a criterion that measures the triplet loss given input tensors</source>
          <target state="translated">주어진 입력 텐서의 삼중 손실을 측정하는 기준을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="3ab7e1fd5db78a2e9eadeb7c1fe0ccb243aebcd4" translate="yes" xml:space="preserve">
          <source>Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input</source>
          <target state="translated">입력 간 다중 클래스 분류 힌지 손실 (마진 기반 손실)을 최적화하는 기준을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="0f6e83829c9a9db048daa12ae2ef6905382cf541" translate="yes" xml:space="preserve">
          <source>Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input</source>
          <target state="translated">입력 간 다중 클래스 다중 분류 힌지 손실 (마진 기반 손실)을 최적화하는 기준을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="74a30cc1dfda2e2ca6b72fcb42e123c4afa11917" translate="yes" xml:space="preserve">
          <source>Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input</source>
          <target state="translated">입력 간 최대 엔트로피를 기반으로 다중 레이블 일대일 손실을 최적화하는 기준을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="38862788d574ffd77ba587c5570ecdb676e04e5f" translate="yes" xml:space="preserve">
          <source>Creates a criterion that optimizes a two-class classification logistic loss between input tensor</source>
          <target state="translated">입력 텐서 간의 2 클래스 분류 로지스틱 손실을 최적화하는 기준을 생성합니다.</target>
        </trans-unit>
        <trans-unit id="d4196579876a20368dab571bd9df29f488cfb987" translate="yes" xml:space="preserve">
          <source>Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.</source>
          <target state="translated">요소 별 절대 오차가 베타 이하로 떨어지면 제곱항을 사용하고 그렇지 않으면 L1 항을 사용하는 기준을 만듭니다.</target>
        </trans-unit>
        <trans-unit id="416d9c5bdd3061ae8e8917cecda26e9fe1f6412a" translate="yes" xml:space="preserve">
          <source>Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. It is less sensitive to outliers than the &lt;code&gt;MSELoss&lt;/code&gt; and in some cases prevents exploding gradients (e.g. see &lt;code&gt;Fast R-CNN&lt;/code&gt; paper by Ross Girshick). Also known as the Huber loss:</source>
          <target state="translated">요소 별 절대 오차가 베타 이하로 떨어지면 제곱항을 사용하고 그렇지 않으면 L1 항을 사용하는 기준을 만듭니다. &lt;code&gt;MSELoss&lt;/code&gt; 보다 특이 치에 덜 민감하며 경우에 따라 급격한 기울기를 방지합니다 (예 : Ross Girshick의 &lt;code&gt;Fast R-CNN&lt;/code&gt; 논문 참조 ). Huber 손실이라고도합니다.</target>
        </trans-unit>
        <trans-unit id="1ebf98c1eaeed448f01f072825db935178bbb990" translate="yes" xml:space="preserve">
          <source>Creates a new distributed group.</source>
          <target state="translated">새 분산 그룹을 만듭니다.</target>
        </trans-unit>
        <trans-unit id="073fb44c832f84ccfa42ff94accd75b4dd72e7da" translate="yes" xml:space="preserve">
          <source>Creates a one-dimensional tensor of size &lt;code&gt;steps&lt;/code&gt; whose values are evenly spaced from</source>
          <target state="translated">값의 간격이 균등 한 1 차원 텐서 크기 &lt;code&gt;steps&lt;/code&gt; 를 만듭니다.</target>
        </trans-unit>
        <trans-unit id="350152525d5dacc5decf599c92018f7fa5823d28" translate="yes" xml:space="preserve">
          <source>Creates a one-dimensional tensor of size &lt;code&gt;steps&lt;/code&gt; whose values are evenly spaced from &lt;code&gt;start&lt;/code&gt; to &lt;code&gt;end&lt;/code&gt;, inclusive.</source>
          <target state="translated">값이 &lt;code&gt;start&lt;/code&gt; 부터 &lt;code&gt;end&lt;/code&gt; 균등 한 간격 ( 포함) 인 1 차원 텐서 크기 &lt;code&gt;steps&lt;/code&gt; 를 만듭니다 .</target>
        </trans-unit>
        <trans-unit id="9f41c69b98f8c4bce082aab055bcac8455947a3e" translate="yes" xml:space="preserve">
          <source>Creates a one-dimensional tensor of size &lt;code&gt;steps&lt;/code&gt; whose values are evenly spaced from &lt;code&gt;start&lt;/code&gt; to &lt;code&gt;end&lt;/code&gt;, inclusive. That is, the value are:</source>
          <target state="translated">값이 &lt;code&gt;start&lt;/code&gt; 부터 &lt;code&gt;end&lt;/code&gt; 균등 한 간격 ( 포함) 인 1 차원 텐서 크기 &lt;code&gt;steps&lt;/code&gt; 를 만듭니다 . 즉, 값은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="9962e59b2ab133d8c3b2267ae38553f28f8a79bd" translate="yes" xml:space="preserve">
          <source>Creates a quantized module from a float module or qparams_dict.</source>
          <target state="translated">float 모듈 또는 qparams_dict에서 양자화 된 모듈을 만듭니다.</target>
        </trans-unit>
        <trans-unit id="d2147eb4da2cad834986bf746aeef9a006067d0f" translate="yes" xml:space="preserve">
          <source>Creates a tensor of size &lt;code&gt;size&lt;/code&gt; filled with &lt;code&gt;fill_value&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;fill_value&lt;/code&gt; 로 채워진 크기 &lt;code&gt;size&lt;/code&gt; 의 텐서를 만듭니다 .</target>
        </trans-unit>
        <trans-unit id="ed2b3c6c1dcd8ed54d69945a6e4acc25f855c267" translate="yes" xml:space="preserve">
          <source>Creates a tensor of size &lt;code&gt;size&lt;/code&gt; filled with &lt;code&gt;fill_value&lt;/code&gt;. The tensor&amp;rsquo;s dtype is inferred from &lt;code&gt;fill_value&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;fill_value&lt;/code&gt; 로 채워진 크기 &lt;code&gt;size&lt;/code&gt; 의 텐서를 만듭니다 . 텐서의 dtype은 &lt;code&gt;fill_value&lt;/code&gt; 에서 유추됩니다 .</target>
        </trans-unit>
        <trans-unit id="12f064b3e975b58c920e1056343b0cd452f5ecab" translate="yes" xml:space="preserve">
          <source>Creates a tensor whose diagonals of certain 2D planes (specified by &lt;code&gt;dim1&lt;/code&gt; and &lt;code&gt;dim2&lt;/code&gt;) are filled by &lt;code&gt;input&lt;/code&gt;.</source>
          <target state="translated">누구 (의해 지정된 특정 2D 평면의 대각선 텐서 작성 &lt;code&gt;dim1&lt;/code&gt; 및 &lt;code&gt;dim2&lt;/code&gt; )로 채워진다 &lt;code&gt;input&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="a4e06d7745c58e373370f1d2833461757426df92" translate="yes" xml:space="preserve">
          <source>Creates a tensor whose diagonals of certain 2D planes (specified by &lt;code&gt;dim1&lt;/code&gt; and &lt;code&gt;dim2&lt;/code&gt;) are filled by &lt;code&gt;input&lt;/code&gt;. To facilitate creating batched diagonal matrices, the 2D planes formed by the last two dimensions of the returned tensor are chosen by default.</source>
          <target state="translated">누구 (의해 지정된 특정 2D 평면의 대각선 텐서 작성 &lt;code&gt;dim1&lt;/code&gt; 및 &lt;code&gt;dim2&lt;/code&gt; )로 채워진다 &lt;code&gt;input&lt;/code&gt; . 배치 된 대각 행렬을 쉽게 생성하기 위해 반환 된 텐서의 마지막 2 차원으로 형성된 2D 평면이 기본적으로 선택됩니다.</target>
        </trans-unit>
        <trans-unit id="54d06cd8eb770a4665e06b480e41c18b35882c33" translate="yes" xml:space="preserve">
          <source>Creates an asynchronous task executing &lt;code&gt;func&lt;/code&gt; and a reference to the value of the result of this execution.</source>
          <target state="translated">&lt;code&gt;func&lt;/code&gt; 를 실행하고이 실행 결과 값에 대한 참조를 실행하는 비동기 작업을 만듭니다 .</target>
        </trans-unit>
        <trans-unit id="a528098d6266957ce08cc5436471c4e687e167c2" translate="yes" xml:space="preserve">
          <source>Creates an asynchronous task executing &lt;code&gt;func&lt;/code&gt; and a reference to the value of the result of this execution. &lt;code&gt;fork&lt;/code&gt; will return immediately, so the return value of &lt;code&gt;func&lt;/code&gt; may not have been computed yet. To force completion of the task and access the return value invoke &lt;code&gt;torch.jit.wait&lt;/code&gt; on the Future. &lt;code&gt;fork&lt;/code&gt; invoked with a &lt;code&gt;func&lt;/code&gt; which returns &lt;code&gt;T&lt;/code&gt; is typed as &lt;code&gt;torch.jit.Future[T]&lt;/code&gt;. &lt;code&gt;fork&lt;/code&gt; calls can be arbitrarily nested, and may be invoked with positional and keyword arguments. Asynchronous execution will only occur when run in TorchScript. If run in pure python, &lt;code&gt;fork&lt;/code&gt; will not execute in parallel. &lt;code&gt;fork&lt;/code&gt; will also not execute in parallel when invoked while tracing, however the &lt;code&gt;fork&lt;/code&gt; and &lt;code&gt;wait&lt;/code&gt; calls will be captured in the exported IR Graph. .. warning:</source>
          <target state="translated">&lt;code&gt;func&lt;/code&gt; 를 실행하고이 실행 결과 값에 대한 참조를 실행하는 비동기 작업을 만듭니다 . &lt;code&gt;fork&lt;/code&gt; 는 즉시 반환되므로 &lt;code&gt;func&lt;/code&gt; 의 반환 값은 아직 계산되지 않았을 수 있습니다. 작업을 강제로 완료하고 반환 값에 액세스 하려면 Future에서 &lt;code&gt;torch.jit.wait&lt;/code&gt; 를 호출 합니다. &lt;code&gt;T&lt;/code&gt; 를 반환 하는 &lt;code&gt;func&lt;/code&gt; 로 호출 된 &lt;code&gt;fork&lt;/code&gt; 는 &lt;code&gt;torch.jit.Future[T]&lt;/code&gt; 형식으로 지정됩니다 . &lt;code&gt;fork&lt;/code&gt; 호출은 임의로 중첩 될 수 있으며 위치 및 키워드 인수로 호출 될 수 있습니다. 비동기 실행은 TorchScript에서 실행될 때만 발생합니다. 순수 파이썬에서 실행하면 &lt;code&gt;fork&lt;/code&gt; 가 병렬로 실행되지 않습니다. &lt;code&gt;fork&lt;/code&gt; 는 추적하는 동안 호출 될 때 병렬로 실행되지 않지만 &lt;code&gt;fork&lt;/code&gt; 및 &lt;code&gt;wait&lt;/code&gt; 호출은 내 보낸 IR 그래프에 캡처됩니다. .. 경고 :</target>
        </trans-unit>
        <trans-unit id="5c73fcf2c3b7a88652c30df05d4a49be7a9a3a82" translate="yes" xml:space="preserve">
          <source>Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers.</source>
          <target state="translated">의사 난수를 생성하는 알고리즘의 상태를 관리하는 생성기 객체를 만들고 반환합니다.</target>
        </trans-unit>
        <trans-unit id="7bf69af350942e6f19285c4071e0d9bc1d0390fb" translate="yes" xml:space="preserve">
          <source>Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers. Used as a keyword argument in many &lt;a href=&quot;../torch#inplace-random-sampling&quot;&gt;In-place random sampling&lt;/a&gt; functions.</source>
          <target state="translated">의사 난수를 생성하는 알고리즘의 상태를 관리하는 생성기 객체를 만들고 반환합니다. 많은 &lt;a href=&quot;../torch#inplace-random-sampling&quot;&gt;내부 임의 샘플링&lt;/a&gt; 함수 에서 키워드 인수로 사용됩니다 .</target>
        </trans-unit>
        <trans-unit id="6db27514dc4a87b77455deb1f68ac2812f51084a" translate="yes" xml:space="preserve">
          <source>Creating TorchScript Code</source>
          <target state="translated">TorchScript 코드 생성</target>
        </trans-unit>
        <trans-unit id="14dc9f6dca53e0df6aaf4e5eb53a7efa6593aab3" translate="yes" xml:space="preserve">
          <source>Creating named tensors</source>
          <target state="translated">명명 된 텐서 만들기</target>
        </trans-unit>
        <trans-unit id="733cbb78a0d286a0935c6c571466400a11828907" translate="yes" xml:space="preserve">
          <source>Creation Ops</source>
          <target state="translated">창조 작전</target>
        </trans-unit>
        <trans-unit id="d66fef2dd7f330ad8c7b2de8229080af12ec8719" translate="yes" xml:space="preserve">
          <source>Creation of this class requires that &lt;code&gt;torch.distributed&lt;/code&gt; to be already initialized, by calling &lt;a href=&quot;../distributed#torch.distributed.init_process_group&quot;&gt;&lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">이 클래스의 창조해야 &lt;code&gt;torch.distributed&lt;/code&gt; 호출하여 이미 초기화 할 &lt;a href=&quot;../distributed#torch.distributed.init_process_group&quot;&gt; &lt;code&gt;torch.distributed.init_process_group()&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="711f625ceb8a65f44028c4e7b7e4e818fc7445a7" translate="yes" xml:space="preserve">
          <source>CrossEntropyLoss</source>
          <target state="translated">CrossEntropyLoss</target>
        </trans-unit>
        <trans-unit id="5eec0b5e11526f784758a0afbe48baade8e71e9e" translate="yes" xml:space="preserve">
          <source>Current implementation of &lt;a href=&quot;#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt; introduces memory overhead, thus it might lead to unexpectedly high memory usage in the applications with many tiny tensors. If this is your case, consider using one large structure.</source>
          <target state="translated">현재 &lt;a href=&quot;#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt; 구현은 메모리 오버 헤드를 도입하므로 작은 텐서가 많은 애플리케이션에서 예기치 않게 높은 메모리 사용량이 발생할 수 있습니다. 이 경우 하나의 큰 구조를 사용하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="96c1a2df528a935c7c808490ba7a2921d055775f" translate="yes" xml:space="preserve">
          <source>Current implementation packs weights on every call, which has penalty on performance. If you want to avoid the overhead, use &lt;a href=&quot;#torch.nn.quantized.Linear&quot;&gt;&lt;code&gt;Linear&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">현재 구현은 모든 호출에 가중치를 부여하므로 성능이 저하됩니다. 오버 헤드를 피하려면 &lt;a href=&quot;#torch.nn.quantized.Linear&quot;&gt; &lt;code&gt;Linear&lt;/code&gt; 를&lt;/a&gt; 사용하십시오 .</target>
        </trans-unit>
        <trans-unit id="392192a4ae05351b1f7b21e538ff3e8ca534e67d" translate="yes" xml:space="preserve">
          <source>Currently &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt;&lt;code&gt;SyncBatchNorm&lt;/code&gt;&lt;/a&gt; only supports &lt;code&gt;DistributedDataParallel&lt;/code&gt; (DDP) with single GPU per process. Use &lt;a href=&quot;#torch.nn.SyncBatchNorm.convert_sync_batchnorm&quot;&gt;&lt;code&gt;torch.nn.SyncBatchNorm.convert_sync_batchnorm()&lt;/code&gt;&lt;/a&gt; to convert &lt;code&gt;BatchNorm*D&lt;/code&gt; layer to &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt;&lt;code&gt;SyncBatchNorm&lt;/code&gt;&lt;/a&gt; before wrapping Network with DDP.</source>
          <target state="translated">현재 &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt; &lt;code&gt;SyncBatchNorm&lt;/code&gt; &lt;/a&gt; 은 프로세스 당 단일 GPU가있는 DDP ( &lt;code&gt;DistributedDataParallel&lt;/code&gt; ) 만 지원합니다 . 네트워크를 DDP로 래핑하기 전에 &lt;a href=&quot;#torch.nn.SyncBatchNorm.convert_sync_batchnorm&quot;&gt; &lt;code&gt;torch.nn.SyncBatchNorm.convert_sync_batchnorm()&lt;/code&gt; &lt;/a&gt; 을 사용 하여 &lt;code&gt;BatchNorm*D&lt;/code&gt; 레이어를 &lt;a href=&quot;#torch.nn.SyncBatchNorm&quot;&gt; &lt;code&gt;SyncBatchNorm&lt;/code&gt; &lt;/a&gt; 으로 변환 합니다.</target>
        </trans-unit>
        <trans-unit id="a78ac4f002658abaaa21040baac2a600bf544edc" translate="yes" xml:space="preserve">
          <source>Currently in the CUDA implementation and the CPU implementation when dim is specified, &lt;code&gt;torch.unique&lt;/code&gt; always sort the tensor at the beginning regardless of the &lt;code&gt;sort&lt;/code&gt; argument. Sorting could be slow, so if your input tensor is already sorted, it is recommended to use &lt;a href=&quot;torch.unique_consecutive#torch.unique_consecutive&quot;&gt;&lt;code&gt;torch.unique_consecutive()&lt;/code&gt;&lt;/a&gt; which avoids the sorting.</source>
          <target state="translated">현재 CUDA 구현 및 dim이 지정된 CPU 구현에서 &lt;code&gt;torch.unique&lt;/code&gt; 는 &lt;code&gt;sort&lt;/code&gt; 인수에 관계없이 항상 처음에 텐서를 정렬합니다 . 정렬이 느릴 수 있으므로 입력 텐서가 이미 정렬 된 경우 정렬을 피하는 &lt;a href=&quot;torch.unique_consecutive#torch.unique_consecutive&quot;&gt; &lt;code&gt;torch.unique_consecutive()&lt;/code&gt; &lt;/a&gt; 를 사용하는 것이 좋습니다 .</target>
        </trans-unit>
        <trans-unit id="c8a61a0d816f8b21bf7f4b46f888c832b03676e2" translate="yes" xml:space="preserve">
          <source>Currently spatial and volumetric upsampling are supported (i.e. expected inputs are 4 or 5 dimensional).</source>
          <target state="translated">현재 공간 및 체적 업 샘플링이 지원됩니다 (예 : 예상 입력은 4 차원 또는 5 차원).</target>
        </trans-unit>
        <trans-unit id="8a9d7a3e06793c263711ce33ba0ffdc49d409027" translate="yes" xml:space="preserve">
          <source>Currently supported operations and subsystems</source>
          <target state="translated">현재 지원되는 작업 및 하위 시스템</target>
        </trans-unit>
        <trans-unit id="3b05bf74a2646cf7882607d1b6e8f3921b3f2124" translate="yes" xml:space="preserve">
          <source>Currently temporal, spatial and volumetric sampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape.</source>
          <target state="translated">현재 시간적, 공간적 및 체적 샘플링이 지원됩니다. 즉, 예상 입력은 모양이 3D, 4D 또는 5D입니다.</target>
        </trans-unit>
        <trans-unit id="af454dd72bdbaa9839806374d040bd77db038329" translate="yes" xml:space="preserve">
          <source>Currently temporal, spatial and volumetric upsampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape.</source>
          <target state="translated">현재 시간적, 공간적 및 체적 업 샘플링이 지원됩니다. 즉, 예상 입력은 모양이 3D, 4D 또는 5D입니다.</target>
        </trans-unit>
        <trans-unit id="d49c5e3f020c5093dc8a1df7be3b5947455c7de6" translate="yes" xml:space="preserve">
          <source>Currently three initialization methods are supported:</source>
          <target state="translated">현재 세 가지 초기화 방법이 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="ecab6b4c3bf958fdadbd30836155137da6ef98e2" translate="yes" xml:space="preserve">
          <source>Currently valid scalar and tensor combination are 1. Scalar of floating dtype and torch.double 2. Scalar of integral dtype and torch.long 3. Scalar of complex dtype and torch.complex128</source>
          <target state="translated">현재 유효한 스칼라 및 텐서 조합은 1. 부동 dtype 및 torch.double의 스칼라 2. 정수 dtype 및 torch.long의 스칼라 3. 복합 dtype 및 torch.complex128의 스칼라</target>
        </trans-unit>
        <trans-unit id="0cf55afa31790d6a153e9124662c9ad715c264be" translate="yes" xml:space="preserve">
          <source>Currently, only 3-D output tensors (unfolded batched image-like tensors) are supported.</source>
          <target state="translated">현재 3D 출력 텐서 (펼쳐진 배치 이미지 유사 텐서) 만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="cf7422cb9205f4ada47421ece107faeeaf3ef476" translate="yes" xml:space="preserve">
          <source>Currently, only 4-D input tensors (batched image-like tensors) are supported.</source>
          <target state="translated">현재는 4 차원 입력 텐서 (일괄 이미지 유사 텐서) 만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="804337292e10b528ce9da39728466ea21dd2d93d" translate="yes" xml:space="preserve">
          <source>Currently, only 4-D output tensors (batched image-like tensors) are supported.</source>
          <target state="translated">현재는 4D 출력 텐서 (일괄 이미지 유사 텐서) 만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="55e4dc48ed51c13b29c9e279e5505c0a895f1cb1" translate="yes" xml:space="preserve">
          <source>Currently, only spatial (4-D) and volumetric (5-D) &lt;code&gt;input&lt;/code&gt; are supported.</source>
          <target state="translated">현재 공간 (4-D) 및 체적 (5-D) &lt;code&gt;input&lt;/code&gt; 만 지원됩니다.</target>
        </trans-unit>
        <trans-unit id="b674ceb20a7250912a1c130e2165e5889d62f4ab" translate="yes" xml:space="preserve">
          <source>Custom operators</source>
          <target state="translated">맞춤 연산자</target>
        </trans-unit>
        <trans-unit id="3461dd173d30fce828765c425b88ef7516427e68" translate="yes" xml:space="preserve">
          <source>CustomFromMask</source>
          <target state="translated">CustomFromMask</target>
        </trans-unit>
        <trans-unit id="50c9e8d5fc98727b4bbc93cf5d64a68db647f04f" translate="yes" xml:space="preserve">
          <source>D</source>
          <target state="translated">D</target>
        </trans-unit>
        <trans-unit id="eb3635e47f534f46cf856ff7cd245f64d37e6d18" translate="yes" xml:space="preserve">
          <source>DCGAN</source>
          <target state="translated">DCGAN</target>
        </trans-unit>
        <trans-unit id="17ba820b5bcd55a78cfd53d227f66dc3adcb9245" translate="yes" xml:space="preserve">
          <source>D_{out} = (D_{in} - 1) \times \text{stride[0]} - 2 \times \text{padding[0]} + \text{kernel\_size[0]}</source>
          <target state="translated">D_ {out} = (D_ {in}-1) \ times \ text {stride [0]}-2 \ times \ text {padding [0]} + \ text {kernel \ _size [0]}</target>
        </trans-unit>
        <trans-unit id="11e96ca0ef0a7d1c0968f6c0aa661473e65963b1" translate="yes" xml:space="preserve">
          <source>D_{out} = (D_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1</source>
          <target state="translated">D_ {out} = (D_ {in}-1) \ times \ text {stride} [0]-2 \ times \ text {padding} [0] + \ text {dilation} [0] \ times (\ text { 커널 \ _size} [0]-1) + \ text {output \ _padding} [0] + 1</target>
        </trans-unit>
        <trans-unit id="c8db0974bd119b8e244f7c9a0fbb5cd60340ca9b" translate="yes" xml:space="preserve">
          <source>D_{out} = D_{in} + \text{padding\_front} + \text{padding\_back}</source>
          <target state="translated">D_ {out} = D_ {in} + \ text {padding \ _front} + \ text {padding \ _back}</target>
        </trans-unit>
        <trans-unit id="343f5280293229b193ab7d6cc2f6fecb4a2a501b" translate="yes" xml:space="preserve">
          <source>D_{out} = \left\lfloor D_{in} \times \text{scale\_factor} \right\rfloor</source>
          <target state="translated">D_ {out} = \ left \ lfloor D_ {in} \ times \ text {scale \ _factor} \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="5daaf33316a5a1e996b64f9e865fa2c06a216412" translate="yes" xml:space="preserve">
          <source>D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor</source>
          <target state="translated">D_ {out} = \ left \ lfloor \ frac {D_ {in} + 2 \ times \ text {padding} [0]-\ text {dilation} [0] \ times (\ text {kernel \ _size} [0] -1)-1} {\ text {stride} [0]} + 1 \ right \ rfloor</target>
        </trans-unit>
        <trans-unit id="a58eeddbe8ce176e982a4bd5546958165921baf4" translate="yes" xml:space="preserve">
          <source>D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor</source>
          <target state="translated">D_ {out} = \ left \ lfloor \ frac {D_ {in} + 2 \ times \ text {padding} [0]-\ text {kernel \ _size} [0]} {\ text {stride} [0]} + 1 \ 오른쪽 \ r 바닥</target>
        </trans-unit>
        <trans-unit id="58d17582a05d81dbc970a2fb5dde4c86c385cf6c" translate="yes" xml:space="preserve">
          <source>Danger</source>
          <target state="translated">Danger</target>
        </trans-unit>
        <trans-unit id="ee503fe5765b8d9456bf21def7f9e06d2b12ebb6" translate="yes" xml:space="preserve">
          <source>Data type</source>
          <target state="translated">데이터 형식</target>
        </trans-unit>
        <trans-unit id="5e3a2e3c18839bb47fb3084db81d051ea2d9e572" translate="yes" xml:space="preserve">
          <source>DataParallel</source>
          <target state="translated">DataParallel</target>
        </trans-unit>
        <trans-unit id="f41afa14a2956bd6dee2529d81450b9fde84c7bb" translate="yes" xml:space="preserve">
          <source>DataParallel Layers (multi-GPU, distributed)</source>
          <target state="translated">DataParallel 레이어 (다중 GPU, 분산)</target>
        </trans-unit>
        <trans-unit id="2c2866ebc153b9ba5ee12510871be4ea8e2d66ad" translate="yes" xml:space="preserve">
          <source>DataParallel functions (multi-GPU, distributed)</source>
          <target state="translated">DataParallel 함수 (다중 GPU, 분산)</target>
        </trans-unit>
        <trans-unit id="75c05e642a4a82474f0ded1d73a7d77a7cab17de" translate="yes" xml:space="preserve">
          <source>DeQuantize</source>
          <target state="translated">DeQuantize</target>
        </trans-unit>
        <trans-unit id="895b27c88016513d278a0ce3dc0663fae3829d58" translate="yes" xml:space="preserve">
          <source>Debugging</source>
          <target state="translated">Debugging</target>
        </trans-unit>
        <trans-unit id="2d9ba523f6bd6f1366ef31447c6896de739c5ebc" translate="yes" xml:space="preserve">
          <source>Debugging this script with &lt;code&gt;pdb&lt;/code&gt; works except for when we invoke the &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;@torch.jit.script&lt;/code&gt;&lt;/a&gt; function. We can globally disable JIT, so that we can call the &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt;&lt;code&gt;@torch.jit.script&lt;/code&gt;&lt;/a&gt; function as a normal Python function and not compile it. If the above script is called &lt;code&gt;disable_jit_example.py&lt;/code&gt;, we can invoke it like so:</source>
          <target state="translated">&lt;code&gt;pdb&lt;/code&gt; 로이 스크립트를 디버깅 하는 것은 &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;@torch.jit.script&lt;/code&gt; &lt;/a&gt; 함수를 호출하는 경우를 제외하고 작동 합니다. JIT를 전역 적으로 비활성화하여 &lt;a href=&quot;generated/torch.jit.script#torch.jit.script&quot;&gt; &lt;code&gt;@torch.jit.script&lt;/code&gt; &lt;/a&gt; 함수를 일반적인 Python 함수로 호출하고 컴파일하지 않을 수 있습니다. 위의 스크립트가 &lt;code&gt;disable_jit_example.py&lt;/code&gt; 인 경우 다음과 같이 호출 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="9f7b680b643589b9ed8668bf524f99287df6399c" translate="yes" xml:space="preserve">
          <source>Decodes a DLPack to a tensor.</source>
          <target state="translated">DLPack을 텐서로 디코딩합니다.</target>
        </trans-unit>
        <trans-unit id="bbe3208131f8562d117c292d268d2f8db716693e" translate="yes" xml:space="preserve">
          <source>DeepLabV3</source>
          <target state="translated">DeepLabV3</target>
        </trans-unit>
        <trans-unit id="67529023df524563878c2ac2647a317ad8500070" translate="yes" xml:space="preserve">
          <source>DeepLabV3 ResNet101</source>
          <target state="translated">DeepLabV3 ResNet101</target>
        </trans-unit>
        <trans-unit id="617f04b5a03ab100dd0d3e0e5532b5eb50903269" translate="yes" xml:space="preserve">
          <source>DeepLabV3 ResNet50</source>
          <target state="translated">DeepLabV3 ResNet50</target>
        </trans-unit>
        <trans-unit id="d2b8f3a731cc6350b5d6b3c7afddb1acf6006314" translate="yes" xml:space="preserve">
          <source>DeepLabV3 ResNet50, ResNet101</source>
          <target state="translated">DeepLabV3 ResNet50, ResNet101</target>
        </trans-unit>
        <trans-unit id="8c8189f5b90e5b1272f55bc887fde7008a4bafdf" translate="yes" xml:space="preserve">
          <source>Default Types</source>
          <target state="translated">기본 유형</target>
        </trans-unit>
        <trans-unit id="e51881f10b051af0af29bc3e379261ed831a6098" translate="yes" xml:space="preserve">
          <source>Default is &lt;code&gt;&quot;backward&quot;&lt;/code&gt; (no normalization).</source>
          <target state="translated">기본값은 &lt;code&gt;&quot;backward&quot;&lt;/code&gt; (정규화 없음)입니다.</target>
        </trans-unit>
        <trans-unit id="0e0fda91e6a2713611da6dc3cb2473efcd25b651" translate="yes" xml:space="preserve">
          <source>Default is &lt;code&gt;&quot;backward&quot;&lt;/code&gt; (normalize by &lt;code&gt;1/n&lt;/code&gt;).</source>
          <target state="translated">기본값은 &lt;code&gt;&quot;backward&quot;&lt;/code&gt; ( &lt;code&gt;1/n&lt;/code&gt; 으로 정규화 ).</target>
        </trans-unit>
        <trans-unit id="8e4f64360906e7f1452e75f1085dc507b2bff801" translate="yes" xml:space="preserve">
          <source>Default: &lt;code&gt;None&lt;/code&gt;</source>
          <target state="translated">기본값 : &lt;code&gt;None&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="be8bdbcf95f1b2c543f6b5f2c72d836f7c5b5704" translate="yes" xml:space="preserve">
          <source>Defaults to zero if not provided. where</source>
          <target state="translated">제공되지 않은 경우 기본값은 0입니다. 어디</target>
        </trans-unit>
        <trans-unit id="0c57e9446082424cd876b20c90b07486b438b9b9" translate="yes" xml:space="preserve">
          <source>Define the symbolic function in &lt;code&gt;torch/onnx/symbolic_opset&amp;lt;version&amp;gt;.py&lt;/code&gt;, for example &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py&quot;&gt;torch/onnx/symbolic_opset9.py&lt;/a&gt;. Make sure the function has the same name as the ATen operator/function defined in &lt;code&gt;VariableType.h&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;torch/onnx/symbolic_opset&amp;lt;version&amp;gt;.py&lt;/code&gt; 기호 함수를 정의합니다 ( 예 : &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py&quot;&gt;torch / onnx / symbolic_opset9.py)&lt;/a&gt; . 함수의 이름이 &lt;code&gt;VariableType.h&lt;/code&gt; 에 정의 된 ATen 연산자 / 함수와 동일한 지 확인하십시오 .</target>
        </trans-unit>
        <trans-unit id="7773324d0fbb19d2b097125eeec23bba37fd9fae" translate="yes" xml:space="preserve">
          <source>Defines the computation performed at every call.</source>
          <target state="translated">모든 호출에서 수행되는 계산을 정의합니다.</target>
        </trans-unit>
        <trans-unit id="9fdfda877e82e2aeb2ae17e08e28fcbd6d18a121" translate="yes" xml:space="preserve">
          <source>Deletes the key-value pair associated with &lt;code&gt;key&lt;/code&gt; from the store. Returns &lt;code&gt;true&lt;/code&gt; if the key was successfully deleted, and &lt;code&gt;false&lt;/code&gt; if it was not.</source>
          <target state="translated">저장소에서 &lt;code&gt;key&lt;/code&gt; 와 연결된 키-값 쌍을 삭제합니다 . 키가 성공적으로 삭제 된 경우 &lt;code&gt;true&lt;/code&gt; 를 반환하고 그렇지 않은 경우 &lt;code&gt;false&lt;/code&gt; 를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="fa7b4a50c0e0f2cffa01451426972ab2f487a9dd" translate="yes" xml:space="preserve">
          <source>DenseNet</source>
          <target state="translated">DenseNet</target>
        </trans-unit>
        <trans-unit id="8b2ff8d2942b70f38d615ed1e52ba52e2285edab" translate="yes" xml:space="preserve">
          <source>Densenet-121</source>
          <target state="translated">Densenet-121</target>
        </trans-unit>
        <trans-unit id="d26e787bd56b9c7aab1c534a7f7ead9ab4b12d73" translate="yes" xml:space="preserve">
          <source>Densenet-121 model from &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;&amp;ldquo;Densely Connected Convolutional Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&quot;밀집 &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;연결된 컨볼 루션 네트워크&quot;의&lt;/a&gt; Densenet-121 모델</target>
        </trans-unit>
        <trans-unit id="dc79947b23872b66c941b9c7a01bd0904908a8d9" translate="yes" xml:space="preserve">
          <source>Densenet-161</source>
          <target state="translated">Densenet-161</target>
        </trans-unit>
        <trans-unit id="8079b2939275d34b5b7b870ad8e0a1f7fe8edd0d" translate="yes" xml:space="preserve">
          <source>Densenet-161 model from &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;&amp;ldquo;Densely Connected Convolutional Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&quot;밀집 &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;연결된 컨볼 루션 네트워크&quot;의&lt;/a&gt; Densenet-161 모델</target>
        </trans-unit>
        <trans-unit id="f8e6fd5af55a790890421b436f68cbc8406a0b19" translate="yes" xml:space="preserve">
          <source>Densenet-169</source>
          <target state="translated">Densenet-169</target>
        </trans-unit>
        <trans-unit id="7480a230cf176343f7e3077f6904ede5855a19d8" translate="yes" xml:space="preserve">
          <source>Densenet-169 model from &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;&amp;ldquo;Densely Connected Convolutional Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&quot;밀집 &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;연결된 컨볼 루션 네트워크&quot;의&lt;/a&gt; Densenet-169 모델</target>
        </trans-unit>
        <trans-unit id="fc80ee1a1bf823e3bf3be2c3a3194f577392f377" translate="yes" xml:space="preserve">
          <source>Densenet-201</source>
          <target state="translated">Densenet-201</target>
        </trans-unit>
        <trans-unit id="7124ecd6d83a80564dc1ba72e45a576f118ac991" translate="yes" xml:space="preserve">
          <source>Densenet-201 model from &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;&amp;ldquo;Densely Connected Convolutional Networks&amp;rdquo;&lt;/a&gt;</source>
          <target state="translated">&quot;밀집 &lt;a href=&quot;https://arxiv.org/pdf/1608.06993.pdf&quot;&gt;연결된 컨볼 루션 네트워크&quot;의&lt;/a&gt; Densenet-201 모델</target>
        </trans-unit>
        <trans-unit id="7a89d9b0077197cdfbba3a66cddb06012c599011" translate="yes" xml:space="preserve">
          <source>Depending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-correlation&quot;&gt;cross-correlation&lt;/a&gt;, and not a full &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-correlation&quot;&gt;cross-correlation&lt;/a&gt;. It is up to the user to add proper padding.</source>
          <target state="translated">커널의 크기에 따라 입력의 여러 열 (마지막)이 손실 될 수 있습니다. 이는 전체 &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-correlation&quot;&gt;상호 &lt;/a&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-correlation&quot;&gt;상관&lt;/a&gt; 이 아니라 유효한 상호 상관 이기 때문입니다 . 적절한 패딩을 추가하는 것은 사용자에게 달려 있습니다.</target>
        </trans-unit>
        <trans-unit id="7a0685deb3b8e4521a47904d7970aabb4610a92a" translate="yes" xml:space="preserve">
          <source>Depending on the custom operator, you can export it as one or a combination of existing ONNX ops. You can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain and version (custom opset) using the &lt;code&gt;custom_opsets&lt;/code&gt; dictionary at export. If not explicitly specified, the custom opset version is set to 1 by default. Using custom ONNX ops, you will need to extend the backend of your choice with matching custom ops implementation, e.g. &lt;a href=&quot;https://caffe2.ai/docs/custom-operators.html&quot;&gt;Caffe2 custom ops&lt;/a&gt;, &lt;a href=&quot;https://github.com/microsoft/onnxruntime/blob/master/docs/AddingCustomOp.md&quot;&gt;ONNX Runtime custom ops&lt;/a&gt;.</source>
          <target state="translated">커스텀 연산자에 따라 하나 또는 기존 ONNX 작업의 조합으로 내보낼 수 있습니다. ONNX에서도 사용자 지정 작업으로 내보낼 수도 있습니다. 이 경우 내보내기시 &lt;code&gt;custom_opsets&lt;/code&gt; 사전을 사용하여 사용자 지정 도메인 및 버전 (사용자 지정 opset)을 지정할 수 있습니다 . 명시 적으로 지정되지 않은 경우 사용자 지정 opset 버전은 기본적으로 1로 설정됩니다. 사용자 지정 ONNX 작업을 사용하면 일치하는 사용자 지정 작업 구현 (예 : &lt;a href=&quot;https://caffe2.ai/docs/custom-operators.html&quot;&gt;Caffe2 사용자 지정 작업&lt;/a&gt; , &lt;a href=&quot;https://github.com/microsoft/onnxruntime/blob/master/docs/AddingCustomOp.md&quot;&gt;ONNX 런타임 사용자 지정 작업)으로&lt;/a&gt; 선택한 백엔드를 확장해야합니다 .</target>
        </trans-unit>
        <trans-unit id="0ab688461ca113091fd169e55e7c9e66deb886f0" translate="yes" xml:space="preserve">
          <source>Deprecated enum-like class for reduction operations: &lt;code&gt;SUM&lt;/code&gt;, &lt;code&gt;PRODUCT&lt;/code&gt;, &lt;code&gt;MIN&lt;/code&gt;, and &lt;code&gt;MAX&lt;/code&gt;.</source>
          <target state="translated">축소 작업에 대해 사용되지 않는 열거 형 클래스 : &lt;code&gt;SUM&lt;/code&gt; , &lt;code&gt;PRODUCT&lt;/code&gt; , &lt;code&gt;MIN&lt;/code&gt; 및 &lt;code&gt;MAX&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="44fd5a225ad67410d4630aa610f55ced5a7a7c71" translate="yes" xml:space="preserve">
          <source>Dequantizes an incoming tensor</source>
          <target state="translated">들어오는 텐서를 역 양자화합니다.</target>
        </trans-unit>
        <trans-unit id="55f8ebc805e65b5b71ddafdae390e3be2bcd69af" translate="yes" xml:space="preserve">
          <source>Description</source>
          <target state="translated">Description</target>
        </trans-unit>
        <trans-unit id="5966dbc5c4d197355b50f3dc66783c2fa78a5226" translate="yes" xml:space="preserve">
          <source>Design Notes</source>
          <target state="translated">디자인 노트</target>
        </trans-unit>
        <trans-unit id="fbe406308e1df41bfda74c701d4a66a5af72c8b6" translate="yes" xml:space="preserve">
          <source>Design Reasoning</source>
          <target state="translated">디자인 추론</target>
        </trans-unit>
        <trans-unit id="91441a05e70cda570458aad59617766bb3ed7236" translate="yes" xml:space="preserve">
          <source>Detaches the Tensor from the graph that created it, making it a leaf. Views cannot be detached in-place.</source>
          <target state="translated">Tensor를 생성 한 그래프에서 분리하여 리프로 만듭니다. 뷰는 제자리에서 분리 할 수 ​​없습니다.</target>
        </trans-unit>
        <trans-unit id="0e01337242e0297823f889a52bc9dad47f00c9ad" translate="yes" xml:space="preserve">
          <source>Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion &lt;a href=&quot;../tensor_attributes#type-promotion-doc&quot;&gt;documentation&lt;/a&gt;.</source>
          <target state="translated">유형 승격 &lt;a href=&quot;../tensor_attributes#type-promotion-doc&quot;&gt;문서에&lt;/a&gt; 설명 된 PyTorch 캐스팅 규칙에서 유형 변환이 허용되는지 여부를 결정합니다 .</target>
        </trans-unit>
        <trans-unit id="d2244fb2618c9d05384c145a55a7a5a37bf99078" translate="yes" xml:space="preserve">
          <source>Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion &lt;a href=&quot;tensor_attributes#type-promotion-doc&quot;&gt;documentation&lt;/a&gt;.</source>
          <target state="translated">유형 승격 &lt;a href=&quot;tensor_attributes#type-promotion-doc&quot;&gt;문서에&lt;/a&gt; 설명 된 PyTorch 캐스팅 규칙에서 유형 변환이 허용되는지 여부를 결정합니다 .</target>
        </trans-unit>
        <trans-unit id="a5a74a6df09278b88cb6ea23b7d7f2570c33babf" translate="yes" xml:space="preserve">
          <source>Device</source>
          <target state="translated">Device</target>
        </trans-unit>
        <trans-unit id="1c1f67e2f072a5af5bf17679821b62f151e69437" translate="yes" xml:space="preserve">
          <source>Dict Construction</source>
          <target state="translated">Dict 건설</target>
        </trans-unit>
        <trans-unit id="4461566599c5b88c3897f351e4ae4b0ddc655116" translate="yes" xml:space="preserve">
          <source>Different from the standard SVD, the size of returned matrices depend on the specified rank and q values as follows:</source>
          <target state="translated">표준 SVD와 달리 반환 된 행렬의 크기는 다음과 같이 지정된 순위 및 q 값에 따라 다릅니다.</target>
        </trans-unit>
        <trans-unit id="abe2291b9a77a8ae7585fd7e4b1df0a06016b998" translate="yes" xml:space="preserve">
          <source>Dimension names may contain characters or underscore. Furthermore, a dimension name must be a valid Python variable name (i.e., does not start with underscore).</source>
          <target state="translated">차원 이름에는 문자 또는 밑줄이 포함될 수 있습니다. 또한 차원 이름은 유효한 Python 변수 이름이어야합니다 (즉, 밑줄로 시작하지 않음).</target>
        </trans-unit>
        <trans-unit id="e1cf09d41f0116a87479fbe7f5ef39aeac24297b" translate="yes" xml:space="preserve">
          <source>Disable JIT for Debugging</source>
          <target state="translated">디버깅을 위해 JIT 비활성화</target>
        </trans-unit>
        <trans-unit id="dae5511126cad0d55eede647161bef3c8ecf5358" translate="yes" xml:space="preserve">
          <source>Disables denormal floating numbers on CPU.</source>
          <target state="translated">CPU에서 비정규 부동 숫자를 비활성화합니다.</target>
        </trans-unit>
        <trans-unit id="c513ea041a153d26658bb6cf95f8c77cf39ac2f0" translate="yes" xml:space="preserve">
          <source>Disabling gradient calculation is useful for inference, when you are sure that you will not call &lt;a href=&quot;../autograd#torch.Tensor.backward&quot;&gt;&lt;code&gt;Tensor.backward()&lt;/code&gt;&lt;/a&gt;. It will reduce memory consumption for computations that would otherwise have &lt;code&gt;requires_grad=True&lt;/code&gt;.</source>
          <target state="translated">그라디언트 계산을 비활성화하면 &lt;a href=&quot;../autograd#torch.Tensor.backward&quot;&gt; &lt;code&gt;Tensor.backward()&lt;/code&gt; &lt;/a&gt; 호출하지 않을 것이 확실 할 때 추론에 유용합니다 . 그렇지 않으면 &lt;code&gt;requires_grad=True&lt;/code&gt; 가있는 계산에 대한 메모리 소비를 줄일 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="f9c83b5259f966caf231a71b3ad8e69e880665ed" translate="yes" xml:space="preserve">
          <source>Discrete Fourier transforms and related functions.</source>
          <target state="translated">이산 푸리에 변환 및 관련 함수.</target>
        </trans-unit>
        <trans-unit id="938004e21f3f3e82099132266fc1891a7f13de20" translate="yes" xml:space="preserve">
          <source>Distance Functions</source>
          <target state="translated">거리 함수</target>
        </trans-unit>
        <trans-unit id="9cde45ececcca1e1adb41659d3d28a0420087495" translate="yes" xml:space="preserve">
          <source>Distance functions</source>
          <target state="translated">거리 기능</target>
        </trans-unit>
        <trans-unit id="303a9c010db654973c68f36cc8c79b5ce73fe308" translate="yes" xml:space="preserve">
          <source>Distributed Autograd Context</source>
          <target state="translated">분산 된 Autograd 컨텍스트</target>
        </trans-unit>
        <trans-unit id="310713a581eec3788cf900c68063e38885f001bd" translate="yes" xml:space="preserve">
          <source>Distributed Autograd Design</source>
          <target state="translated">분산 형 Autograd 디자인</target>
        </trans-unit>
        <trans-unit id="fa35316634dcb2108118798dd9f61f3d3f7fbb66" translate="yes" xml:space="preserve">
          <source>Distributed Autograd Framework</source>
          <target state="translated">분산 Autograd 프레임 워크</target>
        </trans-unit>
        <trans-unit id="ec9dd95759950a78e8ade825f089a889c2eb7fab" translate="yes" xml:space="preserve">
          <source>Distributed Backward Pass</source>
          <target state="translated">분산 된 역방향 패스</target>
        </trans-unit>
        <trans-unit id="aba915801825e438dbb9e75721a94e96b234ba7d" translate="yes" xml:space="preserve">
          <source>Distributed Data Parallel</source>
          <target state="translated">분산 데이터 병렬</target>
        </trans-unit>
        <trans-unit id="ef10fda1ae05d95a37d1371a4a918aad1148a2da" translate="yes" xml:space="preserve">
          <source>Distributed Key-Value Store</source>
          <target state="translated">분산 키-값 저장소</target>
        </trans-unit>
        <trans-unit id="34eb6d25d260c57a8faa37ee402539b363513315" translate="yes" xml:space="preserve">
          <source>Distributed Optimizer</source>
          <target state="translated">분산 최적화 도구</target>
        </trans-unit>
        <trans-unit id="75f572c75699c1008b8a3d33982f73a57479f298" translate="yes" xml:space="preserve">
          <source>Distributed Pipeline Parallel</source>
          <target state="translated">분산 파이프 라인 병렬</target>
        </trans-unit>
        <trans-unit id="5dfffd4675ffa868d517cf4ad2d7b981131e1dbd" translate="yes" xml:space="preserve">
          <source>Distributed RPC Framework</source>
          <target state="translated">분산 RPC 프레임 워크</target>
        </trans-unit>
        <trans-unit id="e3de08dea5da9f0cffd62432ca7da9726c39e218" translate="yes" xml:space="preserve">
          <source>Distributed communication package - torch.distributed</source>
          <target state="translated">분산 통신 패키지-torch.distributed</target>
        </trans-unit>
        <trans-unit id="67b250f513d5c0c4bc692b9447f357fc7ef369ea" translate="yes" xml:space="preserve">
          <source>DistributedDataParallel</source>
          <target state="translated">DistributedDataParallel</target>
        </trans-unit>
        <trans-unit id="ec463333beac30f92377ae176f21cc4eb4b46520" translate="yes" xml:space="preserve">
          <source>DistributedOptimizer takes remote references to parameters scattered across workers and applies the given optimizer locally for each parameter.</source>
          <target state="translated">DistributedOptimizer는 작업자에 흩어져있는 매개 변수에 대한 원격 참조를 가져와 각 매개 변수에 대해 로컬로 지정된 최적화 프로그램을 적용합니다.</target>
        </trans-unit>
        <trans-unit id="e4468b3425489b998cab9185522d9f737628cd1d" translate="yes" xml:space="preserve">
          <source>Divides each element of the input &lt;code&gt;input&lt;/code&gt; by the corresponding element of &lt;code&gt;other&lt;/code&gt;.</source>
          <target state="translated">입력 &lt;code&gt;input&lt;/code&gt; 의 각 요소 를 &lt;code&gt;other&lt;/code&gt; 의 해당 요소로 나눕니다 .</target>
        </trans-unit>
        <trans-unit id="33968e4e64500a95f8035e27727c823cadd48d34" translate="yes" xml:space="preserve">
          <source>Do cartesian product of the given sequence of tensors.</source>
          <target state="translated">주어진 텐서 시퀀스의 데카르트 곱을 수행하십시오.</target>
        </trans-unit>
        <trans-unit id="721f0d8eda69e29b574fd64fe0f6c7674a1a6f89" translate="yes" xml:space="preserve">
          <source>Do cartesian product of the given sequence of tensors. The behavior is similar to python&amp;rsquo;s &lt;code&gt;itertools.product&lt;/code&gt;.</source>
          <target state="translated">주어진 텐서 시퀀스의 데카르트 곱을 수행하십시오. 동작은 python의 &lt;code&gt;itertools.product&lt;/code&gt; 와 유사합니다 .</target>
        </trans-unit>
        <trans-unit id="fcb94671187d8e10ddb359c9decbb94311e20300" translate="yes" xml:space="preserve">
          <source>Docstring of the function works as a help message. It explains what does the model do and what are the allowed positional/keyword arguments. It&amp;rsquo;s highly recommended to add a few examples here.</source>
          <target state="translated">함수의 독 스트링은 도움말 메시지로 작동합니다. 모델이 수행하는 작업과 허용되는 위치 / 키워드 인수가 무엇인지 설명합니다. 여기에 몇 가지 예를 추가하는 것이 좋습니다.</target>
        </trans-unit>
        <trans-unit id="96f57a29074dff5e315a68cd23e9cc22e3995e9f" translate="yes" xml:space="preserve">
          <source>Does a linear interpolation of two tensors &lt;code&gt;start&lt;/code&gt; (given by &lt;code&gt;input&lt;/code&gt;) and &lt;code&gt;end&lt;/code&gt; based on a scalar or tensor &lt;code&gt;weight&lt;/code&gt; and returns the resulting &lt;code&gt;out&lt;/code&gt; tensor.</source>
          <target state="translated">두 텐서의 선형 보간 않음 &lt;code&gt;start&lt;/code&gt; (주어진 &lt;code&gt;input&lt;/code&gt; ) 및 &lt;code&gt;end&lt;/code&gt; 스칼라 또는 텐서에 기초 &lt;code&gt;weight&lt;/code&gt; 복귀 결과 &lt;code&gt;out&lt;/code&gt; 텐서.</target>
        </trans-unit>
        <trans-unit id="e06ffaf662b8ea46a067dea9fb380262a87db6a2" translate="yes" xml:space="preserve">
          <source>Down/up samples the input to either the given &lt;code&gt;size&lt;/code&gt; or the given &lt;code&gt;scale_factor&lt;/code&gt;</source>
          <target state="translated">입력을 주어진 &lt;code&gt;size&lt;/code&gt; 또는 주어진 &lt;code&gt;scale_factor&lt;/code&gt; 로 다운 / 업 샘플링합니다.</target>
        </trans-unit>
        <trans-unit id="e61af53453cf013b5be43bae97307de95bad250e" translate="yes" xml:space="preserve">
          <source>Download object at the given URL to a local path.</source>
          <target state="translated">주어진 URL의 개체를 로컬 경로로 다운로드합니다.</target>
        </trans-unit>
        <trans-unit id="361a0dc66d29b16c754d93724204f2ac9044c37d" translate="yes" xml:space="preserve">
          <source>Draws binary random numbers (0 or 1) from a Bernoulli distribution.</source>
          <target state="translated">Bernoulli 분포에서 이진 난수 (0 또는 1)를 그립니다.</target>
        </trans-unit>
        <trans-unit id="99a0d2b3ff3d729e26380413abca3d7df775cdde" translate="yes" xml:space="preserve">
          <source>Dropout</source>
          <target state="translated">Dropout</target>
        </trans-unit>
        <trans-unit id="132aa5f9595506ef0ca296d6be570b1e4aa7acb2" translate="yes" xml:space="preserve">
          <source>Dropout Layers</source>
          <target state="translated">드롭 아웃 레이어</target>
        </trans-unit>
        <trans-unit id="15a66e933b5615cb5433ce9102df020458b34bcb" translate="yes" xml:space="preserve">
          <source>Dropout functions</source>
          <target state="translated">드롭 아웃 기능</target>
        </trans-unit>
        <trans-unit id="cfe20a4021b31e2ea913c19efd2987a230ac723c" translate="yes" xml:space="preserve">
          <source>Dropout2d</source>
          <target state="translated">Dropout2d</target>
        </trans-unit>
        <trans-unit id="830971c9b2da3b705ee96a71f9d0d4e5128e9bb7" translate="yes" xml:space="preserve">
          <source>Dropout3d</source>
          <target state="translated">Dropout3d</target>
        </trans-unit>
        <trans-unit id="a7c8e330062301237ad2aac63813f57bd9bb1e14" translate="yes" xml:space="preserve">
          <source>Due to limited dynamic range of half datatype, performing this operation in half precision may cause the first element of result to overflow for certain inputs.</source>
          <target state="translated">half 데이터 유형의 제한된 동적 범위로 인해이 작업을 반 정밀도로 수행하면 특정 입력에 대해 결과의 첫 번째 요소가 오버플로 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="36b5672884c133b8c278456fa603d4322ce8f54a" translate="yes" xml:space="preserve">
          <source>Due to the asynchronous nature of CUDA kernels, when running against CUDA code, the cProfile output and CPU-mode autograd profilers may not show correct timings: the reported CPU time reports the amount of time used to launch the kernels but does not include the time the kernel spent executing on a GPU unless the operation does a synchronize. Ops that do synchronize appear to be extremely expensive under regular CPU-mode profilers. In these case where timings are incorrect, the CUDA-mode autograd profiler may be helpful.</source>
          <target state="translated">CUDA 커널의 비동기 특성으로 인해 CUDA 코드에 대해 실행할 때 cProfile 출력 및 CPU 모드 autograd 프로파일 러가 올바른 타이밍을 표시하지 않을 수 있습니다.보고 된 CPU 시간은 커널을 시작하는 데 사용 된 시간을보고하지만 시간은 포함하지 않습니다. 작업이 동기화를 수행하지 않는 한 커널은 GPU에서 실행하는 데 소비했습니다. 동기화를 수행하는 작업은 일반 CPU 모드 프로파일 러에서 매우 많은 비용이 드는 것으로 보입니다. 타이밍이 올바르지 않은 경우 CUDA 모드 autograd 프로파일 러가 도움이 될 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="aa6e674ff3a01dbf141329517674a02a51f4e055" translate="yes" xml:space="preserve">
          <source>Due to the conjugate symmetry, &lt;code&gt;input&lt;/code&gt; do not need to contain the full complex frequency values. Roughly half of the values will be sufficient, as is the case when &lt;code&gt;input&lt;/code&gt; is given by &lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt;&lt;code&gt;rfft()&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;rfft(signal, onesided=True)&lt;/code&gt;. In such case, set the &lt;code&gt;onesided&lt;/code&gt; argument of this method to &lt;code&gt;True&lt;/code&gt;. Moreover, the original signal shape information can sometimes be lost, optionally set &lt;code&gt;signal_sizes&lt;/code&gt; to be the size of the original signal (without the batch dimensions if in batched mode) to recover it with correct shape.</source>
          <target state="translated">켤레 대칭으로 인해 &lt;code&gt;input&lt;/code&gt; 은 전체 복소 주파수 값을 포함 할 필요가 없습니다. 시의 경우와 거의 절반의 값은 충분하다 &lt;code&gt;input&lt;/code&gt; 주어진다 &lt;a href=&quot;torch.rfft#torch.rfft&quot;&gt; &lt;code&gt;rfft()&lt;/code&gt; &lt;/a&gt; 와 &lt;code&gt;rfft(signal, onesided=True)&lt;/code&gt; . 이 경우이 메서드 의 &lt;code&gt;onesided&lt;/code&gt; 인수를 &lt;code&gt;True&lt;/code&gt; 로 설정 합니다. 또한 원래 신호 모양 정보가 손실 될 수 있습니다. 선택적으로 &lt;code&gt;signal_sizes&lt;/code&gt; 를 원래 신호의 크기로 설정하여 (배치 모드 인 경우 배치 차원없이) 올바른 모양으로 복구합니다.</target>
        </trans-unit>
        <trans-unit id="3f628f956c51a607f601ca3a9c65aebac60f6d34" translate="yes" xml:space="preserve">
          <source>Duplicate modules are returned only once. In the following example, &lt;code&gt;l&lt;/code&gt; will be returned only once.</source>
          <target state="translated">중복 모듈은 한 번만 반환됩니다. 다음 예에서 &lt;code&gt;l&lt;/code&gt; 은 한 번만 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="50fa35c3d84d578bb945e995330eef7c63dcf91c" translate="yes" xml:space="preserve">
          <source>During backward, only gradients at &lt;code&gt;nnz&lt;/code&gt; locations of &lt;code&gt;input&lt;/code&gt; will propagate back. Note that the gradients of &lt;code&gt;input&lt;/code&gt; is coalesced.</source>
          <target state="translated">역방향 동안에는 &lt;code&gt;input&lt;/code&gt; &lt;code&gt;nnz&lt;/code&gt; 위치에있는 그래디언트 만 다시 전파됩니다. &lt;code&gt;input&lt;/code&gt; 의 기울기 가 합쳐집니다.</target>
        </trans-unit>
        <trans-unit id="8e08ad58135bf66f71d67cdf9e3eb42be9cdd416" translate="yes" xml:space="preserve">
          <source>During evaluation the module simply computes an identity function.</source>
          <target state="translated">평가하는 동안 모듈은 단순히 식별 함수를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="c42cc58336b9bff21da008fbb38ac485f6938f2c" translate="yes" xml:space="preserve">
          <source>During inference, the model requires only the input tensors, and returns the post-processed predictions as a &lt;code&gt;List[Dict[Tensor]]&lt;/code&gt;, one for each input image. The fields of the &lt;code&gt;Dict&lt;/code&gt; are as follows:</source>
          <target state="translated">추론하는 동안 모델은 입력 텐서 만 필요하고 후 처리 된 예측 을 각 입력 이미지에 대해 하나씩 &lt;code&gt;List[Dict[Tensor]]&lt;/code&gt; 로 반환합니다 . &lt;code&gt;Dict&lt;/code&gt; 의 필드는 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="899a3b765d6ee8d823d5852dbea555348e22fa78" translate="yes" xml:space="preserve">
          <source>During training, it randomly masks some of the elements of the input tensor with probability &lt;em&gt;p&lt;/em&gt; using samples from a bernoulli distribution. The elements to masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit standard deviation.</source>
          <target state="translated">훈련 중에 베르누이 분포의 샘플을 사용하여 입력 텐서의 일부 요소를 확률 &lt;em&gt;p&lt;/em&gt; 로 무작위로 마스킹 합니다. 마스킹 할 요소는 모든 포워드 호출에서 무작위 화되고 평균 0과 단위 표준 편차를 유지하기 위해 크기가 조정되고 이동됩니다.</target>
        </trans-unit>
        <trans-unit id="1ddb72e8fb2bca4bbdd9cee1bd0f4124ba674fe1" translate="yes" xml:space="preserve">
          <source>During training, randomly zeroes some of the elements of the input tensor with probability &lt;code&gt;p&lt;/code&gt; using samples from a Bernoulli distribution.</source>
          <target state="translated">훈련 중에 Bernoulli 분포의 표본을 사용하여 확률 &lt;code&gt;p&lt;/code&gt; 로 입력 텐서의 일부 요소를 무작위로 0으로 만듭니다.</target>
        </trans-unit>
        <trans-unit id="7e07f3de383a6b909b32c2c8420ab783da80d74b" translate="yes" xml:space="preserve">
          <source>During training, randomly zeroes some of the elements of the input tensor with probability &lt;code&gt;p&lt;/code&gt; using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call.</source>
          <target state="translated">훈련 중에 Bernoulli 분포의 표본을 사용하여 확률 &lt;code&gt;p&lt;/code&gt; 로 입력 텐서의 일부 요소를 무작위로 0으로 만듭니다. 각 채널은 모든 착신 전환에서 독립적으로 0이됩니다.</target>
        </trans-unit>
        <trans-unit id="e72931d7ae530922c7186be0d18873ff29d42369" translate="yes" xml:space="preserve">
          <source>During training, the model expects both the input tensors, as well as a targets (list of dictionary), containing:</source>
          <target state="translated">학습 중에 모델은 입력 텐서와 다음을 포함하는 대상 (사전 목록)을 모두 예상합니다.</target>
        </trans-unit>
        <trans-unit id="e0184adedf913b076626646d3f52c3b49c39ad6d" translate="yes" xml:space="preserve">
          <source>E</source>
          <target state="translated">E</target>
        </trans-unit>
        <trans-unit id="a466f9bd6da8a16be528d1b97b22f8b80c50fc61" translate="yes" xml:space="preserve">
          <source>E (&lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;Tensor&lt;/a&gt;)</source>
          <target state="translated">E ( &lt;a href=&quot;../tensors#torch.Tensor&quot;&gt;텐서&lt;/a&gt; )</target>
        </trans-unit>
        <trans-unit id="db547439274fed58f2b9ee57841dc8931b457c07" translate="yes" xml:space="preserve">
          <source>ELU</source>
          <target state="translated">ELU</target>
        </trans-unit>
        <trans-unit id="c4cf619f3157e4a678ac53e564134cffe6210613" translate="yes" xml:space="preserve">
          <source>Each &lt;code&gt;torch.Tensor&lt;/code&gt; has a &lt;a href=&quot;#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;#torch.torch.layout&quot;&gt;&lt;code&gt;torch.layout&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">각 &lt;code&gt;torch.Tensor&lt;/code&gt; 에는 &lt;a href=&quot;#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;#torch.torch.layout&quot;&gt; &lt;code&gt;torch.layout&lt;/code&gt; 이&lt;/a&gt; 있습니다.</target>
        </trans-unit>
        <trans-unit id="1c0427e3aafaddc6d9583e9cefbad6140b6d07cf" translate="yes" xml:space="preserve">
          <source>Each element of the tensor &lt;code&gt;input&lt;/code&gt; is multiplied by the corresponding element of the Tensor &lt;code&gt;other&lt;/code&gt;. The resulting tensor is returned.</source>
          <target state="translated">텐서 &lt;code&gt;input&lt;/code&gt; 의 각 요소에 Tensor &lt;code&gt;other&lt;/code&gt; 의 해당 요소가 곱해집니다 . 결과 텐서가 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="728738a25fa4324927c05f66f2cfc30782cc84df" translate="yes" xml:space="preserve">
          <source>Each element of the tensor &lt;code&gt;other&lt;/code&gt; is multiplied by the scalar &lt;code&gt;alpha&lt;/code&gt; and added to each element of the tensor &lt;code&gt;input&lt;/code&gt;. The resulting tensor is returned.</source>
          <target state="translated">&lt;code&gt;other&lt;/code&gt; 텐서의 각 요소에 스칼라 &lt;code&gt;alpha&lt;/code&gt; 곱하고 텐서 &lt;code&gt;input&lt;/code&gt; 의 각 요소에 더 합니다 . 결과 텐서가 반환됩니다.</target>
        </trans-unit>
        <trans-unit id="6ea1bfa1407efaeb9ea4478b91f07efe596058d3" translate="yes" xml:space="preserve">
          <source>Each element will be masked independently on every forward call with probability &lt;code&gt;p&lt;/code&gt; using samples from a Bernoulli distribution. The elements to be masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit variance.</source>
          <target state="translated">각 요소는 Bernoulli 분포의 샘플을 사용하여 확률 &lt;code&gt;p&lt;/code&gt; 로 모든 착신 전환에서 독립적으로 마스킹됩니다 . 마스킹 할 요소는 모든 포워드 호출에서 무작위 화되고 평균 및 단위 분산을 0으로 유지하기 위해 크기가 조정되고 이동됩니다.</target>
        </trans-unit>
        <trans-unit id="04229e567ec03d81b5712793e639928468412520" translate="yes" xml:space="preserve">
          <source>Each process contains an independent Python interpreter, eliminating the extra interpreter overhead and &amp;ldquo;GIL-thrashing&amp;rdquo; that comes from driving several execution threads, model replicas, or GPUs from a single Python process. This is especially important for models that make heavy use of the Python runtime, including models with recurrent layers or many small components.</source>
          <target state="translated">각 프로세스에는 독립적 인 Python 인터프리터가 포함되어있어 단일 Python 프로세스에서 여러 실행 스레드, 모델 복제본 또는 GPU를 구동 할 때 발생하는 추가 인터프리터 오버 헤드와 &quot;GIL 스 래싱&quot;을 제거합니다. 이는 반복 레이어 또는 많은 작은 구성 요소가있는 모델을 포함하여 Python 런타임을 많이 사용하는 모델에 특히 중요합니다.</target>
        </trans-unit>
        <trans-unit id="3c11c2aae826827df9743912100e743d6a724614" translate="yes" xml:space="preserve">
          <source>Each process maintains its own optimizer and performs a complete optimization step with each iteration. While this may appear redundant, since the gradients have already been gathered together and averaged across processes and are thus the same for every process, this means that no parameter broadcast step is needed, reducing time spent transferring tensors between nodes.</source>
          <target state="translated">각 프로세스는 자체 최적화 프로그램을 유지하고 각 반복마다 완전한 최적화 단계를 수행합니다. 중복 된 것처럼 보일 수 있지만 그래디언트가 이미 함께 수집되고 프로세스간에 평균화되어 모든 프로세스에 대해 동일하므로 매개 변수 브로드 캐스트 단계가 필요하지 않으므로 노드간에 텐서를 전송하는 데 소요되는 시간이 줄어 듭니다.</target>
        </trans-unit>
        <trans-unit id="25ddc73f3c685d39e655eadbdd24861cccbd855c" translate="yes" xml:space="preserve">
          <source>Each process scatters list of input tensors to all processes in a group and return gathered list of tensors in output list.</source>
          <target state="translated">각 프로세스는 입력 텐서 목록을 그룹의 모든 프로세스에 분산시키고 수집 된 텐서 목록을 출력 목록에 반환합니다.</target>
        </trans-unit>
        <trans-unit id="e01a4afade34e5c383597dd828cf5ce98ca5f574" translate="yes" xml:space="preserve">
          <source>Each process will receive exactly one tensor and store its data in the &lt;code&gt;tensor&lt;/code&gt; argument.</source>
          <target state="translated">각 프로세스는 정확히 하나의 텐서를 수신하고 해당 데이터를 &lt;code&gt;tensor&lt;/code&gt; 인수 에 저장합니다 .</target>
        </trans-unit>
        <trans-unit id="a930fc3f8ffe54e36efcacdd772fc5e7328c868c" translate="yes" xml:space="preserve">
          <source>Each tensor has an associated &lt;code&gt;torch.Storage&lt;/code&gt;, which holds its data. The tensor class also provides multi-dimensional, &lt;a href=&quot;https://en.wikipedia.org/wiki/Stride_of_an_array&quot;&gt;strided&lt;/a&gt; view of a storage and defines numeric operations on it.</source>
          <target state="translated">각 텐서에는 데이터를 보유 하는 연결된 &lt;code&gt;torch.Storage&lt;/code&gt; 가 있습니다. 텐서 클래스는 또한 스토리지에 대한 다차원의 &lt;a href=&quot;https://en.wikipedia.org/wiki/Stride_of_an_array&quot;&gt;스트라이드&lt;/a&gt; 뷰를 제공하고 이에 대한 숫자 연산을 정의합니다.</target>
        </trans-unit>
        <trans-unit id="0df7dd6db96e14d83140e8da4324d11663fe359e" translate="yes" xml:space="preserve">
          <source>Each tensor in &lt;code&gt;output_tensor_list&lt;/code&gt; should reside on a separate GPU, as should each list of tensors in &lt;code&gt;input_tensor_lists&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;output_tensor_list&lt;/code&gt; 의 각 텐서 는 &lt;code&gt;input_tensor_lists&lt;/code&gt; 의 각 텐서 목록과 마찬가지로 별도의 GPU에 있어야합니다 .</target>
        </trans-unit>
        <trans-unit id="96aad5e1712347cedb5784e57172eb768752a304" translate="yes" xml:space="preserve">
          <source>Efficient softmax approximation as described in &lt;a href=&quot;https://arxiv.org/abs/1609.04309&quot;&gt;Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss&amp;eacute;, David Grangier, and Herv&amp;eacute; J&amp;eacute;gou&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;https://arxiv.org/abs/1609.04309&quot;&gt;Edouard Grave, Armand Joulin, Moustapha Ciss&amp;eacute;, David Grangier 및 Herv&amp;eacute; J&amp;eacute;gou의 GPU에 대한 효율적인 소프트 맥스&lt;/a&gt; 근사에 설명 된대로 효율적인 소프트 맥스 근사 .</target>
        </trans-unit>
        <trans-unit id="cba1a5641f37d31826ff43a90ac27303cd2de000" translate="yes" xml:space="preserve">
          <source>Element-wise arctangent of</source>
          <target state="translated">요소 별 아크 탄젠트</target>
        </trans-unit>
        <trans-unit id="c0ce0d75b4a69f8d83fed56dda584bc104824e33" translate="yes" xml:space="preserve">
          <source>Elements lower than min and higher than max are ignored.</source>
          <target state="translated">최소보다 낮고 최대보다 높은 요소는 무시됩니다.</target>
        </trans-unit>
        <trans-unit id="279868256307295ed763af9335611205d2a9e0e9" translate="yes" xml:space="preserve">
          <source>Eliminates all but the first element from every consecutive group of equivalent elements.</source>
          <target state="translated">동일한 요소의 모든 연속 그룹에서 첫 번째 요소를 제외한 모든 요소를 ​​제거합니다.</target>
        </trans-unit>
        <trans-unit id="65e872502ba89d481aa8b75804d63958258d47d9" translate="yes" xml:space="preserve">
          <source>Embedding</source>
          <target state="translated">Embedding</target>
        </trans-unit>
        <trans-unit id="fb52acf7809d84b49f20c1e0c2aeb4d9421e883d" translate="yes" xml:space="preserve">
          <source>Embedding (no optional arguments supported)</source>
          <target state="translated">임베딩 (지원되는 선택적 인수 없음)</target>
        </trans-unit>
        <trans-unit id="ad4c45718740fb7fd09a983448014fcde632287c" translate="yes" xml:space="preserve">
          <source>EmbeddingBag</source>
          <target state="translated">EmbeddingBag</target>
        </trans-unit>
        <trans-unit id="f25f65444d58a4000e445a3586674f2f0a1c519b" translate="yes" xml:space="preserve">
          <source>EmbeddingBag also supports per-sample weights as an argument to the forward pass. This scales the output of the Embedding before performing a weighted reduction as specified by &lt;code&gt;mode&lt;/code&gt;. If &lt;code&gt;per_sample_weights`&lt;/code&gt; is passed, the only supported &lt;code&gt;mode&lt;/code&gt; is &lt;code&gt;&quot;sum&quot;&lt;/code&gt;, which computes a weighted sum according to &lt;code&gt;per_sample_weights&lt;/code&gt;.</source>
          <target state="translated">EmbeddingBag는 또한 순방향 패스에 대한 인수로 샘플 당 가중치를 지원합니다. 이것은 &lt;code&gt;mode&lt;/code&gt; 에 의해 지정된 가중치 감소를 수행하기 전에 Embedding의 출력을 조정 합니다 . 경우 &lt;code&gt;per_sample_weights`&lt;/code&gt; 가 전달되면, 지원되는 &lt;code&gt;mode&lt;/code&gt; 이다 &lt;code&gt;&quot;sum&quot;&lt;/code&gt; 에 따른 가중 된 합을 계산하고, &lt;code&gt;per_sample_weights&lt;/code&gt; 이 .</target>
        </trans-unit>
        <trans-unit id="7a184f78683775965d53e63c45e77c29a82cb431" translate="yes" xml:space="preserve">
          <source>Enables .grad attribute for non-leaf Tensors.</source>
          <target state="translated">리프가 아닌 Tensor에 .grad 속성을 사용합니다.</target>
        </trans-unit>
        <trans-unit id="4ba6db1c18e6b89f8ae8769b517165aebdeeb5e7" translate="yes" xml:space="preserve">
          <source>Enables gradient calculation, if it has been disabled via &lt;a href=&quot;torch.no_grad#torch.no_grad&quot;&gt;&lt;code&gt;no_grad&lt;/code&gt;&lt;/a&gt; or &lt;a href=&quot;torch.set_grad_enabled#torch.set_grad_enabled&quot;&gt;&lt;code&gt;set_grad_enabled&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;torch.no_grad#torch.no_grad&quot;&gt; &lt;code&gt;no_grad&lt;/code&gt; &lt;/a&gt; 또는 &lt;a href=&quot;torch.set_grad_enabled#torch.set_grad_enabled&quot;&gt; &lt;code&gt;set_grad_enabled&lt;/code&gt; &lt;/a&gt; 를 통해 비활성화 된 경우 그래디언트 계산을 활성화합니다 .</target>
        </trans-unit>
        <trans-unit id="129b2e35235ca3917459fbda73dccc7c7ba4bb9f" translate="yes" xml:space="preserve">
          <source>Ensures that the tensor memory is not reused for another tensor until all current work queued on &lt;code&gt;stream&lt;/code&gt; are complete.</source>
          <target state="translated">&lt;code&gt;stream&lt;/code&gt; 에 대기중인 모든 현재 작업 이 완료 될 때까지 텐서 메모리가 다른 텐서에 재사용되지 않도록 합니다.</target>
        </trans-unit>
        <trans-unit id="02f525cd5d7591c6c78960a007006cf272db0f53" translate="yes" xml:space="preserve">
          <source>Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers.</source>
          <target state="translated">진입 점 함수는 모델 (nn.module) 또는 사용자 워크 플로를 더 원활하게 만드는 보조 도구 (예 : 토크 나이저)를 반환 할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b0dd14c2fc526f82985ca8c8a3a6e1c3b85b6488" translate="yes" xml:space="preserve">
          <source>Environment variable initialization</source>
          <target state="translated">환경 변수 초기화</target>
        </trans-unit>
        <trans-unit id="089481a55a3bf2c4a369d848036bccebf8abe33b" translate="yes" xml:space="preserve">
          <source>Equivalent to input[:,::-1]. Requires the array to be at least 2-D.</source>
          <target state="translated">input [:, ::-1]과 같습니다. 배열이 2 차원 이상이어야합니다.</target>
        </trans-unit>
        <trans-unit id="3b4933d2950181258ba9867fa0e789a6fac6c0e2" translate="yes" xml:space="preserve">
          <source>Equivalent to input[::-1,&amp;hellip;]. Requires the array to be at least 1-D.</source>
          <target state="translated">input [::-1,&amp;hellip;]과 같습니다. 배열이 1 차원 이상이어야합니다.</target>
        </trans-unit>
        <trans-unit id="9c1eedde4e72567b3c48e7393929af1241088b7e" translate="yes" xml:space="preserve">
          <source>Errors such as timeouts for the &lt;code&gt;remote&lt;/code&gt; API are handled on a best-effort basis. This means that when remote calls initiated by &lt;code&gt;remote&lt;/code&gt; fail, such as with a timeout error, we take a best-effort approach to error handling. This means that errors are handled and set on the resulting RRef on an asynchronous basis. If the RRef has not been used by the application before this handling (such as &lt;code&gt;to_here&lt;/code&gt; or fork call), then future uses of the &lt;code&gt;RRef&lt;/code&gt; will appropriately raise errors. However, it is possible that the user application will use the &lt;code&gt;RRef&lt;/code&gt; before the errors are handled. In this case, errors may not be raised as they have not yet been handled.</source>
          <target state="translated">&lt;code&gt;remote&lt;/code&gt; API에 대한 시간 초과와 같은 오류 는 최선을 다해 처리됩니다. 즉 , 시간 초과 오류와 같이 &lt;code&gt;remote&lt;/code&gt; 시작된 원격 호출이 실패 할 때 오류 처리에 최선의 접근 방식을 취합니다. 이는 오류가 비동기식으로 결과 RRef에서 처리되고 설정됨을 의미합니다. 이 처리 전에 응용 프로그램에서 RRef를 사용하지 않은 경우 (예 : &lt;code&gt;to_here&lt;/code&gt; 또는 fork 호출) 나중에 &lt;code&gt;RRef&lt;/code&gt; 를 사용 하면 오류가 발생합니다. 그러나 오류가 처리되기 전에 사용자 응용 프로그램이 &lt;code&gt;RRef&lt;/code&gt; 를 사용할 수 있습니다 . 이 경우 아직 처리되지 않았으므로 오류가 발생하지 않을 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="2352230ae4369b2400cd9a8b59625450e193a06a" translate="yes" xml:space="preserve">
          <source>Estimate</source>
          <target state="translated">Estimate</target>
        </trans-unit>
        <trans-unit id="9f5d3b9ce4cb19a213d647b445b4900659d33a6d" translate="yes" xml:space="preserve">
          <source>Evaluates module(input) in parallel across the GPUs given in device_ids.</source>
          <target state="translated">device_ids에 지정된 GPU에서 병렬로 모듈 (입력)을 평가합니다.</target>
        </trans-unit>
        <trans-unit id="5b74f76cd70090d77d5d968780f896243c537f02" translate="yes" xml:space="preserve">
          <source>Every &lt;a href=&quot;tensors#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt; has a corresponding storage of the same data type.</source>
          <target state="translated">모든 &lt;a href=&quot;tensors#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt; 에는 동일한 데이터 유형의 해당 스토리지가 있습니다.</target>
        </trans-unit>
        <trans-unit id="88cdbf2ac191cecaae2230a964d90524ead7b613" translate="yes" xml:space="preserve">
          <source>Every collective operation function supports the following two kinds of operations:</source>
          <target state="translated">모든 집합 연산 기능은 다음 두 종류의 연산을 지원합니다.</target>
        </trans-unit>
        <trans-unit id="cac312ad37989d865266909c5b6681a3176edc55" translate="yes" xml:space="preserve">
          <source>Everything in a user defined &lt;a href=&quot;torchscript-class&quot;&gt;TorchScript Class&lt;/a&gt; is exported by default, functions can be decorated with &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt;&lt;code&gt;@torch.jit.ignore&lt;/code&gt;&lt;/a&gt; if needed.</source>
          <target state="translated">사용자 정의 &lt;a href=&quot;torchscript-class&quot;&gt;TorchScript 클래스의&lt;/a&gt; 모든 것은 기본적으로 내보내지며 필요한 경우 &lt;a href=&quot;generated/torch.jit.ignore#torch.jit.ignore&quot;&gt; &lt;code&gt;@torch.jit.ignore&lt;/code&gt; &lt;/a&gt; 함수를 꾸밀 수 있습니다 .</target>
        </trans-unit>
        <trans-unit id="0f01ed56a1e32a05e5ef96e4d779f34784af9a96" translate="yes" xml:space="preserve">
          <source>Example</source>
          <target state="translated">Example</target>
        </trans-unit>
        <trans-unit id="b3c7ccbaed13d7cf475ac0a8540d9af04a1abfba" translate="yes" xml:space="preserve">
          <source>Example (a type mismatch)</source>
          <target state="translated">예 (유형 불일치)</target>
        </trans-unit>
        <trans-unit id="fb18992fbb684cb9203c712439d9448a7d50b7c9" translate="yes" xml:space="preserve">
          <source>Example (an exported and ignored method in a module):</source>
          <target state="translated">예 (모듈에서 내보내고 무시 된 메서드) :</target>
        </trans-unit>
        <trans-unit id="3fb80256910d817ce9141fdd4451ee10894ec35c" translate="yes" xml:space="preserve">
          <source>Example (calling a script function in a traced function):</source>
          <target state="translated">예 (추적 된 함수에서 스크립트 함수 호출) :</target>
        </trans-unit>
        <trans-unit id="b5bcda1a3bfb1af96df6dd020d0588805d88feba" translate="yes" xml:space="preserve">
          <source>Example (calling a traced function in script):</source>
          <target state="translated">예 (스크립트에서 추적 된 함수 호출) :</target>
        </trans-unit>
        <trans-unit id="9488ed97f58e814938180bd3de0fc03a37f8d8c1" translate="yes" xml:space="preserve">
          <source>Example (fork a free function):</source>
          <target state="translated">예 (프리 함수 포크) :</target>
        </trans-unit>
        <trans-unit id="8d40b7fb2ebedc8ff715bb44861bcfbdc43fa817" translate="yes" xml:space="preserve">
          <source>Example (fork a module method):</source>
          <target state="translated">예 (모듈 메서드 분기) :</target>
        </trans-unit>
        <trans-unit id="c61fd1aa74a0dc25c5318d75c242307960c7041c" translate="yes" xml:space="preserve">
          <source>Example (refining types on parameters and locals):</source>
          <target state="translated">예 (매개 변수 및 지역에 대한 유형 구체화) :</target>
        </trans-unit>
        <trans-unit id="9f3b97fb80872ca69d304fba5bcdeedcdbea8e85" translate="yes" xml:space="preserve">
          <source>Example (scripting a function):</source>
          <target state="translated">예 (함수 스크립팅) :</target>
        </trans-unit>
        <trans-unit id="de4e7f540085eb6296ec8260ef53372b15064ca1" translate="yes" xml:space="preserve">
          <source>Example (scripting a module with traced submodules):</source>
          <target state="translated">예 (추적 된 하위 모듈로 모듈 스크립팅) :</target>
        </trans-unit>
        <trans-unit id="c5b0acb6ce867883478ae6d5579cfda80cbb9404" translate="yes" xml:space="preserve">
          <source>Example (scripting a simple module with a Parameter):</source>
          <target state="translated">예 (매개 변수로 간단한 모듈 스크립팅) :</target>
        </trans-unit>
        <trans-unit id="98fb0963dfe59db2efb10e03278c046397333fb9" translate="yes" xml:space="preserve">
          <source>Example (tracing a function):</source>
          <target state="translated">예 (함수 추적) :</target>
        </trans-unit>
        <trans-unit id="0dad6a76da0fc25a19704212a257d6379a12034b" translate="yes" xml:space="preserve">
          <source>Example (tracing a module with multiple methods):</source>
          <target state="translated">예 (여러 메서드로 모듈 추적) :</target>
        </trans-unit>
        <trans-unit id="07f0e6f11243b3ae74d74b1d28cef8c785cb16fa" translate="yes" xml:space="preserve">
          <source>Example (tracing an existing module):</source>
          <target state="translated">예 (기존 모듈 추적) :</target>
        </trans-unit>
        <trans-unit id="68ede4837476671e511c48828e0de48f7b06d426" translate="yes" xml:space="preserve">
          <source>Example (type annotations for Python 3):</source>
          <target state="translated">예 (Python 3의 유형 주석) :</target>
        </trans-unit>
        <trans-unit id="87a104ad2463311906e75683079bbad94a4fd5c2" translate="yes" xml:space="preserve">
          <source>Example (using &lt;code&gt;@torch.jit.export&lt;/code&gt; on a method):</source>
          <target state="translated">예 ( 메서드에 &lt;code&gt;@torch.jit.export&lt;/code&gt; 사용 ) :</target>
        </trans-unit>
        <trans-unit id="d1c7a00dbf049f876c9759d8441a029ed8e2f4ac" translate="yes" xml:space="preserve">
          <source>Example (using &lt;code&gt;@torch.jit.ignore(drop=True)&lt;/code&gt; on a method):</source>
          <target state="translated">예제 ( 메서드에 &lt;code&gt;@torch.jit.ignore(drop=True)&lt;/code&gt; 사용) :</target>
        </trans-unit>
        <trans-unit id="f45fe2e88f095f6547360c930d8119bf8c200149" translate="yes" xml:space="preserve">
          <source>Example (using &lt;code&gt;@torch.jit.ignore&lt;/code&gt; on a method):</source>
          <target state="translated">예 ( 메서드에 &lt;code&gt;@torch.jit.ignore&lt;/code&gt; 사용 ) :</target>
        </trans-unit>
        <trans-unit id="ee3db4aefed96eb801a5613278d0970a19c709cc" translate="yes" xml:space="preserve">
          <source>Example (using &lt;code&gt;@torch.jit.unused&lt;/code&gt; on a method):</source>
          <target state="translated">예 ( 메서드에 &lt;code&gt;@torch.jit.unused&lt;/code&gt; 사용 ) :</target>
        </trans-unit>
        <trans-unit id="15234a3935f9814be4e208815ad355afc291f504" translate="yes" xml:space="preserve">
          <source>Example (using a traced module):</source>
          <target state="translated">예 (추적 된 모듈 사용) :</target>
        </trans-unit>
        <trans-unit id="49895a4623d5b4a9344031509b184da5a2c67818" translate="yes" xml:space="preserve">
          <source>Example. if we have the following shape for inputs and outputs:</source>
          <target state="translated">예. 입력 및 출력에 대해 다음과 같은 모양이있는 경우 :</target>
        </trans-unit>
        <trans-unit id="c63737abd7347a7ae582cb9fbdf37d6c0e5b251e" translate="yes" xml:space="preserve">
          <source>Example:</source>
          <target state="translated">Example:</target>
        </trans-unit>
        <trans-unit id="8ec20a84a991e14aeedf10c8e4e9247bf2c9315c" translate="yes" xml:space="preserve">
          <source>Example: End-to-end AlexNet from PyTorch to ONNX</source>
          <target state="translated">예 : PyTorch에서 ONNX 로의 종단 간 AlexNet</target>
        </trans-unit>
        <trans-unit id="038d2f8486ff39be2d765514d254dcc770c02da8" translate="yes" xml:space="preserve">
          <source>Example: Suppose the last window is: &lt;code&gt;[17, 18, 0, 0, 0]&lt;/code&gt; vs &lt;code&gt;[18, 0, 0, 0, 0]&lt;/code&gt;</source>
          <target state="translated">예 : 마지막 창이 다음과 같다고 가정합니다. &lt;code&gt;[17, 18, 0, 0, 0]&lt;/code&gt; 대 &lt;code&gt;[18, 0, 0, 0, 0]&lt;/code&gt;</target>
        </trans-unit>
        <trans-unit id="6104a08ed7eb335488d3ea3f93c6210a1cc4746c" translate="yes" xml:space="preserve">
          <source>Example::</source>
          <target state="translated">Example::</target>
        </trans-unit>
        <trans-unit id="eb01bf04c9a0e8a71c45816513df424f1c7ffedb" translate="yes" xml:space="preserve">
          <source>Examples</source>
          <target state="translated">Examples</target>
        </trans-unit>
        <trans-unit id="fb3447b632f6a431215776dcf254a01001a40c4f" translate="yes" xml:space="preserve">
          <source>Examples:</source>
          <target state="translated">Examples:</target>
        </trans-unit>
        <trans-unit id="de3e24010b1050a8265462ab7e8f3a95c00717ea" translate="yes" xml:space="preserve">
          <source>Examples::</source>
          <target state="translated">Examples::</target>
        </trans-unit>
        <trans-unit id="8cc1b49841e2eac10bc692d1dee6f79c0d6dc0f7" translate="yes" xml:space="preserve">
          <source>Expand this tensor to the same size as &lt;code&gt;other&lt;/code&gt;. &lt;code&gt;self.expand_as(other)&lt;/code&gt; is equivalent to &lt;code&gt;self.expand(other.size())&lt;/code&gt;.</source>
          <target state="translated">이 텐서를 &lt;code&gt;other&lt;/code&gt; 와 같은 크기로 확장합니다 . &lt;code&gt;self.expand_as(other)&lt;/code&gt; 는 &lt;code&gt;self.expand(other.size())&lt;/code&gt; 와 동일합니다 .</target>
        </trans-unit>
        <trans-unit id="7381344dd49d389b0f60204fb4dbd8a7ee76371e" translate="yes" xml:space="preserve">
          <source>Expanding a tensor does not allocate new memory, but only creates a new view on the existing tensor where a dimension of size one is expanded to a larger size by setting the &lt;code&gt;stride&lt;/code&gt; to 0. Any dimension of size 1 can be expanded to an arbitrary value without allocating new memory.</source>
          <target state="translated">텐서를 확장하면 새 메모리가 할당되지 않고, &lt;code&gt;stride&lt;/code&gt; 를 0 으로 설정하여 크기 1의 차원이 더 큰 크기로 확장되는 기존 텐서에 새 뷰만 생성 됩니다. 크기 1의 모든 차원은 임의의 값으로 확장 할 수 있습니다. 새로운 메모리를 할당하지 않고.</target>
        </trans-unit>
        <trans-unit id="76b27001feb75250d904bb68f5aaa46711a6a0f1" translate="yes" xml:space="preserve">
          <source>Expands the dimension &lt;a href=&quot;tensors#torch.Tensor.dim&quot;&gt;&lt;code&gt;dim&lt;/code&gt;&lt;/a&gt; of the &lt;code&gt;self&lt;/code&gt; tensor over multiple dimensions of sizes given by &lt;code&gt;sizes&lt;/code&gt;.</source>
          <target state="translated">차원 확장 &lt;a href=&quot;tensors#torch.Tensor.dim&quot;&gt; &lt;code&gt;dim&lt;/code&gt; &lt;/a&gt; 의 &lt;code&gt;self&lt;/code&gt; 에 의해 주어진 크기의 다차원 위에 텐서 &lt;code&gt;sizes&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="6e40e001bf1e0b420d657a639d6d67387b6d4923" translate="yes" xml:space="preserve">
          <source>Expected inputs are spatial (4 dimensional). Use &lt;code&gt;upsample_trilinear&lt;/code&gt; fo volumetric (5 dimensional) inputs.</source>
          <target state="translated">예상되는 입력은 공간 (4 차원)입니다. 체적 (5 차원) 입력에 대해 &lt;code&gt;upsample_trilinear&lt;/code&gt; 를 사용 합니다.</target>
        </trans-unit>
        <trans-unit id="551ccd41342efa881333f0f1e558164218f9345a" translate="yes" xml:space="preserve">
          <source>Expected result:</source>
          <target state="translated">예상 결과:</target>
        </trans-unit>
        <trans-unit id="8439d6715059ee70b74368357e1335f786d20b96" translate="yes" xml:space="preserve">
          <source>Expects &lt;code&gt;input&lt;/code&gt; to be &amp;lt;= 2-D tensor and transposes dimensions 0 and 1.</source>
          <target state="translated">&lt;code&gt;input&lt;/code&gt; 이 2 차원 텐서보다 작을 것으로 예상 하고 차원 0과 1을 전치합니다.</target>
        </trans-unit>
        <trans-unit id="8133389ca86b79f9ac63f2057897dfbe1cda5cc8" translate="yes" xml:space="preserve">
          <source>Explicit alignment by names</source>
          <target state="translated">이름에 의한 명시 적 정렬</target>
        </trans-unit>
        <trans-unit id="58b807aacff8abe3f97a111c7a1e5f71d192fe91" translate="yes" xml:space="preserve">
          <source>Export a model into ONNX format. This exporter runs your model once in order to get a trace of its execution to be exported; at the moment, it supports a limited set of dynamic models (e.g., RNNs.)</source>
          <target state="translated">모델을 ONNX 형식으로 내 보냅니다. 이 익스포터는 익스포트 될 실행 추적을 얻기 위해 모델을 한 번 실행합니다. 현재로서는 제한된 동적 모델 (예 : RNN) 세트를 지원합니다.</target>
        </trans-unit>
        <trans-unit id="69c85a023a54b1f8defe44bdc84c4b1308b399c8" translate="yes" xml:space="preserve">
          <source>Exporting models with unsupported ONNX operators can be achieved using the &lt;code&gt;operator_export_type&lt;/code&gt; flag in export API. This flag is useful when users try to export ATen and non-ATen operators that are not registered and supported in ONNX.</source>
          <target state="translated">지원되지 않는 ONNX 연산자가있는 모델 내보내기 는 내보내기 API 의 &lt;code&gt;operator_export_type&lt;/code&gt; 플래그를 사용하여 수행 할 수 있습니다 . 이 플래그는 사용자가 ONNX에서 등록 및 지원되지 않는 ATen 및 비 ATen 연산자를 내보내려고 할 때 유용합니다.</target>
        </trans-unit>
        <trans-unit id="ae5fccd8dcd8fc317f8edfc8259af86cd2967a29" translate="yes" xml:space="preserve">
          <source>Expressions</source>
          <target state="translated">Expressions</target>
        </trans-unit>
        <trans-unit id="44bdb40abdeed26ffb35b097c6be620648eb56bc" translate="yes" xml:space="preserve">
          <source>Extending PyTorch</source>
          <target state="translated">PyTorch 확장</target>
        </trans-unit>
        <trans-unit id="a505bba828df658cd19ca21d48f6b057b88e1432" translate="yes" xml:space="preserve">
          <source>Extra care needs to be taken when backward through &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; outputs. Such operation is really only stable when &lt;code&gt;input&lt;/code&gt; is full rank with all distinct singular values. Otherwise, &lt;code&gt;NaN&lt;/code&gt; can appear as the gradients are not properly defined. Also, notice that double backward will usually do an additional backward through &lt;code&gt;U&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; even if the original backward is only on &lt;code&gt;S&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;U&lt;/code&gt; 및 &lt;code&gt;V&lt;/code&gt; 출력을 통해 후진 할 때 각별한주의가 필요합니다 . 이러한 작업은 &lt;code&gt;input&lt;/code&gt; 이 모든 고유 한 특이 값으로 전체 순위 일 때만 안정적 입니다. 그렇지 않으면 그라디언트가 제대로 정의되지 않아 &lt;code&gt;NaN&lt;/code&gt; 이 나타날 수 있습니다. 또한 이중 뒤로는 원래 뒤로가 &lt;code&gt;S&lt;/code&gt; 에만있는 경우에도 일반적으로 &lt;code&gt;U&lt;/code&gt; 및 &lt;code&gt;V&lt;/code&gt; 를 통해 추가 뒤로 작업을 수행합니다 .</target>
        </trans-unit>
        <trans-unit id="7869f96c8dcb40e05a18db44c02551a0604c8dd8" translate="yes" xml:space="preserve">
          <source>Extra care needs to be taken when backward through outputs. Such operation is really only stable when all eigenvalues are distinct. Otherwise, &lt;code&gt;NaN&lt;/code&gt; can appear as the gradients are not properly defined.</source>
          <target state="translated">출력을 뒤로 할 때 추가주의가 필요합니다. 이러한 연산은 모든 고유 값이 구별 될 때만 실제로 안정적입니다. 그렇지 않으면 그라디언트가 제대로 정의되지 않아 &lt;code&gt;NaN&lt;/code&gt; 이 나타날 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="35e4d02caad3850761be7f0a939f52dc8e6d05c6" translate="yes" xml:space="preserve">
          <source>Extracts sliding local blocks from a batched input tensor.</source>
          <target state="translated">일괄 입력 텐서에서 슬라이딩 로컬 블록을 추출합니다.</target>
        </trans-unit>
        <trans-unit id="545d81b806ede14bef050d6bbe02b72b7b5279d1" translate="yes" xml:space="preserve">
          <source>Extracts sliding local blocks from an batched input tensor.</source>
          <target state="translated">일괄 입력 텐서에서 슬라이딩 로컬 블록을 추출합니다.</target>
        </trans-unit>
        <trans-unit id="d1636ed5d55d52fc51dab6a56c1fdb216178f4c5" translate="yes" xml:space="preserve">
          <source>FAST mode algorithm</source>
          <target state="translated">FAST 모드 알고리즘</target>
        </trans-unit>
        <trans-unit id="5d8a2052196e0929b1ddeff4f9fa793509ac8f6c" translate="yes" xml:space="preserve">
          <source>FCN ResNet101</source>
          <target state="translated">FCN ResNet101</target>
        </trans-unit>
        <trans-unit id="d876bcc45100c189667e02c674ab45db60de2d8b" translate="yes" xml:space="preserve">
          <source>FCN ResNet50</source>
          <target state="translated">FCN ResNet50</target>
        </trans-unit>
        <trans-unit id="70deee53be1d417368b869145a93da9f61814dda" translate="yes" xml:space="preserve">
          <source>FCN ResNet50, ResNet101</source>
          <target state="translated">FCN ResNet50, ResNet101</target>
        </trans-unit>
        <trans-unit id="58296524e883e134fa6cc4840027902c76d7e637" translate="yes" xml:space="preserve">
          <source>Factory functions now take a new &lt;code&gt;names&lt;/code&gt; argument that associates a name with each dimension.</source>
          <target state="translated">이제 팩토리 함수 는 이름을 각 차원과 연결하는 새 &lt;code&gt;names&lt;/code&gt; 인수를받습니다.</target>
        </trans-unit>
        <trans-unit id="97cdbdc7feff827efb082a6b6dd2727237cd49fd" translate="yes" xml:space="preserve">
          <source>False</source>
          <target state="translated">False</target>
        </trans-unit>
        <trans-unit id="6bca42e6cb3531d60196488b0aafe02849dfad8a" translate="yes" xml:space="preserve">
          <source>False if the compiler is (likely) ABI-incompatible with PyTorch, else True.</source>
          <target state="translated">컴파일러가 PyTorch와 ABI와 호환되지 않을 가능성이있는 경우 False, 그렇지 않으면 True입니다.</target>
        </trans-unit>
        <trans-unit id="f66134050db6d6f1871f0a392769d7d300bffc78" translate="yes" xml:space="preserve">
          <source>Faster R-CNN</source>
          <target state="translated">더 빠른 R-CNN</target>
        </trans-unit>
        <trans-unit id="19fa372bf4511d5897a6678506a36d53384e83c1" translate="yes" xml:space="preserve">
          <source>Faster R-CNN ResNet-50 FPN</source>
          <target state="translated">더 빠른 R-CNN ResNet-50 FPN</target>
        </trans-unit>
        <trans-unit id="ceaa939a1707b8201f9f233e5c8d2c8a11872247" translate="yes" xml:space="preserve">
          <source>Faster R-CNN is exportable to ONNX for a fixed batch size with inputs images of fixed size.</source>
          <target state="translated">더 빠른 R-CNN은 고정 된 크기의 입력 이미지를 사용하여 고정 된 배치 크기로 ONNX로 내보낼 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b9a92e1a2a80529a37624caec86105f3856f6680" translate="yes" xml:space="preserve">
          <source>FeatureDropout (training mode not supported)</source>
          <target state="translated">FeatureDropout (트레이닝 모드는 지원되지 않음)</target>
        </trans-unit>
        <trans-unit id="023ddfe2580672ca0eeb5f867eac27e114575b07" translate="yes" xml:space="preserve">
          <source>Features described in this documentation are classified by release status:</source>
          <target state="translated">이 문서에 설명 된 기능은 릴리스 상태별로 분류됩니다.</target>
        </trans-unit>
        <trans-unit id="4ed223cb662e6eadab1b5774d593969ec64456ed" translate="yes" xml:space="preserve">
          <source>Features for large-scale deployments</source>
          <target state="translated">대규모 배포를위한 기능</target>
        </trans-unit>
        <trans-unit id="29fd83b7db12e4d9231f579c15fdaeb5d71086e4" translate="yes" xml:space="preserve">
          <source>Fill the main diagonal of a tensor that has at least 2-dimensions. When dims&amp;gt;2, all dimensions of input must be of equal length. This function modifies the input tensor in-place, and returns the input tensor.</source>
          <target state="translated">최소 2 차원이있는 텐서의 주 대각선을 채 웁니다. dims&amp;gt; 2 인 경우 입력의 모든 치수는 길이가 같아야합니다. 이 함수는 입력 텐서를 제자리에서 수정하고 입력 텐서를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="46412e89386beda08d3ce994cbdd41f7b9cb6e05" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with elements drawn from the exponential distribution:</source>
          <target state="translated">지수 분포에서 가져온 요소로 &lt;code&gt;self&lt;/code&gt; 텐서를 채 웁니다 .</target>
        </trans-unit>
        <trans-unit id="1aea402a861ce53378696f47d930c7de8244acde" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with elements drawn from the geometric distribution:</source>
          <target state="translated">기하학적 분포에서 가져온 요소로 &lt;code&gt;self&lt;/code&gt; 텐서를 채 웁니다 .</target>
        </trans-unit>
        <trans-unit id="1abd5eabced3b954f0b9c8f459ed264742cdc1be" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with elements samples from the normal distribution parameterized by &lt;a href=&quot;generated/torch.mean#torch.mean&quot;&gt;&lt;code&gt;mean&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;generated/torch.std#torch.std&quot;&gt;&lt;code&gt;std&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">&lt;a href=&quot;generated/torch.mean#torch.mean&quot;&gt; &lt;code&gt;mean&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;generated/torch.std#torch.std&quot;&gt; &lt;code&gt;std&lt;/code&gt; 로 모수화&lt;/a&gt; 된 정규 분포의 요소 샘플로 &lt;code&gt;self&lt;/code&gt; 텐서를 채 웁니다 .</target>
        </trans-unit>
        <trans-unit id="be7637b77a168dc9781dec5a6963103f27f1e666" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with numbers sampled from the continuous uniform distribution:</source>
          <target state="translated">연속 균일 분포에서 샘플링 된 숫자로 &lt;code&gt;self&lt;/code&gt; 텐서를 채 웁니다 .</target>
        </trans-unit>
        <trans-unit id="963887f4ec9debd27ff138179ec127b4ceb1a324" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with numbers sampled from the discrete uniform distribution over &lt;code&gt;[from, to - 1]&lt;/code&gt;. If not specified, the values are usually only bounded by &lt;code&gt;self&lt;/code&gt; tensor&amp;rsquo;s data type. However, for floating point types, if unspecified, range will be &lt;code&gt;[0, 2^mantissa]&lt;/code&gt; to ensure that every value is representable. For example, &lt;code&gt;torch.tensor(1, dtype=torch.double).random_()&lt;/code&gt; will be uniform in &lt;code&gt;[0, 2^53]&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 텐서를 &lt;code&gt;[from, to - 1]&lt;/code&gt; 걸쳐 이산 균등 분포에서 샘플링 한 숫자로 채 웁니다 . 지정하지 않으면 값은 일반적으로 &lt;code&gt;self&lt;/code&gt; 텐서의 데이터 유형에 의해서만 제한됩니다 . 그러나 부동 소수점 유형의 경우 지정되지 않은 경우 범위는 모든 값을 표현할 수 있도록 &lt;code&gt;[0, 2^mantissa]&lt;/code&gt; 됩니다. 예를 들어 &lt;code&gt;torch.tensor(1, dtype=torch.double).random_()&lt;/code&gt; 은 &lt;code&gt;[0, 2^53]&lt;/code&gt; 에서 균일 합니다.</target>
        </trans-unit>
        <trans-unit id="b7f10ef5f693feaeaa010e2aa027c2e16d3faf27" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with numbers samples from the log-normal distribution parameterized by the given mean</source>
          <target state="translated">주어진 평균으로 모수화 된 로그 정규 분포의 숫자 샘플로 &lt;code&gt;self&lt;/code&gt; 텐서를 채 웁니다.</target>
        </trans-unit>
        <trans-unit id="a537a70caec95c45887848e05dbf76327e57f2bf" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with the specified value.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 텐서를 지정된 값으로 채 웁니다 .</target>
        </trans-unit>
        <trans-unit id="84e59f1fa6c91fd2336c166329083335c07742c5" translate="yes" xml:space="preserve">
          <source>Fills &lt;code&gt;self&lt;/code&gt; tensor with zeros.</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 텐서를 0으로 채 웁니다 .</target>
        </trans-unit>
        <trans-unit id="c357bdf1d8cc36baaa85a26ceaf45fb123516806" translate="yes" xml:space="preserve">
          <source>Fills each location of &lt;code&gt;self&lt;/code&gt; with an independent sample from</source>
          <target state="translated">&lt;code&gt;self&lt;/code&gt; 각 위치 를 다음의 독립적 인 샘플로 채 웁니다.</target>
        </trans-unit>
        <trans-unit id="75c253cce2f7953023782f50f2d324ceb99f06ec" translate="yes" xml:space="preserve">
          <source>Fills elements of &lt;code&gt;self&lt;/code&gt; tensor with &lt;code&gt;value&lt;/code&gt; where &lt;code&gt;mask&lt;/code&gt; is True. The shape of &lt;code&gt;mask&lt;/code&gt; must be &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;broadcastable&lt;/a&gt; with the shape of the underlying tensor.</source>
          <target state="translated">의 요소를 채 웁니다 &lt;code&gt;self&lt;/code&gt; 와 텐서를 &lt;code&gt;value&lt;/code&gt; &lt;code&gt;mask&lt;/code&gt; True입니다. 의 모양 &lt;code&gt;mask&lt;/code&gt; 해야 &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/broadcasting.html#broadcasting-semantics&quot;&gt;캐스트 가능한&lt;/a&gt; 기본 텐서의 모양.</target>
        </trans-unit>
        <trans-unit id="7f2c547e676c650b0294e152d6742725981b5e7b" translate="yes" xml:space="preserve">
          <source>Fills the 2-dimensional input &lt;code&gt;Tensor&lt;/code&gt; with the identity matrix. Preserves the identity of the inputs in &lt;code&gt;Linear&lt;/code&gt; layers, where as many inputs are preserved as possible.</source>
          <target state="translated">2 차원 입력 &lt;code&gt;Tensor&lt;/code&gt; 를 단위 행렬로 채 웁니다 . 가능한 한 많은 입력이 보존되는 &lt;code&gt;Linear&lt;/code&gt; 레이어 의 입력 ID를 유지합니다.</target>
        </trans-unit>
        <trans-unit id="5e10a9506b95f83d62c2104cb07cd55facde3766" translate="yes" xml:space="preserve">
          <source>Fills the 2D input &lt;code&gt;Tensor&lt;/code&gt; as a sparse matrix, where the non-zero elements will be drawn from the normal distribution</source>
          <target state="translated">2D 입력 &lt;code&gt;Tensor&lt;/code&gt; 를 희소 행렬로 채 웁니다. 여기서 0이 아닌 요소는 정규 분포에서 그려집니다.</target>
        </trans-unit>
        <trans-unit id="5e66c6e2e841f8a8523fead84aa32e52990c1b4b" translate="yes" xml:space="preserve">
          <source>Fills the elements of the &lt;code&gt;self&lt;/code&gt; tensor with value &lt;code&gt;val&lt;/code&gt; by selecting the indices in the order given in &lt;code&gt;index&lt;/code&gt;.</source>
          <target state="translated">의 요소 채운다 &lt;code&gt;self&lt;/code&gt; 값과 텐서 &lt;code&gt;val&lt;/code&gt; 주어진 순서대로 인덱스를 선택하여 &lt;code&gt;index&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="3a21a74e5af8bd75efccfeb2dc749fe2483f779d" translate="yes" xml:space="preserve">
          <source>Fills the input &lt;code&gt;Tensor&lt;/code&gt; with a (semi) orthogonal matrix, as described in &lt;code&gt;Exact solutions to the nonlinear dynamics of learning in deep linear neural networks&lt;/code&gt; - Saxe, A. et al. (2013). The input tensor must have at least 2 dimensions, and for tensors with more than 2 dimensions the trailing dimensions are flattened.</source>
          <target state="translated">입력 채운다 &lt;code&gt;Tensor&lt;/code&gt; 에 설명 된대로 (세미) 직교 행렬 &lt;code&gt;Exact solutions to the nonlinear dynamics of learning in deep linear neural networks&lt;/code&gt; 색스, A. 외 알 -. (2013). 입력 텐서는 2 차원 이상을 가져야하며 2 차원을 초과하는 텐서의 경우 후행 차원이 평면화됩니다.</target>
        </trans-unit>
        <trans-unit id="06242c8039ea176e183491ddfbaaf7c2d66c68e8" translate="yes" xml:space="preserve">
          <source>Fills the input &lt;code&gt;Tensor&lt;/code&gt; with values according to the method described in &lt;code&gt;Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification&lt;/code&gt; - He, K. et al. (2015), using a normal distribution. The resulting tensor will have values sampled from</source>
          <target state="translated">&lt;code&gt;Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification&lt;/code&gt; -He, K. et al.에 설명 된 방법에 따라 입력 &lt;code&gt;Tensor&lt;/code&gt; 를 값으로 채 웁니다 . (2015), 정규 분포를 사용합니다. 결과 텐서는 다음에서 샘플링 된 값을 갖습니다.</target>
        </trans-unit>
        <trans-unit id="342727420e0d4e4b77efe66611c6eb6db5be0acf" translate="yes" xml:space="preserve">
          <source>Fills the input &lt;code&gt;Tensor&lt;/code&gt; with values according to the method described in &lt;code&gt;Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification&lt;/code&gt; - He, K. et al. (2015), using a uniform distribution. The resulting tensor will have values sampled from</source>
          <target state="translated">&lt;code&gt;Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification&lt;/code&gt; -He, K. et al.에 설명 된 방법에 따라 입력 &lt;code&gt;Tensor&lt;/code&gt; 를 값으로 채 웁니다 . (2015), 균등 분포를 사용합니다. 결과 텐서는 다음에서 샘플링 된 값을 갖습니다.</target>
        </trans-unit>
        <trans-unit id="71e3100123b6866d1997e6df9daeefd940ad80de" translate="yes" xml:space="preserve">
          <source>Fills the input &lt;code&gt;Tensor&lt;/code&gt; with values according to the method described in &lt;code&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/code&gt; - Glorot, X. &amp;amp; Bengio, Y. (2010), using a normal distribution. The resulting tensor will have values sampled from</source>
          <target state="translated">정규 분포를 사용하여 &lt;code&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/code&gt; -Glorot, X. &amp;amp; Bengio, Y. (2010)에 설명 된 방법에 따라 입력 &lt;code&gt;Tensor&lt;/code&gt; 에 값을 채 웁니다 . 결과 텐서는 다음에서 샘플링 된 값을 갖습니다.</target>
        </trans-unit>
        <trans-unit id="2313477ebbbc49bb7f4bf7d5c39c5ea0206267ac" translate="yes" xml:space="preserve">
          <source>Fills the input &lt;code&gt;Tensor&lt;/code&gt; with values according to the method described in &lt;code&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/code&gt; - Glorot, X. &amp;amp; Bengio, Y. (2010), using a uniform distribution. The resulting tensor will have values sampled from</source>
          <target state="translated">균일 분포를 사용하여 &lt;code&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/code&gt; -Glorot, X. &amp;amp; Bengio, Y. (2010)에 설명 된 방법에 따라 입력 &lt;code&gt;Tensor&lt;/code&gt; 에 값을 채 웁니다 . 결과 텐서는 다음에서 샘플링 된 값을 갖습니다.</target>
        </trans-unit>
        <trans-unit id="afacae29047abc8bbea1f5a18cb9f7afddce8004" translate="yes" xml:space="preserve">
          <source>Fills the input Tensor with the scalar value &lt;code&gt;0&lt;/code&gt;.</source>
          <target state="translated">입력 Tensor를 스칼라 값 &lt;code&gt;0&lt;/code&gt; 으로 채 웁니다 .</target>
        </trans-unit>
        <trans-unit id="0749fb4668d1e4b76f3a1ba9aa423e792bc38d58" translate="yes" xml:space="preserve">
          <source>Fills the input Tensor with the scalar value &lt;code&gt;1&lt;/code&gt;.</source>
          <target state="translated">입력 Tensor를 스칼라 값 &lt;code&gt;1&lt;/code&gt; 로 채 웁니다 .</target>
        </trans-unit>
        <trans-unit id="498455641766cf82c112342c938821f2f05dedf1" translate="yes" xml:space="preserve">
          <source>Fills the input Tensor with the value</source>
          <target state="translated">입력 Tensor를 값으로 채 웁니다.</target>
        </trans-unit>
        <trans-unit id="92513a6a0cb417782fe4aae045c4a1b66c923b2f" translate="yes" xml:space="preserve">
          <source>Fills the input Tensor with values drawn from the normal distribution</source>
          <target state="translated">입력 Tensor를 정규 분포에서 가져온 값으로 채 웁니다.</target>
        </trans-unit>
        <trans-unit id="6b3d75035cf3e14fc34e56b36e9d5379c6dd95d1" translate="yes" xml:space="preserve">
          <source>Fills the input Tensor with values drawn from the uniform distribution</source>
          <target state="translated">입력 Tensor를 균일 분포에서 가져온 값으로 채 웁니다.</target>
        </trans-unit>
        <trans-unit id="53430dbd9b3ec1b5ed7bd9a57a201a7f2d1a60f2" translate="yes" xml:space="preserve">
          <source>Fills the tensor with numbers drawn from the Cauchy distribution:</source>
          <target state="translated">코시 분포에서 추출한 숫자로 텐서를 채 웁니다.</target>
        </trans-unit>
        <trans-unit id="c6632aebf9bc7adafe6c2b8a271e6b35dc1107ed" translate="yes" xml:space="preserve">
          <source>Fills the {3, 4, 5}-dimensional input &lt;code&gt;Tensor&lt;/code&gt; with the Dirac delta function. Preserves the identity of the inputs in &lt;code&gt;Convolutional&lt;/code&gt; layers, where as many input channels are preserved as possible. In case of groups&amp;gt;1, each group of channels preserves identity</source>
          <target state="translated">{3, 4, 5} 차원 입력 &lt;code&gt;Tensor&lt;/code&gt; 를 Dirac 델타 함수로 채 웁니다 . 가능한 한 많은 입력 채널이 보존되는 &lt;code&gt;Convolutional&lt;/code&gt; 레이어 의 입력 ID를 유지합니다. 그룹&amp;gt; 1의 경우 각 채널 그룹은 ID를 유지합니다.</target>
        </trans-unit>
        <trans-unit id="695c574fe765006dd0a54d1a5cdae48d767984bd" translate="yes" xml:space="preserve">
          <source>Find the indices from the &lt;em&gt;innermost&lt;/em&gt; dimension of &lt;code&gt;sorted_sequence&lt;/code&gt; such that, if the corresponding values in &lt;code&gt;values&lt;/code&gt; were inserted before the indices, the order of the corresponding &lt;em&gt;innermost&lt;/em&gt; dimension within &lt;code&gt;sorted_sequence&lt;/code&gt; would be preserved.</source>
          <target state="translated">로부터 인덱스 찾기 &lt;em&gt;최&lt;/em&gt; 의 치수 &lt;code&gt;sorted_sequence&lt;/code&gt; 에서 해당 값 경우 그에 따라, &lt;code&gt;values&lt;/code&gt; 인덱스 앞에 삽입하고, 해당 순서 &lt;em&gt;최&lt;/em&gt; 내의 치수 &lt;code&gt;sorted_sequence&lt;/code&gt; 이 보존 될있다.</target>
        </trans-unit>
        <trans-unit id="bc20e9bbc443d3b0fd0b14b6ff920327eb09f1d2" translate="yes" xml:space="preserve">
          <source>Find the indices from the &lt;em&gt;innermost&lt;/em&gt; dimension of &lt;code&gt;sorted_sequence&lt;/code&gt; such that, if the corresponding values in &lt;code&gt;values&lt;/code&gt; were inserted before the indices, the order of the corresponding &lt;em&gt;innermost&lt;/em&gt; dimension within &lt;code&gt;sorted_sequence&lt;/code&gt; would be preserved. Return a new tensor with the same size as &lt;code&gt;values&lt;/code&gt;. If &lt;code&gt;right&lt;/code&gt; is False (default), then the left boundary of &lt;code&gt;sorted_sequence&lt;/code&gt; is closed. More formally, the returned index satisfies the following rules:</source>
          <target state="translated">로부터 인덱스 찾기 &lt;em&gt;최&lt;/em&gt; 의 치수 &lt;code&gt;sorted_sequence&lt;/code&gt; 에서 해당 값 경우 그에 따라, &lt;code&gt;values&lt;/code&gt; 인덱스 앞에 삽입하고, 해당 순서 &lt;em&gt;최&lt;/em&gt; 내의 치수 &lt;code&gt;sorted_sequence&lt;/code&gt; 이 보존 될있다. &lt;code&gt;values&lt;/code&gt; 와 같은 크기의 새 텐서를 반환 합니다 . 경우 &lt;code&gt;right&lt;/code&gt; (기본값) False입니다, 다음의 왼쪽 경계 &lt;code&gt;sorted_sequence&lt;/code&gt; 이 닫힙니다. 보다 공식적으로 반환 된 인덱스는 다음 규칙을 충족합니다.</target>
        </trans-unit>
        <trans-unit id="03215b19f0bffce926c83716ff9399ab447bdf55" translate="yes" xml:space="preserve">
          <source>Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods.</source>
          <target state="translated">행렬이없는 LOBPCG 방법을 사용하여 양의 대칭으로 정의 된 일반 고유 값 문제의 k 개의 가장 큰 (또는 가장 작은) 고유 값과 해당하는 고유 벡터를 찾습니다.</target>
        </trans-unit>
        <trans-unit id="d6ca52cc281a8bbbaa3d9b53b49079e29eb878fa" translate="yes" xml:space="preserve">
          <source>First convert your model from GPU to CPU and then save it, like so:</source>
          <target state="translated">먼저 모델을 GPU에서 CPU로 변환 한 다음 다음과 같이 저장합니다.</target>
        </trans-unit>
        <trans-unit id="56b7d451e08bf1ccd9aa8267b497203df559b032" translate="yes" xml:space="preserve">
          <source>First, if you repeatedly perform an operation that can produce duplicate entries (e.g., &lt;a href=&quot;#torch.sparse.FloatTensor.add&quot;&gt;&lt;code&gt;torch.sparse.FloatTensor.add()&lt;/code&gt;&lt;/a&gt;), you should occasionally coalesce your sparse tensors to prevent them from growing too large.</source>
          <target state="translated">첫째, 중복 항목을 생성 할 수있는 작업 (예 : &lt;a href=&quot;#torch.sparse.FloatTensor.add&quot;&gt; &lt;code&gt;torch.sparse.FloatTensor.add()&lt;/code&gt; &lt;/a&gt; ) 을 반복적으로 수행하는 경우 가끔씩 희소 텐서를 병합하여 너무 커지지 않도록해야합니다.</target>
        </trans-unit>
        <trans-unit id="0af14ddb20aabbe1bd98f28f2c2f744284ccedb3" translate="yes" xml:space="preserve">
          <source>Flatten</source>
          <target state="translated">Flatten</target>
        </trans-unit>
        <trans-unit id="557473442912b0bbbc1f4c6573c0fe9837b23ef2" translate="yes" xml:space="preserve">
          <source>Flattens &lt;code&gt;dims&lt;/code&gt; into a single dimension with name &lt;code&gt;out_dim&lt;/code&gt;.</source>
          <target state="translated">&lt;code&gt;out_dim&lt;/code&gt; 이라는 이름으로 단일 차원으로 평평 &lt;code&gt;dims&lt;/code&gt; 합니다 .</target>
        </trans-unit>
        <trans-unit id="80a931c216b5d2a192745812d7ca953d11d59b70" translate="yes" xml:space="preserve">
          <source>Flattens a contiguous range of dims in a tensor.</source>
          <target state="translated">텐서에서 연속 된 범위의 희미 함을 평평하게합니다.</target>
        </trans-unit>
        <trans-unit id="e2ed5b97e25777e5a578840b676a3b9e44b41cab" translate="yes" xml:space="preserve">
          <source>Flattens a contiguous range of dims into a tensor.</source>
          <target state="translated">연속 된 범위의 Dim을 텐서로 평평하게 만듭니다.</target>
        </trans-unit>
        <trans-unit id="bcfce6cdf44c8905f6faca75da235eedd5635aac" translate="yes" xml:space="preserve">
          <source>Flattens a contiguous range of dims into a tensor. For use with &lt;code&gt;Sequential&lt;/code&gt;.</source>
          <target state="translated">연속 된 범위의 Dim을 텐서로 평평하게 만듭니다. &lt;code&gt;Sequential&lt;/code&gt; 과 함께 사용 합니다.</target>
        </trans-unit>
        <trans-unit id="db0a60d5a36fb0f5467808a539ac8c0b01a8dadd" translate="yes" xml:space="preserve">
          <source>Flip array in the left/right direction, returning a new tensor.</source>
          <target state="translated">새 텐서를 반환하는 왼쪽 / 오른쪽 방향으로 배열을 뒤집습니다.</target>
        </trans-unit>
        <trans-unit id="e9abeaf079a0d69d0b2ee4704e055dbacab017f3" translate="yes" xml:space="preserve">
          <source>Flip array in the up/down direction, returning a new tensor.</source>
          <target state="translated">배열을 위 / 아래 방향으로 뒤집어 새로운 텐서를 반환합니다.</target>
        </trans-unit>
        <trans-unit id="d6b601389a5cfb7f115ef7332083ae3431e4e4e3" translate="yes" xml:space="preserve">
          <source>Flip the entries in each column in the up/down direction. Rows are preserved, but appear in a different order than before.</source>
          <target state="translated">각 열의 항목을 위 / 아래 방향으로 뒤집습니다. 행은 유지되지만 이전과 다른 순서로 나타납니다.</target>
        </trans-unit>
        <trans-unit id="f14bb6de935d5a8cea7f1dc6e4323a38f4e11462" translate="yes" xml:space="preserve">
          <source>Flip the entries in each row in the left/right direction. Columns are preserved, but appear in a different order than before.</source>
          <target state="translated">각 행의 항목을 왼쪽 / 오른쪽 방향으로 뒤집습니다. 열은 유지되지만 이전과 다른 순서로 나타납니다.</target>
        </trans-unit>
        <trans-unit id="23c9f78a2bf71d761dc5a430092962a70ec045f2" translate="yes" xml:space="preserve">
          <source>FloatFunctional</source>
          <target state="translated">FloatFunctional</target>
        </trans-unit>
        <trans-unit id="0352ecfbeaceff6ef8214a3af42e602c1d2f0c6a" translate="yes" xml:space="preserve">
          <source>Flushes the event file to disk. Call this method to make sure that all pending events have been written to disk.</source>
          <target state="translated">이벤트 파일을 디스크로 플러시합니다. 이 메서드를 호출하여 보류중인 모든 이벤트가 디스크에 기록되었는지 확인합니다.</target>
        </trans-unit>
        <trans-unit id="b6ba0db1f814179114ec82fbf188b2eb5be2596e" translate="yes" xml:space="preserve">
          <source>Fold</source>
          <target state="translated">Fold</target>
        </trans-unit>
        <trans-unit id="ab2123970899470af7c3bcdbf9832cd5ea8344b1" translate="yes" xml:space="preserve">
          <source>Following this tutorial &lt;a href=&quot;https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html&quot;&gt;Extending TorchScript with Custom C++ Operators&lt;/a&gt;, you can create and register your own custom ops implementation in PyTorch. Here&amp;rsquo;s how to export such model to ONNX.:</source>
          <target state="translated">이 튜토리얼에 따라 &lt;a href=&quot;https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html&quot;&gt;사용자 지정 C ++ 연산자를 사용하여 TorchScript 확장, PyTorch에서 사용자&lt;/a&gt; 지정 작업 구현을 만들고 등록 할 수 있습니다. 이러한 모델을 ONNX로 내보내는 방법은 다음과 같습니다. :</target>
        </trans-unit>
        <trans-unit id="c762389bca1bb2b05603a57f1353b6b58529cac2" translate="yes" xml:space="preserve">
          <source>For &lt;a href=&quot;futures#torch.futures.Future&quot;&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; objects returned by &lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt;&lt;code&gt;rpc_async()&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;future.wait()&lt;/code&gt; should not be called after &lt;code&gt;shutdown()&lt;/code&gt;.</source>
          <target state="translated">들어 &lt;a href=&quot;futures#torch.futures.Future&quot;&gt; &lt;code&gt;Future&lt;/code&gt; &lt;/a&gt; 에 의해 반환 된 객체 &lt;a href=&quot;#torch.distributed.rpc.rpc_async&quot;&gt; &lt;code&gt;rpc_async()&lt;/code&gt; &lt;/a&gt; , &lt;code&gt;future.wait()&lt;/code&gt; 이후에 호출 할 수 없습니다 &lt;code&gt;shutdown()&lt;/code&gt; .</target>
        </trans-unit>
        <trans-unit id="068880daf2a8a0b7a7463ef0178819889346c85b" translate="yes" xml:space="preserve">
          <source>For &lt;code&gt;N&lt;/code&gt;-dimensional padding, use &lt;a href=&quot;../nn.functional#torch.nn.functional.pad&quot;&gt;&lt;code&gt;torch.nn.functional.pad()&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">들면 &lt;code&gt;N&lt;/code&gt; 차원 패딩 사용 &lt;a href=&quot;../nn.functional#torch.nn.functional.pad&quot;&gt; &lt;code&gt;torch.nn.functional.pad()&lt;/code&gt; &lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="0224a530bfe0946c7afd8a4140361f1cd0dcd86b" translate="yes" xml:space="preserve">
          <source>For &lt;code&gt;torch.nn.functional&lt;/code&gt; operators, we support the following:</source>
          <target state="translated">들어 &lt;code&gt;torch.nn.functional&lt;/code&gt; 사업자, 우리는 다음을 지원합니다 :</target>
        </trans-unit>
        <trans-unit id="e1e6b2fc17a48e201a5b7dbea3ac88cd7be57af5" translate="yes" xml:space="preserve">
          <source>For CPU tensors, this method is currently only available with MKL. Use &lt;a href=&quot;../backends#torch.backends.mkl.is_available&quot;&gt;&lt;code&gt;torch.backends.mkl.is_available()&lt;/code&gt;&lt;/a&gt; to check if MKL is installed.</source>
          <target state="translated">CPU 텐서의 경우이 방법은 현재 MKL에서만 사용할 수 있습니다. &lt;a href=&quot;../backends#torch.backends.mkl.is_available&quot;&gt; &lt;code&gt;torch.backends.mkl.is_available()&lt;/code&gt; &lt;/a&gt; 을 사용 하여 MKL이 설치되어 있는지 확인하십시오.</target>
        </trans-unit>
        <trans-unit id="5a9dc733d94431c6a4d80dbab50dc02413b91172" translate="yes" xml:space="preserve">
          <source>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same configuration. See &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cufft-plan-cache&quot;&gt;cuFFT plan cache&lt;/a&gt; for more details on how to monitor and control the cache.</source>
          <target state="translated">CUDA 텐서의 경우, 동일한 구성을 가진 동일한 지오메트리의 텐서에서 FFT 메서드를 반복적으로 실행하는 속도를 높이기 위해 cuFFT 계획에 LRU 캐시가 사용됩니다. &lt;a href=&quot;https://pytorch.org/docs/1.7.0/notes/cuda.html#cufft-plan-cache&quot;&gt;캐시&lt;/a&gt; 를 모니터링하고 제어하는 ​​방법에 대한 자세한 내용 은 cuFFT 계획 캐시 를 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="c3f7242e16cf3e0469ec63da054b257bd101a205" translate="yes" xml:space="preserve">
          <source>For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides. For CPU tensors, an error is thrown.</source>
          <target state="translated">CUDA 텐서의 경우이 함수는 텐서가있는 GPU의 장치 서수를 반환합니다. CPU 텐서의 경우 오류가 발생합니다.</target>
        </trans-unit>
        <trans-unit id="ce3ce048e3708a40dbc3057fc7dbd0788b47316f" translate="yes" xml:space="preserve">
          <source>For Tensors that have &lt;a href=&quot;autograd#torch.Tensor.requires_grad&quot;&gt;&lt;code&gt;requires_grad&lt;/code&gt;&lt;/a&gt; which is &lt;code&gt;True&lt;/code&gt;, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so &lt;code&gt;grad_fn&lt;/code&gt; is None.</source>
          <target state="translated">이 텐서를 들어 &lt;a href=&quot;autograd#torch.Tensor.requires_grad&quot;&gt; &lt;code&gt;requires_grad&lt;/code&gt; &lt;/a&gt; 인 &lt;code&gt;True&lt;/code&gt; 그들이 사용자에 의해 생성 된 경우, 그들은 잎 텐서 될 것입니다. 즉, 작업의 결과가 아니므로 &lt;code&gt;grad_fn&lt;/code&gt; 은 None입니다.</target>
        </trans-unit>
        <trans-unit id="8d2d3af0bd6ef80615c67c0f72bb823e693063c0" translate="yes" xml:space="preserve">
          <source>For a 3-D tensor the output is specified by:</source>
          <target state="translated">3 차원 텐서의 경우 출력은 다음과 같이 지정됩니다.</target>
        </trans-unit>
        <trans-unit id="3f570fa1da407a7894b9ad9cdc01e54242454c59" translate="yes" xml:space="preserve">
          <source>For a 3-D tensor, &lt;code&gt;self&lt;/code&gt; is updated as:</source>
          <target state="translated">3 차원 텐서의 경우 &lt;code&gt;self&lt;/code&gt; 는 다음과 같이 업데이트됩니다.</target>
        </trans-unit>
        <trans-unit id="dd4d8e8f7d7355e68f887ffe9f112d1d44ab2731" translate="yes" xml:space="preserve">
          <source>For a comprehensive list of name inference rules, see &lt;a href=&quot;name_inference#name-inference-reference-doc&quot;&gt;Named Tensors operator coverage&lt;/a&gt;. Here are two common operations that may be useful to go over:</source>
          <target state="translated">이름 유추 규칙의 전체 목록은 &lt;a href=&quot;name_inference#name-inference-reference-doc&quot;&gt;명명 된 텐서 연산자 적용 범위를&lt;/a&gt; 참조하십시오 . 살펴보기에 유용한 두 가지 일반적인 작업은 다음과 같습니다.</target>
        </trans-unit>
        <trans-unit id="f91f8919582617014f59317b7352c3f1154712ab" translate="yes" xml:space="preserve">
          <source>For a full listing of supported Python features, see &lt;a href=&quot;jit_python_reference#python-language-reference&quot;&gt;Python Language Reference Coverage&lt;/a&gt;.</source>
          <target state="translated">지원되는 Python 기능의 전체 목록은 &lt;a href=&quot;jit_python_reference#python-language-reference&quot;&gt;Python 언어 참조 범위를 참조&lt;/a&gt; 하십시오 .</target>
        </trans-unit>
        <trans-unit id="4cd19a97e1a392f82662e959d703403f2351a98b" translate="yes" xml:space="preserve">
          <source>For a gentle introduction to TorchScript, see the &lt;a href=&quot;https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html&quot;&gt;Introduction to TorchScript&lt;/a&gt; tutorial.</source>
          <target state="translated">TorchScript에 대한 &lt;a href=&quot;https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html&quot;&gt;간단한 소개는 TorchScript 소개&lt;/a&gt; 자습서 를 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="8e0e8c59f7042dfe9b36d63277e5e7440cb658b4" translate="yes" xml:space="preserve">
          <source>For a tensor &lt;code&gt;input&lt;/code&gt; of sizes</source>
          <target state="translated">크기 의 텐서 &lt;code&gt;input&lt;/code&gt; 의 경우</target>
        </trans-unit>
        <trans-unit id="7afccb9a65464232e6685319e5c664295be01466" translate="yes" xml:space="preserve">
          <source>For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see the &lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_export.html&quot;&gt;Loading a PyTorch Model in C++&lt;/a&gt; tutorial.</source>
          <target state="translated">PyTorch 모델을 TorchScript로 변환하고 C ++에서 실행하는 종단 간 예제는 C ++에서 &lt;a href=&quot;https://pytorch.org/tutorials/advanced/cpp_export.html&quot;&gt;PyTorch 모델로드&lt;/a&gt; 자습서를 참조하세요.</target>
        </trans-unit>
        <trans-unit id="c08a3397ebd78b324e2b03c935d901274f6914be" translate="yes" xml:space="preserve">
          <source>For bags of constant length and no &lt;code&gt;per_sample_weights&lt;/code&gt;, this class</source>
          <target state="translated">길이가 일정하고 &lt;code&gt;per_sample_weights&lt;/code&gt; 가 없는 bag 의 경우이 클래스</target>
        </trans-unit>
        <trans-unit id="ce5d81031fb4d7e97b33ce443ec1d33497cec1cd" translate="yes" xml:space="preserve">
          <source>For details on input arguments, parameters, and implementation see &lt;a href=&quot;generated/torch.nn.conv1d#torch.nn.Conv1d&quot;&gt;&lt;code&gt;Conv1d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">입력 인수, 매개 변수 및 구현에 대한 자세한 내용은 &lt;a href=&quot;generated/torch.nn.conv1d#torch.nn.Conv1d&quot;&gt; &lt;code&gt;Conv1d&lt;/code&gt; 를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="36b1210fa3359dec217e0f8625012720c3f10bc6" translate="yes" xml:space="preserve">
          <source>For details on input arguments, parameters, and implementation see &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt;&lt;code&gt;Conv2d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">입력 인수, 매개 변수 및 구현에 대한 자세한 내용은 &lt;a href=&quot;generated/torch.nn.conv2d#torch.nn.Conv2d&quot;&gt; &lt;code&gt;Conv2d&lt;/code&gt; 를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="17fc1bf0dec370ea37946985facbba42ffd22e0f" translate="yes" xml:space="preserve">
          <source>For details on input arguments, parameters, and implementation see &lt;a href=&quot;generated/torch.nn.conv3d#torch.nn.Conv3d&quot;&gt;&lt;code&gt;Conv3d&lt;/code&gt;&lt;/a&gt;.</source>
          <target state="translated">입력 인수, 매개 변수 및 구현에 대한 자세한 내용은 &lt;a href=&quot;generated/torch.nn.conv3d#torch.nn.Conv3d&quot;&gt; &lt;code&gt;Conv3d&lt;/code&gt; 를&lt;/a&gt; 참조하십시오 .</target>
        </trans-unit>
        <trans-unit id="88fa6dcc55dbef1b20fcb850f35a6375f7b3938c" translate="yes" xml:space="preserve">
          <source>For each element in the input sequence, each layer computes the following function:</source>
          <target state="translated">입력 시퀀스의 각 요소에 대해 각 계층은 다음 함수를 계산합니다.</target>
        </trans-unit>
        <trans-unit id="d139363266e05a6b552e5defd18e04652bf83d44" translate="yes" xml:space="preserve">
          <source>For each mini-batch sample, the loss in terms of the 1D input</source>
          <target state="translated">각 미니 배치 샘플에 대해 1D 입력 측면에서 손실</target>
        </trans-unit>
        <trans-unit id="431645996fbd3fae9490db55c077050efdd9469b" translate="yes" xml:space="preserve">
          <source>For each output location &lt;code&gt;output[n, :, h, w]&lt;/code&gt;, the size-2 vector &lt;code&gt;grid[n, h, w]&lt;/code&gt; specifies &lt;code&gt;input&lt;/code&gt; pixel locations &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;, which are used to interpolate the output value &lt;code&gt;output[n, :, h, w]&lt;/code&gt;. In the case of 5D inputs, &lt;code&gt;grid[n, d, h, w]&lt;/code&gt; specifies the &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;, &lt;code&gt;z&lt;/code&gt; pixel locations for interpolating &lt;code&gt;output[n, :, d, h, w]&lt;/code&gt;. &lt;code&gt;mode&lt;/code&gt; argument specifies &lt;code&gt;nearest&lt;/code&gt; or &lt;code&gt;bilinear&lt;/code&gt; interpolation method to sample the input pixels.</source>
          <target state="translated">각 출력 위치 &lt;code&gt;output[n, :, h, w]&lt;/code&gt; 에 대해 size-2 벡터 &lt;code&gt;grid[n, h, w]&lt;/code&gt; 는 출력 값 &lt;code&gt;output[n, :, h, w]&lt;/code&gt; 을 보간하는 데 사용되는 &lt;code&gt;input&lt;/code&gt; 픽셀 위치 &lt;code&gt;x&lt;/code&gt; 및 &lt;code&gt;y&lt;/code&gt; 를 지정합니다 . h, w] . 5D 입력의 경우 &lt;code&gt;grid[n, d, h, w]&lt;/code&gt; 는 &lt;code&gt;output[n, :, d, h, w]&lt;/code&gt; 보간을위한 &lt;code&gt;x&lt;/code&gt; , &lt;code&gt;y&lt;/code&gt; , &lt;code&gt;z&lt;/code&gt; 픽셀 위치를 지정합니다 . &lt;code&gt;mode&lt;/code&gt; 인수 는 입력 픽셀을 샘플링하기 위해 &lt;code&gt;nearest&lt;/code&gt; 또는 &lt;code&gt;bilinear&lt;/code&gt; 보간 방법을 지정합니다 .</target>
        </trans-unit>
        <trans-unit id="a707b704ee783d908df45582d74fb5eed5208cc2" translate="yes" xml:space="preserve">
          <source>For example, assigning to &lt;code&gt;self&lt;/code&gt; outside of the &lt;code&gt;__init__()&lt;/code&gt; method:</source>
          <target state="translated">예를 들어, &lt;code&gt;__init__()&lt;/code&gt; 메서드 외부 에서 &lt;code&gt;self&lt;/code&gt; 에 할당 :</target>
        </trans-unit>
        <trans-unit id="598cd3a9f173b17b51c024dfc8ec80175fdbbd3c" translate="yes" xml:space="preserve">
          <source>For example, if &lt;code&gt;input&lt;/code&gt; is a vector of size N, the result will also be a vector of size N, with elements.</source>
          <target state="translated">예를 들어 &lt;code&gt;input&lt;/code&gt; 이 크기가 N 인 벡터 인 경우 결과는 요소가있는 크기가 N 인 벡터도됩니다.</target>
        </trans-unit>
        <trans-unit id="c5eb0f2bf9e3391a2f0f43ac5e99b77a4de61cf1" translate="yes" xml:space="preserve">
          <source>For example, if &lt;code&gt;input&lt;/code&gt; is of shape:</source>
          <target state="translated">예를 들어 &lt;code&gt;input&lt;/code&gt; 이 형태 인 경우 :</target>
        </trans-unit>
        <trans-unit id="cb7354d4a324678f0e4b20e916940e161856f119" translate="yes" xml:space="preserve">
          <source>For example, if a dataset contains 100 positive and 300 negative examples of a single class, then &lt;code&gt;pos_weight&lt;/code&gt; for the class should be equal to</source>
          <target state="translated">예를 들어 데이터 세트에 단일 클래스의 100 개의 긍정 및 300 개의 부정적인 예제가 포함 된 경우 클래스의 &lt;code&gt;pos_weight&lt;/code&gt; 는 다음 과 같아야합니다.</target>
        </trans-unit>
        <trans-unit id="ec89e688ce49781ad9518cc67374c388cc79a9c9" translate="yes" xml:space="preserve">
          <source>For example, if the system we use for distributed training has 2 nodes, each of which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would like to all-reduce. The following code can serve as a reference:</source>
          <target state="translated">예를 들어 분산 학습에 사용하는 시스템에 2 개의 노드가 있고 각 노드에는 8 개의 GPU가 있습니다. 16 개의 GPU 각각에는 모두 축소하려는 텐서가 있습니다. 다음 코드는 참조로 사용할 수 있습니다.</target>
        </trans-unit>
        <trans-unit id="b076a6f19f5c3f95d3197664aa2092f90ac3e4fd" translate="yes" xml:space="preserve">
          <source>For example, suppose that we wanted to implement an operator by operating directly on &lt;a href=&quot;#torch.sparse.FloatTensor._values&quot;&gt;&lt;code&gt;torch.sparse.FloatTensor._values()&lt;/code&gt;&lt;/a&gt;. Multiplication by a scalar can be implemented in the obvious way, as multiplication distributes over addition; however, square root cannot be implemented directly, since &lt;code&gt;sqrt(a + b) != sqrt(a) +
sqrt(b)&lt;/code&gt; (which is what would be computed if you were given an uncoalesced tensor.)</source>
          <target state="translated">예를 들어 &lt;a href=&quot;#torch.sparse.FloatTensor._values&quot;&gt; &lt;code&gt;torch.sparse.FloatTensor._values()&lt;/code&gt; &lt;/a&gt; 에서 직접 작동하여 연산자를 구현하고 싶다고 가정 해 보겠습니다 . 스칼라에 의한 곱셈은 곱셈이 덧셈에 분산되므로 명백한 방식으로 구현 될 수 있습니다. 그러나 &lt;code&gt;sqrt(a + b) != sqrt(a) + sqrt(b)&lt;/code&gt; (통합되지 않은 텐서가 주어지면 계산되는 것입니다 ) 때문에 제곱근은 직접 구현할 수 없습니다 .</target>
        </trans-unit>
        <trans-unit id="b3dee60b13728b6faa87ab6d08e758ac130cfdfa" translate="yes" xml:space="preserve">
          <source>For inputs of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt;, &lt;code&gt;value&lt;/code&gt; must be a real number, otherwise an integer.</source>
          <target state="translated">&lt;code&gt;FloatTensor&lt;/code&gt; 또는 &lt;code&gt;DoubleTensor&lt;/code&gt; 유형의 입력의 경우 &lt;code&gt;value&lt;/code&gt; 은 실수 여야하며 그렇지 않으면 정수 여야합니다.</target>
        </trans-unit>
        <trans-unit id="a79d59fd72d10daf74904a50c1c53f2a33467eeb" translate="yes" xml:space="preserve">
          <source>For inputs of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt;, arguments &lt;code&gt;beta&lt;/code&gt; and &lt;code&gt;alpha&lt;/code&gt; must be real numbers, otherwise they should be integers</source>
          <target state="translated">&lt;code&gt;FloatTensor&lt;/code&gt; 또는 &lt;code&gt;DoubleTensor&lt;/code&gt; 유형의 입력의 경우 &lt;code&gt;beta&lt;/code&gt; 및 &lt;code&gt;alpha&lt;/code&gt; 인수 는 실수 여야하며 그렇지 않으면 정수 여야합니다.</target>
        </trans-unit>
        <trans-unit id="c25cdbf09c7b02888e46dd28ab2af0cd145a33a9" translate="yes" xml:space="preserve">
          <source>For inputs of type &lt;code&gt;FloatTensor&lt;/code&gt; or &lt;code&gt;DoubleTensor&lt;/code&gt;, arguments &lt;code&gt;beta&lt;/code&gt; and &lt;code&gt;alpha&lt;/code&gt; must be real numbers, otherwise they should be integers.</source>
          <target state="translated">&lt;code&gt;FloatTensor&lt;/code&gt; 또는 &lt;code&gt;DoubleTensor&lt;/code&gt; 유형의 입력의 경우 &lt;code&gt;beta&lt;/code&gt; 및 &lt;code&gt;alpha&lt;/code&gt; 인수 는 실수 여야하며 그렇지 않으면 정수 여야합니다.</target>
        </trans-unit>
        <trans-unit id="abc897209b2f98b7966665fa36a5eddbbc44f66d" translate="yes" xml:space="preserve">
          <source>For instance:</source>
          <target state="translated">예를 들어 :</target>
        </trans-unit>
        <trans-unit id="22c3fe0b8bd316af1ba2f91851ec2f8f7995ee83" translate="yes" xml:space="preserve">
          <source>For legacy reasons, a device can be constructed via a single device ordinal, which is treated as a cuda device. This matches &lt;a href=&quot;tensors#torch.Tensor.get_device&quot;&gt;&lt;code&gt;Tensor.get_device()&lt;/code&gt;&lt;/a&gt;, which returns an ordinal for cuda tensors and is not supported for cpu tensors.</source>
          <target state="translated">레거시 이유로 장치는 cuda 장치로 취급되는 단일 장치 서수를 통해 구성 될 수 있습니다. 이것은 cuda 텐서에 대한 서수를 반환하고 cpu 텐서에 대해 지원되지 않는 &lt;a href=&quot;tensors#torch.Tensor.get_device&quot;&gt; &lt;code&gt;Tensor.get_device()&lt;/code&gt; &lt;/a&gt; 와 일치 합니다.</target>
        </trans-unit>
        <trans-unit id="0aa3a8773fa895e1e6152468962406770d9b0977" translate="yes" xml:space="preserve">
          <source>For loops over constant nn.ModuleList</source>
          <target state="translated">상수 nn.ModuleList에 대한 For 루프</target>
        </trans-unit>
        <trans-unit id="d0ca847043fc995de69359f3596ce96734b521c7" translate="yes" xml:space="preserve">
          <source>For loops over tuples</source>
          <target state="translated">튜플에 대한 For 루프</target>
        </trans-unit>
        <trans-unit id="5aa0148bf5ef53de46ea15471ea81ef66c0693f4" translate="yes" xml:space="preserve">
          <source>For loops with range</source>
          <target state="translated">범위가있는 루프</target>
        </trans-unit>
        <trans-unit id="0d65cf1ed6eabecd00c26213c93e0d17f7fcdf45" translate="yes" xml:space="preserve">
          <source>For more complicated uses of the profilers (like in a multi-GPU case), please see &lt;a href=&quot;https://docs.python.org/3/library/profile.html&quot;&gt;https://docs.python.org/3/library/profile.html&lt;/a&gt; or &lt;a href=&quot;autograd#torch.autograd.profiler.profile&quot;&gt;&lt;code&gt;torch.autograd.profiler.profile()&lt;/code&gt;&lt;/a&gt; for more information.</source>
          <target state="translated">프로파일 러의 더 복잡한 사용 (예 : 다중 GPU의 경우)에 대해서는 &lt;a href=&quot;https://docs.python.org/3/library/profile.html&quot;&gt;https://docs.python.org/3/library/profile.html&lt;/a&gt; 또는 &lt;a href=&quot;autograd#torch.autograd.profiler.profile&quot;&gt; &lt;code&gt;torch.autograd.profiler.profile()&lt;/code&gt; &lt;/a&gt; 에서 자세한 내용을 참조하십시오.</target>
        </trans-unit>
        <trans-unit id="7b8781b8ea8a1a7fd16e315cb8caa2f0d8fd81ce" translate="yes" xml:space="preserve">
          <source>For more information on &lt;code&gt;torch.sparse_coo&lt;/code&gt; tensors, see &lt;a href=&quot;sparse#sparse-docs&quot;&gt;torch.sparse&lt;/a&gt;.</source>
          <target state="translated">에 대한 자세한 내용은 &lt;code&gt;torch.sparse_coo&lt;/code&gt; 의 텐서, 참조 &lt;a href=&quot;sparse#sparse-docs&quot;&gt;torch.sparse&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="5a0ad772d1091f121cc7efb6b451798990b8911a" translate="yes" xml:space="preserve">
          <source>For more information on tensor views, see &lt;a href=&quot;tensor_view#tensor-view-doc&quot;&gt;Tensor Views&lt;/a&gt;.</source>
          <target state="translated">텐서 뷰에 대한 자세한 내용은 &lt;a href=&quot;tensor_view#tensor-view-doc&quot;&gt;텐서보기&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="1f3909dab7a0a74dfec923801d50d6afccbbd76f" translate="yes" xml:space="preserve">
          <source>For more information on the &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt;&lt;code&gt;torch.dtype&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt;&lt;code&gt;torch.device&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;tensor_attributes#torch.torch.layout&quot;&gt;&lt;code&gt;torch.layout&lt;/code&gt;&lt;/a&gt; attributes of a &lt;a href=&quot;#torch.Tensor&quot;&gt;&lt;code&gt;torch.Tensor&lt;/code&gt;&lt;/a&gt;, see &lt;a href=&quot;tensor_attributes#tensor-attributes-doc&quot;&gt;Tensor Attributes&lt;/a&gt;.</source>
          <target state="translated">온 자세한 내용은 &lt;a href=&quot;tensor_attributes#torch.torch.dtype&quot;&gt; &lt;code&gt;torch.dtype&lt;/code&gt; &lt;/a&gt; , &lt;a href=&quot;tensor_attributes#torch.torch.device&quot;&gt; &lt;code&gt;torch.device&lt;/code&gt; &lt;/a&gt; 및 &lt;a href=&quot;tensor_attributes#torch.torch.layout&quot;&gt; &lt;code&gt;torch.layout&lt;/code&gt; &lt;/a&gt; (A)의 속성 &lt;a href=&quot;#torch.Tensor&quot;&gt; &lt;code&gt;torch.Tensor&lt;/code&gt; &lt;/a&gt; 참조 &lt;a href=&quot;tensor_attributes#tensor-attributes-doc&quot;&gt;텐서 속성&lt;/a&gt; .</target>
        </trans-unit>
        <trans-unit id="e6568b8e5f7166a4f6114617787be1538e34997e" translate="yes" xml:space="preserve">
          <source>For now, normalization code can be found in &lt;code&gt;references/video_classification/transforms.py&lt;/code&gt;, see the &lt;code&gt;Normalize&lt;/code&gt; function there. Note that it differs from standard normalization for images because it assumes the video is 4d.</source>
          <target state="translated">현재 정규화 코드는 &lt;code&gt;references/video_classification/transforms.py&lt;/code&gt; 에서 찾을 수 있습니다 . 여기서 &lt;code&gt;Normalize&lt;/code&gt; 함수를 참조하세요 . 비디오가 4d라고 가정하기 때문에 이미지에 대한 표준 정규화와 다릅니다.</target>
        </trans-unit>
        <trans-unit id="43d57f5d34139266a152ef339407d7c7bfa3e16a" translate="yes" xml:space="preserve">
          <source>For numerical stability the implementation reverts to the linear function when</source>
          <target state="translated">수치 안정성을 위해 구현은 다음과 같은 경우 선형 함수로 되돌아갑니다.</target>
        </trans-unit>
        <trans-unit id="e2c95cbc7cf538a8a02cb4f1bc38de8b5a3ed2b9" translate="yes" xml:space="preserve">
          <source>For object detection and instance segmentation, the pre-trained models return the predictions of the following classes:</source>
          <target state="translated">객체 감지 및 인스턴스 분할의 경우 사전 학습 된 모델은 다음 클래스의 예측을 반환합니다.</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>
